{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:03:31.126366Z","iopub.status.busy":"2023-08-30T08:03:31.126068Z","iopub.status.idle":"2023-08-30T08:03:59.049403Z","shell.execute_reply":"2023-08-30T08:03:59.048130Z","shell.execute_reply.started":"2023-08-30T08:03:31.126338Z"},"trusted":true},"outputs":[],"source":["!pip install accelerate==0.19.0 datasets==2.12.0 transformers==4.29.2 evaluate==0.4.0 scikit-learn text2num tokenizers torch>=2.0.0 torchmetrics tqdm wandb "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T12:44:58.498380Z","iopub.status.busy":"2023-08-30T12:44:58.498092Z","iopub.status.idle":"2023-08-30T12:45:14.598002Z","shell.execute_reply":"2023-08-30T12:45:14.597052Z","shell.execute_reply.started":"2023-08-30T12:44:58.498354Z"},"trusted":true},"outputs":[],"source":["import os, urllib.request, inspect, functools, collections, gc\n","from tqdm.auto import tqdm\n","from typing import List, Optional, Union\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","from torch.optim import AdamW\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","\n","import transformers, datasets\n","from transformers import AutoTokenizer, EncoderDecoderModel\n","\n","import wandb\n","\n","# keep datasets in memory if < 8 GB\n","datasets.config.IN_MEMORY_MAX_SIZE = 8 * 1024**3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T12:45:14.600433Z","iopub.status.busy":"2023-08-30T12:45:14.600023Z","iopub.status.idle":"2023-08-30T12:45:14.872614Z","shell.execute_reply":"2023-08-30T12:45:14.871670Z","shell.execute_reply.started":"2023-08-30T12:45:14.600399Z"},"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","wandb_api_key = user_secrets.get_secret(\"wandb-api-key\")\n","\n","os.environ[\"WANDB_API_KEY\"] = wandb_api_key"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:58.666962Z","iopub.status.busy":"2023-08-30T08:05:58.666072Z","iopub.status.idle":"2023-08-30T08:05:58.673689Z","shell.execute_reply":"2023-08-30T08:05:58.672778Z","shell.execute_reply.started":"2023-08-30T08:05:58.666927Z"},"trusted":true},"outputs":[],"source":["from enum import Enum, auto\n","\n","\n","class AnswerType(Enum):\n","    UNKNOWN = auto()\n","    SPAN = auto()\n","    YES_NO = auto()\n","    FLUENCY = auto()\n","    COUNTING = auto()\n","    MULTIPLE_CHOICE = auto()\n","\n","    def __str__(self):\n","        return self.name.lower()\n","\n","    @classmethod\n","    def list(cls, return_unknown=True):\n","        return [str(c) for c in cls if return_unknown or c != AnswerType.UNKNOWN]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:58.676029Z","iopub.status.busy":"2023-08-30T08:05:58.675323Z","iopub.status.idle":"2023-08-30T08:05:58.699592Z","shell.execute_reply":"2023-08-30T08:05:58.698666Z","shell.execute_reply.started":"2023-08-30T08:05:58.675993Z"},"trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","import os\n","from typing import Optional\n","\n","\n","@dataclass\n","class Config:\n","    @dataclass\n","    class Dataset:\n","        train_url: str = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n","        test_url: str = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n","\n","        data_dir: str = os.path.join(\"..\", \"input\", \"assignment2-dataset\", \"data\")\n","        # data_dir: str = \"data\"\n","\n","        raw_dir: str = os.path.join(data_dir, \"raw\")\n","        train_data_raw: str = os.path.join(raw_dir, \"train.json\")\n","        test_data_raw: str = os.path.join(raw_dir, \"test.json\")\n","\n","        annotated_dir: str = os.path.join(data_dir, \"annotated\")\n","        train_data_annotated: str = os.path.join(annotated_dir, \"train.json\")\n","        test_data_annotated: str = os.path.join(annotated_dir, \"test.json\")\n","\n","        readable_dir: str = os.path.join(data_dir, \"readable\")\n","        train_data_readable: str = os.path.join(readable_dir, \"train.txt\")\n","        test_data_readable: str = os.path.join(readable_dir, \"test.txt\")\n","\n","        filtered_dir: str = os.path.join(data_dir, \"filtered\")\n","        splitted_dir: str = os.path.join(data_dir, \"splitted\")\n","        processed_dir: str = os.path.join(data_dir, \"processed\")\n","        train_dir: str = os.path.join(data_dir, \"train\")\n","\n","        def train(self, model_name: str, history:bool, split=\"\"):\n","            if history:\n","                get_dataset_dir = self.train_with_history\n","            else:\n","                get_dataset_dir = self.train_no_history\n","            return get_dataset_dir(model_name, split=split)\n","\n","        def train_no_history(self, model_name: str, split=\"\") -> str:\n","            return os.path.join(self.train_dir, \"train_no_history\", model_name, split)\n","\n","        def train_with_history(self, model_name: str, split=\"\") -> str:\n","            return os.path.join(self.train_dir, \"train_with_history\", model_name, split)\n","\n","\n","    class Checkpoints:\n","        def __init__(\n","            self, distil_roberta=\"distilroberta-base\", bert_tiny=\"prajjwal1/bert-tiny\"\n","        ) -> None:\n","            self.distil_roberta = distil_roberta\n","            self.bert_tiny = bert_tiny\n","\n","    class Models:\n","        def __init__(\n","            self,\n","            model_dir_name=\"models\",\n","            checkpoint_dir_name=\"checkpoints\",\n","            final_checkpoint_name=\"final.pt\",\n","        ) -> None:\n","            self.__model_dir = model_dir_name\n","            self.__checkpoints_dir_name = checkpoint_dir_name\n","            self.__final_checkpoint_name = final_checkpoint_name\n","\n","        def model_dir(self, model_name, history: Optional[bool] = None):\n","            if history is None:\n","                history_str = \"\"\n","            elif history:\n","                history_str = \"history\"\n","            else:\n","                history_str = \"no_history\"\n","            return os.path.join(self.__model_dir, model_name, history_str)\n","\n","        def checkpoints_dir(self, model_name, history: Optional[bool], seed=None):\n","            checkpoint_dir =  os.path.join(\n","                self.model_dir(model_name, history=history), self.__checkpoints_dir_name\n","            )\n","            if seed is not None:\n","                checkpoint_dir = os.path.join(checkpoint_dir, str(seed))\n","            return checkpoint_dir\n","\n","        def checkpoint(self, model_name, history: Optional[bool], seed=None):\n","            return os.path.join(\n","                self.checkpoints_dir(model_name=model_name, history=history, seed=seed),\n","                self.__final_checkpoint_name,\n","            )\n","\n","    @dataclass\n","    class Preprocessing:\n","        encoder_max_length: int\n","        decoder_max_length: int\n","        stride: int = 196\n","        use_window: bool = False\n","        max_history_length: int = 4\n","\n","    @dataclass\n","    class WandbConfig:\n","        \"\"\"Specify the parameters of `wandb.init`\"\"\"\n","\n","        project: str = \"nlp_assignment2\"\n","        entity: str = \"nlp_assignment2\"\n","\n","    dataset: Dataset = Dataset()\n","    checkpoints: Checkpoints = Checkpoints()\n","    models: Models = Models()\n","\n","    # remove all span answers longer than span_max_length words\n","    span_max_length: int = 37\n","    # ignore loss of rationales longer than rationale_max_length\n","    rationale_max_length: int = 150\n","\n","    encoder_max_length = 512\n","    # decoder_max_length = 350\n","    decoder_max_length = 64\n","\n","    preprocessing = Preprocessing(encoder_max_length, decoder_max_length)\n","    generation = dict(penalty_alpha=0.6, top_k=6)\n","\n","    wandbConfig = WandbConfig()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:58.701324Z","iopub.status.busy":"2023-08-30T08:05:58.700978Z","iopub.status.idle":"2023-08-30T08:05:58.716127Z","shell.execute_reply":"2023-08-30T08:05:58.715159Z","shell.execute_reply.started":"2023-08-30T08:05:58.701289Z"},"trusted":true},"outputs":[],"source":["CONFIG: Config = Config()"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:58.718047Z","iopub.status.busy":"2023-08-30T08:05:58.717682Z","iopub.status.idle":"2023-08-30T08:05:58.761157Z","shell.execute_reply":"2023-08-30T08:05:58.760262Z","shell.execute_reply.started":"2023-08-30T08:05:58.718014Z"},"trusted":true},"outputs":[],"source":["import inspect\n","import itertools\n","import os\n","import random\n","from typing import Dict, Literal\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import datasets\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","from text_to_num import text2num\n","\n","\n","class AvgValue:\n","    def __init__(self, initial_value=0.0) -> None:\n","        self.__value = initial_value\n","        self.__last_value = initial_value\n","        self.__n = 0\n","\n","    def update(self, value, n=1):\n","        self.__last_value = value\n","        old_n = self.__n\n","        self.__n += n\n","        self.__value = old_n / self.__n * self.__value + n / self.__n * value\n","\n","    def value(self):\n","        return self.__value\n","\n","    @property\n","    def last_value(self):\n","        return self.__last_value\n","\n","    @property\n","    def n(self):\n","        return self.__n\n","\n","\n","################# Reproducibility ######################à\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","\n","# Using a generator and the following function as `worker_init_fn` preserves reproducibility when using DataLoader\n","def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","\n","def create_reproducible_dataloader(*args, **kwargs):\n","    generator = torch.Generator()\n","    return DataLoader(*args, **kwargs, worker_init_fn=seed_worker, generator=generator)\n","\n","\n","###############################################################\n","\n","\n","def create_dirs_for_file(file_path):\n","    dir = os.path.dirname(file_path)\n","    ensure_dir_exists(dir)\n","\n","\n","def ensure_dir_exists(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","\n","def is_number(string: str) -> bool:\n","    \"\"\"\n","    Check whether a string is a number, both written or numeric.\n","\n","    Args:\n","    - string (str): The string to be checked.\n","\n","    Returns:\n","    - True if the string is a number, False otherwise.\n","    \"\"\"\n","    if string.isdigit():\n","        return True\n","    try:\n","        # Try to convert written number to integer\n","        text2num(string, \"en\", relaxed=True)\n","        return True\n","    except ValueError:\n","        return False\n","\n","\n","def batched_function(fn, scalar_output=True):\n","    def execute_on_batch(batch):\n","        examples = [\n","            fn(dict(zip(batch.keys(), values))) for values in zip(*batch.values())\n","        ]\n","\n","        if scalar_output:\n","            return {\n","                key: [example[key] for example in examples]\n","                for key in examples[0].keys()\n","            }\n","\n","        return {\n","            key: list(itertools.chain(*(example[key] for example in examples)))\n","            for key in examples[0].keys()\n","        }\n","\n","    return execute_on_batch\n","\n","\n","def create_dataframe(dataset: datasets.DatasetDict):\n","    dataset.set_format(\"pandas\")\n","\n","    dataset_ = []\n","    for split, ds in dataset.items():\n","        split_df = ds[:]\n","        split_df[\"split\"] = split\n","        dataset_.append(split_df)\n","    dataset_ = pd.concat(dataset_)\n","    dataset_.reset_index(drop=True, inplace=True)\n","    dataset.reset_format()\n","\n","    return dataset_\n","\n","\n","def explode_qa(dataset: pd.DataFrame):\n","    dataset = dataset.explode([\"questions\", \"answers\"])\n","    dataset.rename(columns={\"questions\": \"question\", \"answers\": \"answer\"}, inplace=True)\n","\n","    questions = pd.json_normalize(dataset[\"question\"])\n","    questions = questions[[\"turn_id\", \"input_text\"]]\n","    questions.rename(\n","        columns={\"input_text\": \"question\", \"turn_id\": \"turn\"}, inplace=True\n","    )\n","\n","    answers = pd.json_normalize(dataset[\"answer\"])\n","    answers = answers[\n","        [\"input_text\", \"span_text\", \"span_start\", \"span_end\", \"answer_type\"]\n","    ]\n","    answers.rename(\n","        columns={\"input_text\": \"answer\", \"span_text\": \"rationale\"}, inplace=True\n","    )\n","\n","    dataset.reset_index(inplace=True)\n","    dataset.drop([\"index\", \"question\", \"answer\"], axis=1, inplace=True)\n","    dataset = dataset.join(questions)\n","    dataset = dataset.join(answers)\n","\n","    cols = dataset.columns.tolist()\n","    cols.append(cols.pop(cols.index(\"last_turn\")))\n","    cols.append(cols.pop(cols.index(\"qa_length\")))\n","    cols.append(cols.pop(cols.index(\"split\")))\n","    return dataset[cols]\n","\n","\n","def plot_answer_type_distribution(qa_dataset: pd.DataFrame):\n","    plot_distribution(qa_dataset, field=\"answer_type\", hue=\"split\")\n","\n","\n","def plot_distribution(dataset: pd.DataFrame, field: str, hue: str = None):\n","    if hue is not None:\n","        dataset = dataset.groupby(hue)\n","\n","    distribution = dataset[field].value_counts(normalize=True)\n","    distribution = distribution.apply(lambda x: np.round(x, decimals=3) * 100)\n","    distribution = distribution.rename(\"frequency\").reset_index()\n","    ax = sns.barplot(distribution, x=field, y=\"frequency\", hue=hue)\n","\n","    for i in ax.containers:\n","        ax.bar_label(\n","            i,\n","        )\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def show_inputs(tokenizer, data, inputs):\n","    for k, v in inputs.items():\n","        print(f\"{k:<27}: {v}\")\n","    print()\n","\n","    for idx in range(len(inputs[\"input_ids\"])):\n","        show_input(tokenizer, data, inputs, idx)\n","        print()\n","\n","\n","def show_input(tokenizer, data, inputs, idx):\n","    sample_idx = (\n","        inputs[\"overflow_to_sample_mapping\"][idx]\n","        if \"overflow_to_sample_mapping\" in inputs\n","        else idx\n","    )\n","\n","    input_ids = np.asarray(inputs[\"input_ids\"][idx])\n","    passage_mask = np.asarray(inputs[\"passage_mask\"][idx])\n","    rationale_labels = np.asarray(inputs[\"rationale_labels\"][idx])\n","    rationale_start = inputs[\"rationale_start\"][idx]\n","    rationale_end = inputs[\"rationale_end\"][idx]\n","    labels = np.asarray(inputs[\"labels\"][idx])\n","    decoder_attention_mask = np.asarray(inputs[\"decoder_attention_mask\"][idx])\n","\n","    passage = input_ids[passage_mask.astype(np.bool_)]\n","    rationale = input_ids[rationale_labels > 0]\n","    assert np.all(rationale == input_ids[rationale_start:rationale_end])\n","    answer = labels[decoder_attention_mask.astype(np.bool_)]\n","\n","    print(\"Input:\", tokenizer.decode(input_ids))\n","    print(\"Q:\", data[\"question\"][sample_idx])\n","    print(\"P (-):\", data[\"passage\"][sample_idx])\n","    print(\"P (+):\", tokenizer.decode(passage))\n","    print(\"R (-):\", data[\"rationale\"][sample_idx])\n","    print(\"R (+):\", tokenizer.decode(rationale))\n","    print(\"A (-):\", data[\"answer\"][sample_idx])\n","    print(\"A (+):\", tokenizer.decode(answer))\n","    print(\"History:\", data[\"history\"][sample_idx])\n","\n","\n","def logits_to_class(logits, task: Literal[\"binary\", \"multiclass\"]) -> torch.LongTensor:\n","    if task == \"binary\":\n","        return (logits > 0.0).long()\n","    elif task == \"multiclass\":\n","        return torch.argmax(logits, dim=-1).long()\n","    else:\n","        raise ValueError(\n","            \"Invalid task. Supported values are 'binary' and 'multiclass'.\"\n","        )\n","\n","\n","def prepare_model_inputs(\n","    model: nn.Module, inputs: Dict[str, torch.Tensor]\n",") -> Dict[str, torch.Tensor]:\n","    forward_signature = set(inspect.signature(model.forward).parameters)\n","    inputs = {\n","        argument: value.to(model.device)\n","        for argument, value in inputs.items()\n","        if argument in forward_signature\n","    }\n","    return inputs"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:58.763257Z","iopub.status.busy":"2023-08-30T08:05:58.762903Z","iopub.status.idle":"2023-08-30T08:05:58.822732Z","shell.execute_reply":"2023-08-30T08:05:58.821755Z","shell.execute_reply.started":"2023-08-30T08:05:58.763213Z"},"trusted":true},"outputs":[],"source":["import itertools\n","import re\n","import string\n","import transformers\n","\n","import numpy as np\n","\n","from typing import List, Tuple\n","\n","\n","def answer_to_idx(answer: str) -> int:\n","    if answer.lower() == \"yes\":\n","        return 0\n","    if answer.lower() == \"no\":\n","        return 1\n","    return 2\n","\n","\n","def idx_to_answer(idx: int) -> str:\n","    if idx == 0:\n","        return \"yes\"\n","    if idx == 1:\n","        return \"no\"\n","    return None\n","\n","\n","class CoQADatasetPreprocessing:\n","    def __init__(\n","        self,\n","        tokenizer: transformers.PreTrainedTokenizer = None,\n","        label_pad_token_id=-100,\n","        encoder_max_length=512,\n","        decoder_max_length=350,\n","        stride=196,\n","        use_window=False,\n","        max_history_length=4,\n","    ) -> None:\n","        self.tokenizer = tokenizer\n","        self.label_pad_token_id = label_pad_token_id\n","        self.encoder_max_length = encoder_max_length\n","        self.decoder_max_length = decoder_max_length\n","        self.stride = stride\n","        self.use_window = use_window\n","        self.max_history_length = max_history_length\n","\n","    def explode_questions(self, example):\n","        questions = example[\"questions\"]\n","        answers = example[\"answers\"]\n","        histories = []\n","\n","        for idx in range(len(questions)):\n","            history = self.__create_history(idx, questions, answers)\n","            histories.append(history)\n","\n","        output = {\n","            \"id\": [example[\"id\"]] * example[\"qa_length\"],\n","            \"turn\": [question_item[\"turn_id\"] for question_item in questions],\n","            \"question\": [question_item[\"input_text\"] for question_item in questions],\n","            \"answer\": [answer_item[\"input_text\"] for answer_item in answers],\n","            \"rationale\": [answer_item[\"span_text\"] for answer_item in answers],\n","            \"span_start\": [answer_item[\"span_start\"] for answer_item in answers],\n","            \"span_end\": [answer_item[\"span_end\"] for answer_item in answers],\n","            \"answer_type\": [answer_item[\"answer_type\"] for answer_item in answers],\n","            \"history\": histories,\n","            \"history_length\": [len(history) for history in histories],\n","        }\n","        for key, value in example.items():\n","            if key not in output:\n","                output[key] = [value] * example[\"qa_length\"]\n","        return output\n","\n","    def __create_history(self, current_index, questions, answers):\n","        history = [\n","            {\n","                \"question\": questions[i][\"input_text\"],\n","                \"answer\": answers[i][\"input_text\"],\n","                \"turn\": questions[i][\"turn_id\"],\n","            }\n","            for i in range(current_index)\n","        ]\n","        return history\n","\n","    def preprocess_texts(self, example):\n","        handle_rationale = \"rationale\" in example\n","        if handle_rationale:\n","            example = self.__fix_rationale(example)\n","\n","        example = self.__preprocess_passage(example, handle_rationale=handle_rationale)\n","        example = self.__preprocess_questions(example)\n","        example = self.__preprocess_answers(example)\n","\n","        return example\n","\n","    def __fix_rationale(self, example):\n","        rationale, span_start, span_end = fix_rationale(\n","            example[\"passage\"],\n","            example[\"rationale\"],\n","            example[\"span_start\"],\n","            example[\"span_end\"],\n","        )\n","\n","        example[\"rationale\"] = rationale\n","        example[\"span_start\"] = span_start\n","        example[\"span_end\"] = span_end\n","\n","        return example\n","\n","    def __preprocess_passage(self, example, handle_rationale=True):\n","        return self.__fix_passage_white_space(\n","            example, handle_rationale=handle_rationale\n","        )\n","\n","    def __preprocess_questions(self, example):\n","        example[\"question\"] = white_space_fix(example[\"question\"])\n","        for item in example.get(\"history\", []):\n","            item[\"question\"] = white_space_fix(item[\"question\"])\n","        return example\n","\n","    def __preprocess_answers(self, example):\n","        if \"answer\" in example:\n","            example[\"answer\"] = self.__preprocess_answer(example[\"answer\"])\n","        for item in example.get(\"history\", []):\n","            item[\"answer\"] = self.__preprocess_answer(item[\"answer\"])\n","        return example\n","\n","    def __preprocess_answer(self, answer):\n","        answer = white_space_fix(answer)\n","        answer = strip_non_alphanumeric_chars(answer)\n","        return answer\n","\n","    def __fix_passage_white_space(self, example, handle_rationale=True):\n","        passage = example[\"passage\"]\n","        if handle_rationale:\n","            span_start = example[\"span_start\"]\n","            span_end = example[\"span_end\"]\n","\n","            if span_end - span_start > 0:\n","                # assert rationale has already been fixed\n","                assert (\n","                    passage[span_start].isalnum() and passage[span_end - 1].isalnum()\n","                ), \"Rationale must start and end with alphanumeric characters. You must fix it before.\"\n","\n","            passage_start = white_space_fix(passage[:span_start])\n","            rationale = white_space_fix(passage[span_start:span_end])\n","            passage_end = white_space_fix(passage[span_end:])\n","\n","            passage = \" \".join((passage_start, rationale, passage_end))\n","            span_start = len(passage_start) + 1\n","            span_end = span_start + len(rationale)\n","\n","            assert rationale == passage[span_start:span_end]\n","\n","            example[\"passage\"] = passage\n","            example[\"rationale\"] = rationale\n","            example[\"span_start\"] = span_start\n","            example[\"span_end\"] = span_end\n","        else:\n","            example[\"passage\"] = white_space_fix(passage)\n","\n","        return example\n","\n","    def process_data_to_model_inputs(\n","        self,\n","        examples,\n","        add_history=False,\n","        padding=False,\n","    ) -> transformers.BatchEncoding:\n","        assert (\n","            self.tokenizer is not None\n","        ), \"A tokenizer is required to prepare the inputs for the model\"\n","        process_rationale = \"rationale\" in examples\n","        process_answer = \"answer\" in examples\n","\n","        sentences = [examples[\"question\"], examples[\"passage\"]]\n","\n","        if add_history:\n","            sentences[0] = self.__concat_history_and_question(\n","                examples[\"history\"], examples[\"question\"]\n","            )\n","\n","        inputs = self.tokenizer(\n","            *sentences,\n","            padding=padding,\n","            truncation=\"only_second\",\n","            max_length=self.encoder_max_length,\n","            stride=self.stride,\n","            return_overflowing_tokens=self.use_window,\n","            return_offsets_mapping=True,\n","        )\n","        if process_answer:\n","            outputs = self.tokenizer(\n","                examples[\"answer\"],\n","                padding=padding,\n","                truncation=True,\n","                max_length=self.decoder_max_length,\n","            )\n","\n","        offset_mapping = inputs[\"offset_mapping\"]\n","        if self.use_window:\n","            sample_map = lambda i: inputs[\"overflow_to_sample_mapping\"][i]\n","        else:\n","            sample_map = lambda i: i\n","\n","        yes_no_types = []\n","        yng_labels = []\n","\n","        passage_masks = []\n","        rationale_starts = []\n","        rationale_ends = []\n","        rationale_labels = []\n","        decoder_input_ids = []\n","        labels = []\n","        decoder_attention_masks = []\n","\n","        ids = []\n","        turns = []\n","\n","        # # store the presence of the rationale in the passage for at least one row\n","        # rationale_in_passage = [False] * len(examples[\"question\"])\n","        for i, offset in enumerate(offset_mapping):\n","            sample_idx = sample_map(i)\n","            sequence_ids = inputs.sequence_ids(i)\n","\n","            passage_start, passage_end = self.__find_passage(sequence_ids)\n","            passage_masks.append(\n","                self.__create_mask(sequence_ids, passage_start, passage_end + 1)\n","            )\n","\n","            if process_rationale:\n","                start_char = examples[\"span_start\"][sample_idx]\n","                end_char = examples[\"span_end\"][sample_idx]\n","                rationale_start, rationale_end = self.__char2token_rationale_span(\n","                    offset, (passage_start, passage_end), (start_char, end_char)\n","                )\n","                rationale_starts.append(rationale_start)\n","                rationale_ends.append(rationale_end)\n","                rationale_labels_ = self.__create_mask(\n","                    sequence_ids, rationale_start, rationale_end, dtype=np.float32\n","                )\n","                rationale_labels_[passage_masks[-1] == 0] = self.label_pad_token_id\n","                rationale_labels.append(rationale_labels_)\n","\n","                # rationale_in_passage[sample_idx] |= rationale_start != -1\n","\n","            if process_answer:\n","                # Remove <eos> from decoder_input_ids\n","                decoder_input_ids_ = outputs.input_ids[sample_idx][:-1]\n","                # Remove <bos> from labels\n","                labels_ = outputs.input_ids[sample_idx].copy()[1:]\n","                labels_ = [\n","                    self.label_pad_token_id\n","                    if token == self.tokenizer.pad_token_id\n","                    else token\n","                    for token in labels_\n","                ]\n","                decoder_attention_mask = outputs.attention_mask[sample_idx][:-1]\n","\n","                decoder_input_ids.append(decoder_input_ids_)\n","                labels.append(labels_)\n","                decoder_attention_masks.append(decoder_attention_mask)\n","\n","                yng_label = answer_to_idx(examples[\"answer\"][sample_idx])\n","                is_yes_no = yng_label < 2\n","                assert is_yes_no == (examples[\"answer_type\"][sample_idx] == \"yes_no\")\n","                yng_labels.append(yng_label)\n","                yes_no_types.append(int(is_yes_no))\n","\n","            ids.append(examples[\"id\"][sample_idx])\n","            if \"turn\" in examples:\n","                turns.append(examples[\"turn\"][sample_idx])\n","\n","        # if process_rationale:\n","        #     for sample_idx, is_rationale_in_passage in enumerate(rationale_in_passage):\n","        #         if not is_rationale_in_passage:\n","        #             warnings.warn(f\"The rationale is never contained in the passage. Id: {examples['id'][sample_idx]}, turn:{examples['turn'][sample_idx]}\")\n","\n","        inputs[\"passage_mask\"] = passage_masks\n","        if process_rationale:\n","            inputs[\"rationale_start\"] = rationale_starts\n","            inputs[\"rationale_end\"] = rationale_ends\n","            inputs[\"rationale_labels\"] = rationale_labels\n","        if process_answer:\n","            inputs[\"decoder_input_ids\"] = decoder_input_ids\n","            inputs[\"labels\"] = labels\n","            inputs[\"decoder_attention_mask\"] = decoder_attention_masks\n","            inputs[\"yng_label\"] = yng_labels\n","            inputs[\"yes_no\"] = yes_no_types\n","        inputs[\"id\"] = ids\n","        if len(turns) > 0:\n","            inputs[\"turn\"] = turns\n","\n","        return inputs\n","\n","    def __concat_history_and_question(self, histories, questions):\n","        outputs = []\n","        for history, question in zip(histories, questions):\n","            history_str = self.__create_history_str(history)\n","            history_question = self.tokenizer.sep_token.join((history_str, question))\n","            outputs.append(history_question)\n","        return outputs\n","\n","    def __create_history_str(self, history):\n","        history_items = reversed(\n","            tuple(itertools.islice((reversed(history)), self.max_history_length))\n","        )\n","        qa_pairs = []\n","        for item in history_items:\n","            qa = (item[\"question\"], item[\"answer\"])\n","            qa = self.tokenizer.sep_token.join(qa)\n","            qa_pairs.append(qa)\n","        return self.tokenizer.sep_token.join(qa_pairs)\n","\n","    def __find_passage(self, sequence_ids: List[int]) -> Tuple[int, int]:\n","        \"\"\"\n","        Find the start and the end of the passage w.r.t. the tokens.\n","        \"\"\"\n","\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        passage_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        passage_end = idx - 1\n","\n","        return passage_start, passage_end\n","\n","    def __char2token_rationale_span(\n","        self,\n","        token_to_span: List[Tuple[int, int]],\n","        passage_token_span: Tuple[int, int],\n","        rationale_char_span: Tuple[int, int],\n","    ) -> Tuple[int, int]:\n","        \"\"\"\n","        Map the rationale span from char indexes to token indexes\n","        \"\"\"\n","        passage_start, passage_end = passage_token_span\n","        start_char, end_char = rationale_char_span\n","\n","        # If the rationale is not fully inside the passage, returns (-1, -1)\n","        if (\n","            token_to_span[passage_start][0] > start_char\n","            or token_to_span[passage_end][1] < end_char\n","        ):\n","            return (-1, -1)\n","\n","        # Otherwise it's the start and end token positions\n","        idx = passage_start\n","        while idx <= passage_end and token_to_span[idx][0] <= start_char:\n","            idx += 1\n","        start_position = idx - 1\n","\n","        idx = passage_end\n","        while idx >= passage_start and token_to_span[idx][1] >= end_char:\n","            idx -= 1\n","        end_position = idx + 1\n","\n","        return start_position, end_position + 1\n","\n","    def __create_mask(self, arr, start, end, dtype=np.int8):\n","        mask = np.zeros_like(arr, dtype=dtype)\n","        mask[start:end] = 1\n","        return mask\n","\n","\n","def remove_articles_(text):\n","    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n","    return re.sub(regex, \" \", text)\n","\n","\n","def white_space_fix(text: str):\n","    return \" \".join(text.split())\n","\n","\n","def remove_punc(text):\n","    exclude = set(string.punctuation)\n","    return \"\".join(ch for ch in text if ch not in exclude)\n","\n","\n","def lower(text):\n","    return text.lower()\n","\n","\n","def normalize_text(text, remove_articles=False):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    text = remove_punc(lower(text))\n","    if remove_articles:\n","        text = remove_articles_(text)\n","    return white_space_fix(text)\n","\n","\n","def normalize_answer(text):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    return normalize_text(text, remove_articles=True)\n","\n","\n","def strip_non_alphanumeric_chars(text: str):\n","    \"\"\"\n","    Removes trailing and leading non alpha-numeric characters from a given string.\n","    \"\"\"\n","    start_index = 0\n","    while start_index < len(text) and not text[start_index].isalnum():\n","        start_index += 1\n","\n","    end_index = len(text) - 1\n","    while end_index > start_index and not text[end_index].isalnum():\n","        end_index -= 1\n","\n","    return text[start_index : end_index + 1]\n","\n","\n","def find_span(passage: str, text: str, span_start: int = None, span_end: int = None):\n","    if len(text) == 0:\n","        return (span_start, span_start)\n","    assert (\n","        text[0].isalnum() and text[-1].isalnum()\n","    ), \"Text must begin and end with an alphanumeric character.\"\n","\n","    start_idx = passage.find(text, span_start, span_end)\n","    end_idx = start_idx + len(text) - 1\n","\n","    if start_idx == -1:\n","        raise ValueError(\"The text is not present in the passage.\")\n","\n","    # Find the beginning of the word in the passage\n","    while start_idx > 0 and passage[start_idx - 1].isalnum():\n","        start_idx -= 1\n","\n","    # Find the end of the word in the passage\n","    while end_idx < len(passage) - 1 and passage[end_idx + 1].isalnum():\n","        end_idx += 1\n","\n","    return start_idx, end_idx + 1\n","\n","\n","def fix_rationale(passage: str, rationale: str, span_start: int, span_end: int):\n","    rationale = strip_non_alphanumeric_chars(rationale)\n","    span_start, span_end = find_span(\n","        passage, rationale, span_start=span_start, span_end=span_end\n","    )\n","    return passage[span_start:span_end], span_start, span_end\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model definition"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:58.824779Z","iopub.status.busy":"2023-08-30T08:05:58.824443Z","iopub.status.idle":"2023-08-30T08:05:58.895392Z","shell.execute_reply":"2023-08-30T08:05:58.894447Z","shell.execute_reply.started":"2023-08-30T08:05:58.824741Z"},"trusted":true},"outputs":[],"source":["from typing import List, Optional, Union, Tuple\n","from dataclasses import dataclass\n","import warnings\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoModel, EncoderDecoderModel\n","\n","\n","@dataclass\n","class QAEncoderModelOutput(transformers.utils.ModelOutput):\n","    \"\"\"\n","    Base class for outputs of question answering with rationale models.\n","    Args:\n","        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n","            Total rationale extraction loss is the sum of a Binary Cross-Entropy for the tokens in the passage.\n","        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n","            Sequence of hidden-states at the output of the last layer of the model.\n","        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n","            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n","            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n","            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n","        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n","            heads.\n","        rationale_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n","            Rationale classification scores (before Sigmoid).\n","        yng_logits (`torch.FloatTensor` of shape `(batch_size, 3)`):\n","            Yes/No/Generative scores (before Sigmoid).\n","    \"\"\"\n","\n","    loss: Optional[torch.FloatTensor] = None\n","    last_hidden_state: torch.FloatTensor = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    rationale_logits: torch.FloatTensor = None\n","    yng_logits: torch.FloatTensor = None\n","\n","\n","@dataclass\n","class QAEncoderDecoderModelOutput(transformers.utils.ModelOutput):\n","    \"\"\"\n","    Class for [`QAEncoderDecoderModel`] outputs.\n","    Args:\n","        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n","            Language modeling loss.\n","        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n","            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n","        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n","            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n","            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n","            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n","            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n","            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n","        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n","            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n","            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n","            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n","        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n","            self-attention heads.\n","        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n","            weighted average in the cross-attention heads.\n","        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n","            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n","        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n","            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n","            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n","            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n","        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n","            self-attention heads.\n","        encoder_rationale_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n","            Encoder rationale classification scores (before Sigmoid).\n","        encoder_yng_logits (`torch.FloatTensor` of shape `(batch_size, 3)`):\n","            Encoder Yes/No/Generative scores (before Sigmoid).\n","    \"\"\"\n","\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n","    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n","    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_rationale_logits: Optional[torch.FloatTensor] = None\n","    encoder_yng_logits: torch.FloatTensor = None\n","\n","\n","class QAEncoderDecoderModel(transformers.EncoderDecoderModel):\n","    def __init__(\n","        self,\n","        encoder: transformers.PreTrainedModel,\n","        decoder: transformers.PreTrainedModel,\n","        config: Optional[transformers.EncoderDecoderConfig] = None,\n","    ):\n","        super(QAEncoderDecoderModel, self).__init__(\n","            encoder=encoder, decoder=decoder, config=config\n","        )\n","\n","    def tie_weights(self):\n","        # tie encoder & decoder if needed\n","        if self.config.tie_encoder_decoder:\n","            # tie encoder and decoder base model\n","            encoder_base_model_prefix = self.encoder.base_model_prefix\n","            decoder_base_model_prefix = self.decoder.base_model_prefix\n","            self._tie_encoder_decoder_weights(\n","                self.encoder._modules[encoder_base_model_prefix],\n","                self.decoder._modules[decoder_base_model_prefix],\n","                self.decoder.base_model_prefix,\n","            )\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        token_type_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        passage_mask: Optional[torch.BoolTensor] = None,\n","        decoder_input_ids: Optional[torch.LongTensor] = None,\n","        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n","        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n","        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","        # labels: Optional[torch.LongTensor] = None,\n","        use_cache: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","        teacher_force: Optional[float] = None,\n","        rationale_labels: Optional[torch.BoolTensor] = None,\n","        **kwargs,\n","    ) -> Union[Tuple, transformers.modeling_outputs.Seq2SeqLMOutput]:\n","        kwargs_encoder = {\n","            argument: value\n","            for argument, value in kwargs.items()\n","            if not argument.startswith(\"decoder\")\n","        }\n","\n","        if encoder_outputs is None:\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","                passage_mask=passage_mask,\n","                return_dict=return_dict,\n","                teacher_force=teacher_force,\n","                rationale_labels=rationale_labels,\n","                **kwargs_encoder,\n","            )\n","\n","        outputs = super().forward(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            encoder_outputs=encoder_outputs,\n","            past_key_values=past_key_values,\n","            inputs_embeds=inputs_embeds,\n","            decoder_inputs_embeds=decoder_inputs_embeds,\n","            # labels=labels,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            token_type_ids=token_type_ids,\n","            **kwargs,\n","        )\n","\n","        if not return_dict:\n","            return outputs + encoder_outputs[-2:]\n","\n","        return QAEncoderDecoderModelOutput(\n","            **outputs,\n","            encoder_rationale_logits=encoder_outputs.rationale_logits,\n","            encoder_yng_logits=encoder_outputs.yng_logits,\n","        )\n","\n","\n","class QAEncoder(transformers.PreTrainedModel):\n","    base_model_prefix = \"encoder\"\n","\n","    def __init__(self, encoder, config) -> None:\n","        super().__init__(config)\n","        self.config = config\n","\n","        self.encoder = encoder\n","        self.rationale_head = TokenSelectionHead(config)\n","        self.yes_no_gen_head = YesNoGenHead(config)\n","\n","        self.post_init()\n","\n","    def _init_weights(self, module):\n","        \"\"\"Initialize the weights\"\"\"\n","        if isinstance(module, nn.Linear):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.Tensor] = None,\n","        token_type_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        passage_mask: Optional[torch.Tensor] = None,\n","        # labels: Optional[torch.FloatTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","        teacher_force: Optional[float] = None,\n","        rationale_labels: Optional[torch.BoolTensor] = None,\n","        **kwargs,\n","    ) -> Union[Tuple, QAEncoderModelOutput]:\n","        assert passage_mask is not None, \"Passage mask is required\"\n","\n","        outputs = self.encoder(\n","            input_ids=input_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask,\n","            inputs_embeds=inputs_embeds,\n","            # labels=labels,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            **kwargs,\n","        )\n","\n","        last_hidden_state = outputs[0]\n","        pooled_output = outputs[1]\n","\n","        rationale_logits = self.rationale_head(last_hidden_state)\n","\n","        passage_mask = passage_mask.unsqueeze(-1)\n","        p_rationale = torch.sigmoid(rationale_logits)\n","        # substitute the last_hidden_state[passage] with p_rationale * last_hidden_state[passage]\n","        # ideally, our network keeps only the span of the passage which represents the rationale\n","        if self.training:\n","            if teacher_force is not None:\n","                use_labels = torch.rand(rationale_labels.shape[0]) < teacher_force\n","                use_labels = use_labels.to(p_rationale.device)\n","                true_labels = rationale_labels.unsqueeze(-1).type(p_rationale.dtype)\n","                p_rationale = torch.where(use_labels.reshape(-1, 1, 1), true_labels, p_rationale)\n","        else:\n","            p_rationale = (p_rationale > self.config.p_rationale_threshold).type(p_rationale.dtype)\n","\n","        weighted_passage_hidden_state = passage_mask * p_rationale * last_hidden_state\n","        qa_seq_hidden_state = (1 - passage_mask) * last_hidden_state\n","        last_hidden_state = weighted_passage_hidden_state + qa_seq_hidden_state\n","\n","        yng_logits = self.yes_no_gen_head(weighted_passage_hidden_state, pooled_output)\n","\n","        rationale_logits = rationale_logits.squeeze(-1)\n","\n","        loss = None\n","        if not return_dict:\n","            output = (last_hidden_state,) + outputs[2:] + (rationale_logits,)\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return QAEncoderModelOutput(\n","            loss=loss,\n","            last_hidden_state=last_hidden_state,\n","            rationale_logits=rationale_logits,\n","            yng_logits=yng_logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","\n","class TokenSelectionHead(nn.Module):\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n","        self.act_fn = nn.ReLU()\n","        self.hidden_to_logit = nn.Linear(config.hidden_size, 1, bias=False)\n","\n","    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.act_fn(hidden_states)\n","        logits = self.hidden_to_logit(hidden_states)\n","        return logits\n","\n","\n","class YesNoGenHead(nn.Module):\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n","        self.act_fn = nn.ReLU()\n","        self.hidden_to_logit = nn.Linear(config.hidden_size, 1, bias=False)\n","        self.softmax = nn.Softmax(dim=-2)\n","\n","        classifier_dropout = (\n","            config.classifier_dropout\n","            if config.classifier_dropout is not None\n","            else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(2 * config.hidden_size, 3)\n","\n","    def forward(\n","        self,\n","        rationale_weighted_hidden_states: torch.FloatTensor,\n","        pooled_output: torch.FloatTensor,\n","    ) -> torch.FloatTensor:\n","        rationale_weighted_hidden_states = self.dense(rationale_weighted_hidden_states)\n","        rationale_weighted_hidden_states = self.act_fn(\n","            rationale_weighted_hidden_states\n","        )  # BxTxD\n","        logits = self.hidden_to_logit(rationale_weighted_hidden_states)  # BxTx1\n","        attention_scores = self.softmax(logits)  # BxTx1\n","        weighted_tokens = attention_scores * rationale_weighted_hidden_states  # BxTxD\n","        weighted_pooled_output = torch.sum(weighted_tokens, dim=-2)  # BxD\n","        pooled_output = torch.cat(\n","            (weighted_pooled_output, pooled_output), dim=-1\n","        )  # Bx2D\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        return logits\n","\n","\n","def initialize_cross_attention_layer_with_self_attention_layer(\n","    self_attention: nn.Module,\n","    cross_attention: nn.Module,\n","    cross_attention_layer_prefix: str,\n","):\n","    uninitialized_cross_attention_weights: List[str] = []\n","    if cross_attention.__class__ != self_attention.__class__:\n","        print(\n","            f\"{cross_attention.__class__} and {self_attention.__class__} are not equal. In this case make sure that all encoder\"\n","            \" weights are correctly initialized.\"\n","        )\n","\n","    def initialize_cross_attention_with_self_attention_recursively(\n","        self_attention_pointer: nn.Module,\n","        cross_attention_pointer: nn.Module,\n","        module_name: str,\n","        uninitialized_cross_attention_weights: List[str],\n","        depth=0,\n","    ):\n","        assert isinstance(self_attention_pointer, nn.Module) and isinstance(\n","            cross_attention_pointer, nn.Module\n","        ), f\"{self_attention_pointer} and {cross_attention_pointer} have to be of type nn.Module\"\n","        if hasattr(self_attention_pointer, \"weight\"):\n","            assert hasattr(cross_attention_pointer, \"weight\")\n","            cross_attention_pointer.weight.data = (\n","                self_attention_pointer.weight.data.clone().detach()\n","            )\n","            if hasattr(self_attention_pointer, \"bias\"):\n","                assert hasattr(cross_attention_pointer, \"bias\")\n","                cross_attention_pointer.bias.data = (\n","                    self_attention_pointer.bias.data.clone().detach()\n","                )\n","            return\n","\n","        cross_attention_modules = cross_attention_pointer._modules\n","        self_attention_modules = self_attention_pointer._modules\n","        if len(self_attention_modules) > 0:\n","            assert (\n","                len(cross_attention_modules) > 0\n","            ), f\"Cross-attention module {cross_attention_pointer} does not match self-attention module {self_attention_pointer}\"\n","\n","            all_cross_attention_weights = {\n","                module_name + \"/\" + sub_name\n","                for sub_name in cross_attention_modules.keys()\n","            }\n","            cross_attention_layer_pos = 0\n","            for name, module in self_attention_modules.items():\n","                if name.isdigit():\n","                    cross_attention_name = str(int(name) + cross_attention_layer_pos)\n","                    self_attention_name = name\n","                    if not isinstance(\n","                        self_attention_modules[self_attention_name],\n","                        type(cross_attention_modules[cross_attention_name]),\n","                    ) and len(cross_attention_modules) != len(self_attention_modules):\n","                        # this can happen if the name corresponds to the position in a list module list of layers\n","                        # in this case the decoder has added a cross-attention that the encoder does not have\n","                        # thus skip this step and subtract one layer pos from encoder\n","                        cross_attention_layer_pos -= 1\n","                        continue\n","                elif name not in cross_attention_modules:\n","                    continue\n","                elif depth > 500:\n","                    raise ValueError(\n","                        \"Max depth of recursive function `initialize_cross_attention_with_self_attention` reached. It seems that there is\"\n","                        \" a circular dependency between two or more `nn.Modules` of your model.\"\n","                    )\n","                else:\n","                    self_attention_name = cross_attention_name = name\n","                initialize_cross_attention_with_self_attention_recursively(\n","                    self_attention_modules[self_attention_name],\n","                    cross_attention_modules[cross_attention_name],\n","                    module_name + \"/\" + name,\n","                    uninitialized_cross_attention_weights,\n","                    depth=depth + 1,\n","                )\n","                all_cross_attention_weights.remove(\n","                    module_name + \"/\" + cross_attention_name\n","                )\n","\n","            uninitialized_cross_attention_weights += list(all_cross_attention_weights)\n","\n","    # initialize weights recursively\n","    initialize_cross_attention_with_self_attention_recursively(\n","        self_attention,\n","        cross_attention,\n","        cross_attention_layer_prefix,\n","        uninitialized_cross_attention_weights,\n","    )\n","    if len(uninitialized_cross_attention_weights) > 0:\n","        warnings.warn(\n","            f\"The following cross_attention weights were not initialized with self_attention weights: {uninitialized_cross_attention_weights}\"\n","        )\n","\n","\n","def initialize_cross_attention_with_self_attention(model: EncoderDecoderModel):\n","    decoder_base_model_prefix = model.decoder.base_model_prefix\n","    for layer_idx in range(model.config.decoder.num_hidden_layers):\n","        decoder_layer = model.decoder._modules[decoder_base_model_prefix].encoder.layer[\n","            layer_idx\n","        ]\n","        cross_attention = decoder_layer.crossattention\n","        self_attention = decoder_layer.attention\n","        cross_attention_name = f\"layer.{layer_idx}.crossattention\"\n","        initialize_cross_attention_layer_with_self_attention_layer(\n","            self_attention, cross_attention, cross_attention_name\n","        )\n","    print(\"Cross-attention has been initialized with self-attention weights.\")\n","\n","\n","def make_encoder_decoder_model(\n","    checkpoint,\n","    decoder_max_length,\n","    generation_kwargs,\n","    tokenizer: Optional[transformers.PreTrainedTokenizer] = None,\n","    initialize_cross_attention=True,\n","):\n","    tokenizer, encoder = make_qa_encoder(checkpoint, tokenizer=tokenizer)\n","    decoder = transformers.AutoModelForCausalLM.from_pretrained(\n","        checkpoint,\n","        is_decoder=True,\n","        add_cross_attention=True,\n","        decoder_start_token_id=tokenizer.bos_token_id,\n","    )\n","\n","    config = transformers.EncoderDecoderConfig.from_encoder_decoder_configs(\n","        encoder.config,\n","        decoder.config,\n","        tie_encoder_decoder=True,\n","        decoder_start_token_id=tokenizer.cls_token_id,\n","        eos_token_id=tokenizer.sep_token_id,\n","        pad_token_id=tokenizer.pad_token_id,\n","        # sensible parameters for generation\n","        vocab_size=decoder.config.vocab_size,\n","        max_new_tokens=decoder_max_length,\n","        **generation_kwargs,\n","    )\n","\n","    model = QAEncoderDecoderModel(encoder=encoder, decoder=decoder, config=config)\n","    if initialize_cross_attention:\n","        initialize_cross_attention_with_self_attention(model)\n","\n","    return tokenizer, model\n","\n","\n","def make_qa_encoder(\n","    checkpoint, tokenizer: Optional[transformers.PreTrainedTokenizer] = None\n","):\n","    if tokenizer is None:\n","        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","    encoder = AutoModel.from_pretrained(checkpoint)\n","    encoder.config.p_rationale_threshold = 0.5\n","    encoder = QAEncoder(encoder, encoder.config)\n","    return tokenizer, encoder\n"]},{"cell_type":"markdown","metadata":{},"source":["# Losses"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:58.899565Z","iopub.status.busy":"2023-08-30T08:05:58.899205Z","iopub.status.idle":"2023-08-30T08:05:58.949948Z","shell.execute_reply":"2023-08-30T08:05:58.948805Z","shell.execute_reply.started":"2023-08-30T08:05:58.899535Z"},"trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Dict, Optional, Protocol, Tuple\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","\n","_EPSILON = 1e-7\n","\n","\n","def apply_reduction(input: torch.Tensor, reduction: str, dim=0):\n","    if reduction == \"none\":\n","        return input\n","\n","    if input.shape[0] == 0:\n","        input = torch.Tensor([0]).type(dtype=input.dtype).to(device=input.device)\n","    if reduction == \"mean\":\n","        return torch.mean(input, dim=dim)\n","    if reduction == \"sum\":\n","        return torch.sum(input, dim=dim)\n","\n","    raise ValueError(\n","        \"Invalid reduction. Supported values are 'none', 'mean' and 'sum'.\"\n","    )\n","\n","def categorical_focal_loss_with_logits(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor]=None, alpha=1., gamma=2., reduction: str = \"mean\"):\n","    ce_loss = F.cross_entropy(input, target, reduction=\"none\")\n","    pt = torch.exp(-ce_loss)\n","    loss = alpha * (1-pt)**gamma * ce_loss\n","\n","    if weight is not None:\n","        weight = weight[target.long()]\n","        loss *= weight\n","\n","    return apply_reduction(loss, reduction=reduction)\n","\n","\n","class Loss(Protocol):\n","    def __call__(self, outputs, targets: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n","        pass\n","\n","\n","class ComputeLoss(Protocol):\n","    def __call__(\n","        outputs, targets: Dict[str, torch.Tensor]\n","    ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","        pass\n","\n","\n","def wrap_loss_fn(\n","    name: str, loss_fn: Loss\n",") -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","    def loss(outputs, targets):\n","        loss_value = loss_fn(outputs, targets)\n","        return loss_value, {name: loss_value.item()}\n","\n","    return loss\n","\n","\n","@dataclass\n","class Criterion:\n","    name: str\n","    loss_fn: Loss\n","    weight: float = 1.0\n","\n","\n","class UncertaintyLoss(nn.Module, ComputeLoss):\n","    def __init__(self, name: str, loss_fn: Loss, initial_weight: float = 1.0) -> None:\n","        super(UncertaintyLoss, self).__init__()\n","        self.name = name\n","        self.loss_fn = loss_fn\n","        log_sigma_square = -np.log(initial_weight)\n","        self.log_sigma_square = nn.Parameter(\n","            torch.tensor(log_sigma_square, requires_grad=True, dtype=torch.float32)\n","        )\n","\n","    def forward(\n","        self, outputs, targets: Dict[str, torch.Tensor]\n","    ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","        inner_loss = self.loss_fn(outputs, targets)\n","        # 1/sigma^2 * L + 2 log sigma\n","        weight = torch.exp(-self.log_sigma_square)\n","        loss = weight * inner_loss + self.log_sigma_square\n","\n","        return loss, {\n","            self.name: inner_loss.item(),\n","            f\"{self.name}_weight\": weight.item(),\n","        }\n","\n","\n","def generative_loss(\n","    logits: torch.FloatTensor,\n","    labels: torch.IntTensor,\n","    reduction: str = \"mean\",\n","    mask: torch.Tensor = None,\n",") -> torch.FloatTensor:\n","    if mask is not None:\n","        logits = logits[mask.bool()]\n","        labels = labels[mask.bool()]\n","\n","    # swap seq_length with vocabulary dimension\n","    logits = torch.transpose(logits, 1, 2)  # batch_size x seq_length x vocab\n","    loss = F.cross_entropy(\n","        input=logits, target=labels, reduction=\"none\"\n","    )  # batch_size x seq_length\n","    n_tokens_per_sample = torch.sum(labels != -100, dim=-1)  # batch_size\n","    n_tokens_per_sample = torch.clamp(n_tokens_per_sample, min=_EPSILON)\n","    loss = torch.sum(loss, dim=-1) / n_tokens_per_sample  # batch_size\n","    loss = apply_reduction(loss, reduction=reduction)\n","    return loss\n","\n","\n","class EncoderDecoderGenerativeLoss(Loss):\n","    def __init__(self, reduction: str = \"mean\") -> None:\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"logits\"]\n","        labels = targets[\"labels\"]\n","\n","        return generative_loss(logits, labels, reduction=self.reduction, mask=mask)\n","\n","\n","def rationale_loss(\n","    logits: torch.FloatTensor,\n","    labels: torch.IntTensor,\n","    passage_mask: torch.IntTensor,\n","    max_rationale_length: int,\n","    reduction=\"mean\",\n","    mask: torch.Tensor = None,\n",") -> torch.FloatTensor:\n","    \"\"\"\n","    li = w * BCE(y_pred_i, y_true_i)\n","    , where w = w_positive if y_true_i is positive\n","            w = w_negative if y_true_i is negative\n","    w_positive = totals / positives\n","    w_negative = totals / negatives\n","    , where totals, positives and negatives are computed for each sequence\n","\n","    Ls = sum_i=1..seq_length li / sum(w_i)\n","    L = sum_s=1..N Ls / N,\n","    , where N is the #sequences whose rationale length is <= max_rationale_length\n","    \"\"\"\n","\n","    # rationale_logits = outputs[self.rationale_logits_name]\n","    # rationale_labels = targets[self.rationale_labels_name]\n","    # passage_mask = targets[self.passage_mask_name]\n","\n","    labels = labels * passage_mask\n","\n","    rationale_lengths = torch.sum(labels, dim=-1)  # batch_size\n","    valid_rationales = rationale_lengths <= max_rationale_length\n","    if mask is not None:\n","        valid_rationales = valid_rationales & mask.bool()\n","\n","    labels = labels[valid_rationales]\n","    passage_mask = passage_mask[valid_rationales]\n","    logits = logits[valid_rationales]\n","\n","    # n_sequences = torch.sum(valid_rationales)\n","\n","    totals = torch.sum(passage_mask, -1, keepdim=True)  # N x 1\n","    positives = torch.sum(labels, -1, keepdim=True)  # N x 1\n","    negatives = totals - positives  # N x 1\n","    totals = torch.clamp(totals, min=_EPSILON).float()\n","    weights = torch.where(\n","        labels == 1.0, totals / positives, totals / negatives\n","    )  # N x seq_length\n","    weights = torch.where(weights != torch.inf, weights, 0.0)  # N x seq_length\n","    weights = weights * passage_mask  # N x seq_length\n","    normalize_factor = torch.clamp(\n","        torch.sum(weights, dim=-1, keepdim=True), min=_EPSILON\n","    )\n","    weights = weights / normalize_factor  # N x seq_length\n","    # weights = weights * valid_rationales / n_sequences\n","\n","    # N x seq_length\n","    per_token_loss = F.binary_cross_entropy_with_logits(\n","        input=logits,\n","        target=labels,\n","        weight=weights,\n","        reduction=\"none\",\n","    )\n","\n","    loss = torch.sum(per_token_loss, dim=-1)  # N\n","    return apply_reduction(loss, reduction=reduction)\n","\n","\n","class EncoderDecoderRationaleLoss(Loss):\n","    def __init__(self, max_rationale_length: int, reduction: str = \"mean\") -> None:\n","        self.max_rationale_length = max_rationale_length\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"encoder_rationale_logits\"]\n","        labels = targets[\"rationale_labels\"]\n","        passage_mask = targets[\"passage_mask\"]\n","\n","        return rationale_loss(\n","            logits,\n","            labels,\n","            passage_mask,\n","            self.max_rationale_length,\n","            reduction=self.reduction,\n","            mask=mask,\n","        )\n","\n","\n","class EncoderRationaleLoss(Loss):\n","    def __init__(self, max_rationale_length: int, reduction: str = \"mean\") -> None:\n","        self.max_rationale_length = max_rationale_length\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"rationale_logits\"]\n","        labels = targets[\"rationale_labels\"]\n","        passage_mask = targets[\"passage_mask\"]\n","\n","        return rationale_loss(\n","            logits,\n","            labels,\n","            passage_mask,\n","            self.max_rationale_length,\n","            reduction=self.reduction,\n","            mask=mask,\n","        )\n","\n","\n","def yes_no_gen_loss(\n","    logits: torch.FloatTensor,\n","    labels: torch.IntTensor,\n","    weight: Optional[torch.FloatTensor] = None,\n","    reduction=\"mean\",\n","    mask: torch.Tensor = None,\n",") -> torch.FloatTensor:\n","    if mask is not None:\n","        logits = logits[mask.bool()]\n","        labels = labels[mask.bool()]\n","\n","    if weight is not None:\n","        weight.to(logits.device)\n","\n","    # loss = F.cross_entropy(logits, labels, weight=weight, reduction=reduction)\n","    loss = categorical_focal_loss_with_logits(logits, labels, weight=weight, reduction=reduction)\n","    return loss\n","\n","\n","class EncoderDecoderYNGLoss(Loss):\n","    def __init__(\n","        self, weight: Optional[torch.FloatTensor] = None, reduction: str = \"mean\"\n","    ) -> None:\n","        self.weight = weight\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"encoder_yng_logits\"]\n","        labels = targets[\"yng_label\"]\n","\n","        return yes_no_gen_loss(\n","            logits, labels, weight=self.weight, reduction=self.reduction, mask=mask\n","        )\n","\n","\n","class EncoderYNGLoss(Loss):\n","    def __init__(\n","        self, weight: Optional[torch.FloatTensor] = None, reduction: str = \"mean\"\n","    ) -> None:\n","        self.weight = weight\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"yng_logits\"]\n","        labels = targets[\"yng_label\"]\n","\n","        return yes_no_gen_loss(\n","            logits, labels, weight=self.weight, reduction=self.reduction, mask=mask\n","        )\n","\n","\n","class EncoderDecoderLoss(nn.Module):\n","    def __init__(\n","        self,\n","        max_rationale_length,\n","        yng_loss_weight=1.0,\n","        rationale_loss_weight=1.0,\n","        generative_loss_weight=1.0,\n","    ) -> None:\n","        super().__init__()\n","\n","        self.yng_loss_weight = yng_loss_weight\n","        self.rationale_loss_weight = rationale_loss_weight\n","        self.generative_loss_weight = generative_loss_weight\n","\n","        # weight = torch.Tensor([1 / 11.0, 1 / 9.0, 1 / 80.0])\n","        # weight = weight / torch.sum(weight)\n","        # weight = None\n","        self.yes_no_gen_loss_fn = EncoderDecoderYNGLoss()\n","        self.yes_no_gen_loss_fn = EncoderDecoderYNGLoss()\n","        self.rationale_loss_fn = EncoderDecoderRationaleLoss(\n","            max_rationale_length=max_rationale_length\n","        )\n","        self.generative_loss_fn = EncoderDecoderGenerativeLoss()\n","\n","    def forward(\n","        self, outputs, targets: Dict[str, torch.Tensor]\n","    ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","        is_generative = ~targets[\"yes_no\"].bool()\n","\n","        yng_loss = self.yes_no_gen_loss_fn(outputs, targets)\n","        rationale_loss = self.rationale_loss_fn(outputs, targets, mask=is_generative)\n","        generative_loss = self.generative_loss_fn(outputs, targets, mask=is_generative)\n","\n","        total_loss = (\n","            self.yng_loss_weight * yng_loss\n","            + self.rationale_loss_weight * rationale_loss\n","            + self.generative_loss_weight * generative_loss\n","        )\n","        loss_logs = {\n","            \"yng_loss\": yng_loss.item(),\n","            \"rationale_loss\": rationale_loss.item(),\n","            \"generative_loss\": generative_loss.item(),\n","        }\n","\n","        return total_loss, loss_logs\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:58.953421Z","iopub.status.busy":"2023-08-30T08:05:58.952656Z","iopub.status.idle":"2023-08-30T08:05:58.992329Z","shell.execute_reply":"2023-08-30T08:05:58.991275Z","shell.execute_reply.started":"2023-08-30T08:05:58.953381Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","\n","import numpy as np\n","\n","\n","import transformers\n","import datasets\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, Optional, Union\n","\n","class LinearScheduler:\n","    def __init__(\n","        self, total_iters:int, start_value:float=1., end_value:float=0., fraction=0.7\n","    ):\n","\n","        self.start_value = float(start_value)\n","        self.end_value = float(end_value)\n","        self.total_iters = total_iters\n","        self.fraction = fraction\n","        self._total_iters = fraction * total_iters\n","        self.current_step = 0\n","\n","    def step(self):\n","        self.current_step += 1\n","\n","    def get_value(self):\n","        if self.current_step > self._total_iters:\n","            return self.end_value\n","         \n","        return (\n","            self.start_value\n","            + (self.end_value - self.start_value)\n","            / self._total_iters\n","            * self.current_step\n","        )\n","\n","class DummyScheduler:\n","    def step(self):\n","        pass\n","\n","    def get_value(self):\n","        return 0.\n","\n","class DummyLRScheduler:\n","    def __init__(self, optimizer: torch.optim.Optimizer) -> None:\n","        self.optimizer = optimizer\n","\n","    def step(self):\n","        None\n","\n","    def get_last_lr(self):\n","        return [group[\"lr\"] for group in self.optimizer.param_groups]\n","\n","    def state_dict(self):\n","        return {}\n","\n","\n","@dataclass\n","class DynamicPaddingCollatorForSeq2Seq:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs received, as well as the labels.\n","\n","    Args:\n","        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n","            The tokenizer used for encoding the data.\n","        model ([`PreTrainedModel`]):\n","            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n","            prepare the *decoder_input_ids*\n","\n","            This is useful when using *label_smoothing* to avoid calculating loss twice.\n","        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","\n","            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n","              sequence is provided).\n","            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n","              acceptable input length for the model if that argument is not provided.\n","            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n","        max_length (`int`, *optional*):\n","            Maximum length of the returned list and optionally padding length (see above).\n","        pad_to_multiple_of (`int`, *optional*):\n","            If set will pad the sequence to a multiple of the provided value.\n","\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","        label_pad_token_id (`int`, *optional*, defaults to -100):\n","            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n","        return_tensors (`str`):\n","            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n","    \"\"\"\n","\n","    tokenizer: transformers.PreTrainedTokenizerBase\n","    model: Optional[Any] = None\n","    padding: Union[bool, str, transformers.utils.PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    label_pad_token_id: int = -100\n","    return_tensors: str = \"pt\"\n","\n","    def __call__(self, features, return_tensors=None):\n","        if return_tensors is None:\n","            return_tensors = self.return_tensors\n","\n","        # We have to pad the labels and other features not in  `tokenizer.model_input_names` before calling `tokenizer.pad`\n","        # as `tokenizer.pad` method will pad only features in `tokenizer.model_input_names`\n","        tokenizer_input_names = set(self.tokenizer.model_input_names)\n","        for feature_name in features[0].keys():\n","            if feature_name not in tokenizer_input_names and isinstance(\n","                features[0][feature_name], list\n","            ):\n","                if feature_name.endswith(\"labels\"):\n","                    self.pad_feature(feature_name, features, self.label_pad_token_id)\n","                else:\n","                    self.pad_feature(feature_name, features)\n","\n","        features = self.tokenizer.pad(\n","            features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=return_tensors,\n","        )\n","\n","        # prepare decoder_input_ids\n","        if (\n","            \"labels\" in features\n","            and \"decoder_input_ids\" not in features\n","            and self.model is not None\n","            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n","        ):\n","            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(\n","                labels=features[\"labels\"]\n","            )\n","            features[\"decoder_input_ids\"] = decoder_input_ids\n","\n","        return features\n","\n","    def pad_feature(self, feature_name, features, pad_id=0):\n","        items = (\n","            [feature[feature_name] for feature in features]\n","            if feature_name in features[0].keys()\n","            else None\n","        )\n","        # We have to pad the feature before calling `tokenizer.pad` as this method won't pad them and needs them of the\n","        # same length to return tensors.\n","        if items is not None:\n","            max_item_length = max(len(l) for l in items)\n","            if self.pad_to_multiple_of is not None:\n","                max_item_length = (\n","                    (max_item_length + self.pad_to_multiple_of - 1)\n","                    // self.pad_to_multiple_of\n","                    * self.pad_to_multiple_of\n","                )\n","\n","            padding_side = self.tokenizer.padding_side\n","            for feature in features:\n","                remainder = [pad_id] * (max_item_length - len(feature[feature_name]))\n","                if isinstance(feature[feature_name], list):\n","                    feature[feature_name] = (\n","                        feature[feature_name] + remainder\n","                        if padding_side == \"right\"\n","                        else remainder + feature[feature_name]\n","                    )\n","                elif padding_side == \"right\":\n","                    feature[feature_name] = np.concatenate(\n","                        [feature[feature_name], remainder]\n","                    )\n","                else:\n","                    feature[feature_name] = np.concatenate(\n","                        [remainder, feature[feature_name]]\n","                    )\n","\n","\n","def save_checkpoint(\n","    model, optimizer, scheduler, epoch, step, checkpoint_counter, checkpoint_path\n","):\n","    checkpoint = {\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","        \"scheduler_state_dict\": scheduler.state_dict(),\n","        \"epoch\": epoch,\n","        \"step\": step,\n","        \"checkpoint_counter\": checkpoint_counter,\n","    }\n","\n","    torch.save(checkpoint, checkpoint_path)\n","\n","\n","def load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None):\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","    if scheduler is not None:\n","        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","    epoch = checkpoint[\"epoch\"]\n","    step = checkpoint[\"step\"]\n","    checkpoint_counter = checkpoint[\"checkpoint_counter\"]\n","\n","    print(f\"Loaded checkpoint from '{checkpoint_path}'\")\n","\n","    return model, optimizer, scheduler, epoch, step, checkpoint_counter\n","\n","def prepare_inputs_for_train(\n","    dataset: datasets.DatasetDict,\n","    checkpoints: Dict[str, str],\n","    filename_fn,\n","    add_history=False,\n","    num_processes=None,\n","    verbose=True,\n","    **preprocessing_kwargs,\n","):\n","    for name, checkpoint in checkpoints.items():\n","        tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n","        preprocessing = CoQADatasetPreprocessing(tokenizer, **preprocessing_kwargs)\n","\n","        if verbose:\n","            print(\"Preparing inputs for\", name, \"...\")\n","\n","        if not os.path.exists(filename_fn(name)):\n","            dataset_ = dataset.map(\n","                preprocessing.process_data_to_model_inputs,\n","                fn_kwargs={\"add_history\": add_history},\n","                batched=True,\n","                remove_columns=dataset[\"train\"].column_names,\n","                num_proc=num_processes,\n","            )\n","\n","            dataset_.save_to_disk(filename_fn(name))\n","            del dataset_\n","\n","        if verbose:\n","            dataset_ = datasets.load_from_disk(filename_fn(name))\n","            print(dataset_)\n","            print()\n","            print(\"Showing some input examples:\")\n","            decoded_inputs = tokenizer.batch_decode(dataset_[\"train\"][:5][\"input_ids\"])\n","            for decoded in decoded_inputs:\n","                print(decoded)\n","            print()\n","            del dataset_"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:58.995065Z","iopub.status.busy":"2023-08-30T08:05:58.993938Z","iopub.status.idle":"2023-08-30T08:05:59.010962Z","shell.execute_reply":"2023-08-30T08:05:59.009793Z","shell.execute_reply.started":"2023-08-30T08:05:58.995024Z"},"trusted":true},"outputs":[],"source":["\"\"\"Functions taken from [the official evaluation script]\n","(https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/)\n","for SQuAD version 2.0.\n","\"\"\"\n","\n","import collections\n","import re\n","import string\n","\n","\n","def normalize_answer(s):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","\n","    def remove_articles(text):\n","        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n","        return re.sub(regex, \" \", text)\n","\n","    def white_space_fix(text):\n","        return \" \".join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return \"\".join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","\n","def get_tokens(s):\n","    if not s:\n","        return []\n","    return normalize_answer(s).split()\n","\n","\n","def compute_f1(a_gold, a_pred):\n","    gold_toks = get_tokens(a_gold)\n","    pred_toks = get_tokens(a_pred)\n","    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","    num_same = sum(common.values())\n","    if len(gold_toks) == 0 or len(pred_toks) == 0:\n","        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n","        return float(gold_toks == pred_toks)\n","    if num_same == 0:\n","        return 0.\n","    precision = 1.0 * num_same / len(pred_toks)\n","    recall = 1.0 * num_same / len(gold_toks)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","\n","def squad_f1(targets, predictions):\n","    f1 = 0.0\n","    for a_gold, a_pred in zip(targets, predictions):\n","        f1 += compute_f1(a_gold, a_pred)\n","    return f1 / len(targets)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:59.013342Z","iopub.status.busy":"2023-08-30T08:05:59.012738Z","iopub.status.idle":"2023-08-30T08:05:59.710401Z","shell.execute_reply":"2023-08-30T08:05:59.709333Z","shell.execute_reply.started":"2023-08-30T08:05:59.013302Z"},"trusted":true},"outputs":[],"source":["from typing import List, Optional, Union\n","import numpy as np\n","\n","import torch\n","from torchmetrics.classification import MulticlassF1Score\n","\n","import datasets\n","from accelerate import Accelerator\n","\n","\n","per_token_f1_metric = MulticlassF1Score(\n","    num_classes=2,\n","    average=\"macro\",\n","    multidim_average=\"samplewise\",\n","    ignore_index=-100,\n",")\n","\n","macro_f1 = MulticlassF1Score(\n","    num_classes=3,\n","    average=\"macro\",\n","    ignore_index=-100,\n",")\n","\n","\n","def labels_to_answer(labels: torch.Tensor, tokenizer, ignore_index=-100) -> str:\n","    labels[labels == ignore_index] = tokenizer.pad_token_id\n","    answer = tokenizer.decode(labels, skip_special_tokens=True)\n","    return answer\n","\n","\n","def pad_input_tensors(inputs, collator):\n","    features = [dict(zip(inputs.keys(), values)) for values in zip(*inputs.values())]\n","    features = collator(features)\n","\n","    return features\n","\n","\n","def evaluate_answer(example: dict):\n","    return {\"answer_f1\": compute_f1(example[\"answer\"], example[\"pred_answer\"])}\n","\n","\n","def evaluate_rationale_f1(example: dict):\n","    rationale_f1 = per_token_f1_metric(\n","        example[\"pred_rationale_labels\"].long(),\n","        example[\"rationale_labels\"].long(),\n","    )\n","    # Ensure it is an array, not a scalar\n","    if rationale_f1.dim() == 0:\n","        rationale_f1.unsqueeze_(dim=0)\n","    return {\"rationale_f1\": rationale_f1}\n","\n","\n","def evaluate_model(model, tokenizer, dataset: datasets.Dataset, config):\n","    accelerator = Accelerator(mixed_precision=config.mixed_precision, cpu=config.cpu)\n","    model = accelerator.prepare(model)\n","    model.eval()\n","\n","    collator = DynamicPaddingCollatorForSeq2Seq(tokenizer, model)\n","\n","    dataset = dataset.map(\n","        lambda example: pad_input_tensors(example, collator),\n","        batched=True,\n","        batch_size=config.generate_batch_size,\n","        load_from_cache_file=False,\n","    )\n","\n","    dataset = dataset.with_format(\"torch\", device=model.device)\n","\n","    dataset = dataset.map(\n","        lambda example: generate_answer(model, tokenizer, example),\n","        batched=True,\n","        batch_size=config.generate_batch_size,\n","        load_from_cache_file=False,\n","    )\n","\n","    dataset = dataset.map(\n","        lambda example: {\n","            \"answer\": labels_to_answer(example[\"labels\"], tokenizer=tokenizer)\n","        },\n","        load_from_cache_file=False,\n","    )\n","\n","    dataset = dataset.map(evaluate_answer, load_from_cache_file=False)\n","    dataset = dataset.map(\n","        evaluate_rationale_f1,\n","        batched=True,\n","        batch_size=config.generate_batch_size,\n","        load_from_cache_file=False,\n","    )\n","    macro_f1_ = macro_f1.to(model.device)\n","    yng_f1 = macro_f1_(dataset[\"pred_yng_label\"], dataset[\"yng_label\"]).item()\n","\n","    rationale_f1 = torch.mean(dataset[\"rationale_f1\"]).item()\n","    answer_squad_f1 = torch.mean(dataset[\"answer_f1\"]).item()\n","\n","    dataset.reset_format()\n","    return dataset, {\n","        \"yng_f1\": yng_f1,\n","        \"rationale_f1\": rationale_f1,\n","        \"answer_squad_f1\": answer_squad_f1,\n","    }\n","\n","\n","def evaluate_model_raw_data(model, tokenizer, data):\n","    model.eval()\n","    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    preprocessing = CoQADatasetPreprocessing(tokenizer, **CONFIG.preprocessing.__dict__)\n","\n","    outputs = data.map(\n","        lambda example: generate_answer_from_raw_data(\n","            model, tokenizer, preprocessing, example[\"passage\"], example[\"question\"]\n","        ),\n","        batched=True,\n","        batch_size=32,\n","    )\n","\n","    outputs = outputs.map(evaluate_answer)\n","\n","    return outputs.select_columns(\n","        [\n","            \"source\",\n","            \"passage\",\n","            \"question\",\n","            \"rationale\",\n","            \"answer\",\n","            \"pred_answer\",\n","            \"answer_type\",\n","            \"answer_squad_f1\",\n","        ]\n","    )\n","\n","\n","def generate_answer_from_raw_data(\n","    model,\n","    tokenizer,\n","    preprocessing,\n","    passage: Union[str, List[str]],\n","    question: Union[str, List[str]],\n","    history: Optional[Union[str, List[str]]] = None,\n",") -> List[str]:\n","    use_history = history is not None\n","    preprocess = batched_function(preprocessing.preprocess_texts)\n","    if isinstance(passage, str):\n","        passage = [passage]\n","        question = [question]\n","        history = [history]\n","\n","    inputs = {\n","        \"id\": list(range(len(passage))),\n","        \"passage\": passage,\n","        \"question\": question,\n","    }\n","    if use_history:\n","        inputs[\"history\"] = history\n","\n","    inputs = preprocess(inputs)\n","    inputs = preprocessing.process_data_to_model_inputs(\n","        inputs, add_history=use_history, padding=\"max_length\"\n","    )\n","    inputs = inputs.convert_to_tensors(\"pt\")\n","\n","    return generate_answer(model, tokenizer, inputs)\n","\n","\n","def generate_answer(model, tokenizer, inputs):\n","    encoder = model.get_encoder()\n","    encoder_inputs = prepare_model_inputs(encoder, inputs)\n","\n","    inputs = prepare_model_inputs(model, inputs)\n","    inputs.pop(\"decoder_input_ids\", None)\n","\n","    with torch.no_grad():\n","        encoder_outputs = model.encoder(**encoder_inputs, return_dict=True)\n","        # output_str = np.zeros(\n","        #     encoder_outputs[\"last_hidden_state\"].shape[0], dtype=np.string_\n","        # )\n","        outputs = model.generate(**inputs)\n","\n","    answer_types = logits_to_class(encoder_outputs[\"yng_logits\"], task=\"multiclass\")\n","    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    for i in range(len(output_str)):\n","        if answer_types[i] != 2:\n","            output_str[i] = idx_to_answer(answer_types[i])\n","\n","    return {\n","        \"pred_answer\": output_str,\n","        \"yng_logits\": encoder_outputs[\"yng_logits\"],\n","        \"pred_yng_label\": logits_to_class(\n","            encoder_outputs[\"yng_logits\"], task=\"multiclass\"\n","        ),\n","        \"rationale_logits\": encoder_outputs[\"rationale_logits\"],\n","        \"pred_rationale_labels\": logits_to_class(\n","            encoder_outputs[\"rationale_logits\"], task=\"binary\"\n","        ),\n","    }\n"]},{"cell_type":"markdown","metadata":{},"source":["# Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:59.713893Z","iopub.status.busy":"2023-08-30T08:05:59.713021Z","iopub.status.idle":"2023-08-30T08:05:59.776028Z","shell.execute_reply":"2023-08-30T08:05:59.775107Z","shell.execute_reply.started":"2023-08-30T08:05:59.713838Z"},"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","import gc\n","import inspect\n","import os\n","import torch\n","import torch.nn as nn\n","\n","from tqdm import tqdm\n","\n","\n","import wandb\n","import transformers\n","import datasets\n","from accelerate import Accelerator\n","\n","from typing import Dict, Tuple, Union\n","\n","def make_model(config):\n","    checkpoint = CONFIG.checkpoints.__dict__[config.checkpoint_name]\n","    if config.model_type == \"encoder_decoder\":\n","        return make_encoder_decoder_model(\n","            checkpoint=checkpoint,\n","            decoder_max_length=CONFIG.decoder_max_length,\n","            generation_kwargs=CONFIG.generation,\n","            initialize_cross_attention=config.initialize_cross_attention,\n","        )\n","    if config.model_type == \"encoder\":\n","        return make_qa_encoder(checkpoint=checkpoint)\n","\n","    raise ValueError(\n","        \"Invalid model_type. Supported values are 'encoder_decoder' and 'encoder'.\"\n","    )\n","\n","\n","def get_data(split: str, config):\n","    path = CONFIG.dataset.train(\n","        config.checkpoint_name,\n","        history=config.get(\"add_history\", False),\n","        split=split\n","    )\n","\n","    dataset = datasets.load_from_disk(path)\n","    dataset = dataset.remove_columns(\n","        [\"id\", \"turn\", \"offset_mapping\", \"rationale_start\", \"rationale_end\"]\n","    )\n","    return dataset\n","\n","\n","def make_dataloader(dataset, tokenizer, config, split: str):\n","    data_collator = DynamicPaddingCollatorForSeq2Seq(tokenizer)\n","    dataloader = create_reproducible_dataloader(\n","        dataset,\n","        batch_size=config.val_batch_size\n","        if split != \"train\" and \"val_batch_size\" in config\n","        else config.batch_size,\n","        collate_fn=data_collator,\n","        num_workers=config.val_num_workers\n","        if split != \"train\" and \"val_num_workers\" in config\n","        else config.num_workers,\n","        pin_memory=True,\n","        shuffle=True,\n","    )\n","    return dataloader\n","\n","\n","def make_loss(config) -> ComputeLoss:\n","    if config.model_type == \"encoder_decoder\":\n","\n","        loss = EncoderDecoderLoss(\n","            max_rationale_length=CONFIG.rationale_max_length,\n","            yng_loss_weight=config.yng_loss_weight,\n","            rationale_loss_weight=config.rationale_loss_weight,\n","            generative_loss_weight=config.generative_loss_weight,\n","        )\n","\n","    elif config.model_type == \"encoder\":\n","        loss = EncoderRationaleLoss(max_rationale_length=CONFIG.rationale_max_length)\n","    else:\n","        raise ValueError(\n","            \"Invalid model_type. Supported values are 'encoder_decoder' and 'encoder'.\"\n","        )\n","    return loss\n","\n","\n","def make_optimizer(model, loss_fn, config):\n","    optimizer_cls = getattr(torch.optim, config.optimizer_name)\n","    parameters = [{\"params\": model.parameters()}]\n","\n","    if hasattr(loss_fn, \"_parameters\"):\n","        loss_params = {\"params\": loss_fn.parameters()}\n","        if \"loss_learning_rate\" in config:\n","            loss_params[\"lr\"] = config.loss_learning_rate\n","        parameters.append(loss_params)\n","\n","    return optimizer_cls(\n","        parameters,\n","        lr=config.learning_rate,\n","        **config.get(\"optimizer_args\", {}),\n","    )\n","\n","\n","def make_scheduler(optimizer, steps_per_epoch, config):\n","    total_steps = steps_per_epoch * config.num_epochs\n","    warmup_steps = int(config.warmup_fraction * total_steps)\n","    if config.get(\"scheduler\", \"none\") != \"none\":\n","        return transformers.get_scheduler(\n","            config.scheduler,\n","            optimizer=optimizer,\n","            num_warmup_steps=warmup_steps,\n","            num_training_steps=total_steps,\n","        )\n","\n","    return DummyLRScheduler(optimizer=optimizer)\n","\n","\n","def make_teacher_force_scheduler(steps_per_epoch, config):\n","    total_steps = steps_per_epoch * config.num_epochs\n","    if config.get(\"teacher_force_scheduler\", \"none\") != \"none\":\n","        return LinearScheduler(\n","            start_value=config.tf_start,\n","            end_value=config.tf_end,\n","            total_iters=total_steps,\n","            fraction=config.tf_fraction,\n","        )\n","\n","    return DummyScheduler()\n","\n","\n","def train(\n","    model: nn.Module,\n","    train_dataloader: torch.utils.data.DataLoader,\n","    val_dataloader: torch.utils.data.DataLoader,\n","    loss_fn: Union[ComputeLoss, nn.Module],\n","    optimizer: torch.optim.Optimizer,\n","    lr_scheduler,\n","    config,\n","    teacher_force_scheduler=None,\n","    # metrics: Dict[str, Metric] = {},\n","):\n","    watch_list = [model]\n","\n","    accelerator = Accelerator(mixed_precision=config.mixed_precision, cpu=config.cpu)\n","    (\n","        model,\n","        optimizer,\n","        train_dataloader,\n","        val_dataloader,\n","        lr_scheduler,\n","    ) = accelerator.prepare(\n","        model, optimizer, train_dataloader, val_dataloader, lr_scheduler\n","    )\n","    if isinstance(loss_fn, nn.Module):\n","        watch_list.append(loss_fn)\n","        loss_fn = accelerator.prepare(loss_fn)\n","\n","    wandb.watch(watch_list, log=\"all\", log_freq=config.log_interval)\n","\n","    # Run training and track with wandb\n","    steps_per_epoch = len(train_dataloader)\n","    total_steps = steps_per_epoch * config.num_epochs\n","\n","    checkpoint_counter = 0\n","    step = 0\n","    avg_loss = AvgValue()\n","    avg_inner_losses = defaultdict(AvgValue)\n","    model.train()\n","\n","    forward_signature = set(inspect.signature(model.forward).parameters)\n","    progress_bar = tqdm(range(total_steps))\n","    for epoch in range(config.num_epochs):\n","        for data in train_dataloader:\n","            inputs = {\n","                argument: value\n","                for argument, value in data.items()\n","                if argument in forward_signature\n","            }\n","\n","            lr = lr_scheduler.get_last_lr()[0]\n","            tf = teacher_force_scheduler.get_value() if teacher_force_scheduler is not None else 0.\n","            loss, inner_losses = train_batch(\n","                inputs=inputs,\n","                data=data,\n","                step=step,\n","                model=model,\n","                loss_fn=loss_fn,\n","                optimizer=optimizer,\n","                lr_scheduler=lr_scheduler,\n","                teacher_force_scheduler=teacher_force_scheduler,\n","                accelerator=accelerator,\n","                config=config,\n","            )\n","            progress_bar.update(1)\n","\n","            # Compute statistics\n","            n_samples = len(next(iter(data.values())))\n","            step += 1\n","            avg_loss.update(loss, n_samples)\n","            for loss_name, loss_value in inner_losses.items():\n","                avg_inner_losses[f\"avg_{loss_name}\"].update(loss_value, n_samples)\n","\n","            wandb.log(\n","                {\n","                    \"train_loss\": loss,\n","                    **inner_losses,\n","                    \"lr\": lr,\n","                    \"teacher_force\": tf,\n","                },\n","                step=step,\n","            )\n","\n","            # Evaluate the model and save checkpoints\n","            if (step % config.log_interval == 0) or (step == total_steps):\n","                # Evaluate the model\n","                val_loss, val_inner_losses, val_metrics = train_evaluation(\n","                    model,\n","                    val_dataloader,\n","                    loss_fn,\n","                    # metrics=metrics,\n","                )\n","                model.train()\n","\n","                train_log(\n","                    avg_loss,\n","                    avg_inner_losses,\n","                    val_loss,\n","                    val_inner_losses,\n","                    val_metrics,\n","                    lr=lr,\n","                    teacher_force=tf,\n","                    step=step,\n","                )\n","                avg_loss = AvgValue()\n","                avg_inner_losses = defaultdict(AvgValue)\n","\n","            if (step % config.checkpoint_interval == 0) or (step == total_steps):\n","                # Saving checkpoint\n","                save_model_checkpoint(\n","                    accelerator.unwrap_model(model),\n","                    optimizer,\n","                    lr_scheduler,\n","                    epoch,\n","                    step,\n","                    checkpoint_counter,\n","                    config,\n","                )\n","                wandb.log(\n","                    {\n","                        \"checkpoint_counter\": checkpoint_counter,\n","                    },\n","                    step=step,\n","                )\n","                checkpoint_counter += 1\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","    wandb.unwatch(watch_list)\n","    accelerator.free_memory()\n","\n","\n","def train_batch(\n","    inputs,\n","    data,\n","    step,\n","    model,\n","    loss_fn,\n","    optimizer,\n","    lr_scheduler,\n","    config,\n","    teacher_force_scheduler=None,\n","    accelerator=None,\n","    device=None,\n","):\n","    assert (\n","        accelerator is not None or device is not None\n","    ), \"One between accelerator and device must be set.\"\n","\n","    if accelerator is None:\n","        data = {key: value.to(device) for key, value in data.items()}\n","\n","    outputs = model(\n","        **inputs, teacher_force=teacher_force_scheduler.get_value(), return_dict=True\n","    )\n","\n","    loss, inner_losses = loss_fn(outputs, data)\n","    if accelerator is not None:\n","        accelerator.backward(loss)\n","    else:\n","        loss.backward()\n","\n","    if config.get(\"gradient_clip\", \"none\") != \"none\":\n","        if accelerator is not None and accelerator.sync_gradients:\n","            accelerator.clip_grad_norm_(model.parameters(), config.gradient_clip)\n","        else:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n","\n","    if step % config.accumulation_steps == 0:\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    lr_scheduler.step()\n","    teacher_force_scheduler.step()\n","\n","    return loss.item(), inner_losses\n","\n","\n","def train_evaluation(\n","    model,\n","    dataloader,\n","    compute_loss: ComputeLoss = None\n","    # , metrics: Dict[str, Metric] = {}\n",") -> Tuple[AvgValue, Dict[str, AvgValue], Dict[str, AvgValue]]:\n","    model.eval()\n","    avg_loss = AvgValue()\n","    avg_inner_losses = defaultdict(AvgValue)\n","    avg_metrics = defaultdict(AvgValue)\n","\n","    forward_signature = set(inspect.signature(model.forward).parameters)\n","    with torch.no_grad():\n","        for data in dataloader:\n","            inputs_kwargs = {\n","                argument: value\n","                for argument, value in data.items()\n","                if argument in forward_signature\n","            }\n","            n_samples = len(next(iter(data.values())))\n","\n","            outputs = model(**inputs_kwargs, return_dict=True)\n","            if compute_loss is not None:\n","                loss, inner_losses = compute_loss(outputs, data)\n","\n","                avg_loss.update(loss.item(), n_samples)\n","                for loss_name, loss_value in inner_losses.items():\n","                    avg_inner_losses[loss_name].update(loss_value, n_samples)\n","\n","            # for metric_name, metric in metrics.items():\n","            #     metric_value = metric(outputs, data)\n","            #     avg_metrics[metric_name].update(metric_value, n_samples)\n","\n","    return avg_loss, avg_inner_losses, avg_metrics\n","\n","\n","def train_log(\n","    train_loss: AvgValue,\n","    train_inner_losses: Dict[str, AvgValue],\n","    val_loss: AvgValue,\n","    val_inner_losses: Dict[str, AvgValue],\n","    val_metrics: Dict[str, AvgValue],\n","    lr,\n","    step,\n","    teacher_force,\n","):\n","    train_loss = train_loss.value()\n","    train_inner_losses = {\n","        f\"{loss_name}\": loss_value.value()\n","        for loss_name, loss_value in train_inner_losses.items()\n","    }\n","\n","    val_loss = val_loss.value()\n","    val_inner_losses = {\n","        f\"val_{loss_name}\": loss_value.value()\n","        for loss_name, loss_value in val_inner_losses.items()\n","    }\n","\n","    val_metrics = {\n","        f\"val_{metric_name}\": metric_value.value()\n","        for metric_name, metric_value in val_metrics.items()\n","    }\n","\n","    wandb.log(\n","        {\n","            \"avg_train_loss\": train_loss,\n","            **train_inner_losses,\n","            \"val_loss\": val_loss,\n","            **val_inner_losses,\n","            **val_metrics,\n","            \"lr\": lr,\n","            \"teacher_force\": teacher_force,\n","        },\n","        step=step,\n","    )\n","    print(\n","        f\"Iteration: {step:6}\",\n","        f\"train loss: {train_loss:.4f}\",\n","        f\"val loss: {val_loss:.4f}\",\n","        f\"lr: {lr:.6f}\",\n","        sep=\"\\t\",\n","    )\n","\n","\n","def save_model_checkpoint(\n","    model, optimizer, lr_scheduler, epoch, step, checkpoint_counter, config\n","):\n","    checkpoint_dir = CONFIG.models.checkpoints_dir(\n","        config.model_name, config.get(\"add_history\", False), seed=config.seed\n","    )\n","    filename = f\"checkpoint_{checkpoint_counter}.pt\"\n","    checkpoint_file = os.path.join(checkpoint_dir, filename)\n","\n","    create_dirs_for_file(checkpoint_file)\n","    save_checkpoint(\n","        model,\n","        optimizer,\n","        lr_scheduler,\n","        epoch,\n","        step,\n","        checkpoint_counter,\n","        checkpoint_path=checkpoint_file,\n","    )\n","    wandb.save(checkpoint_file)\n","\n","\n","def load_model_checkpoint(\n","    checkpoint_counter, config, model, optimizer=None, lr_scheduler=None\n","):\n","    checkpoint_file = os.path.join(\n","        CONFIG.models.checkpoints_dir(\n","            config.model_name, config.get(\"add_history\", False), seed=config.seed\n","        ),\n","        f\"checkpoint_{checkpoint_counter}.pt\",\n","    )\n","    return load_checkpoint(\n","        checkpoint_file, model, optimizer=optimizer, scheduler=lr_scheduler\n","    )\n","\n","\n","def evaluate(model, tokenizer, train_data: datasets.Dataset, val_data: datasets.Dataset, test_data: datasets.Dataset, config):\n","    datasets = [(\"train\", train_data), (\"val\", val_data), (\"test\", test_data)]\n","    results = {}\n","    for dataset_name, dataset in datasets:\n","        print(f\"eval  {dataset_name}\")\n","        outputs, metrics = evaluate_model(model, tokenizer, dataset, config)\n","        results[dataset_name] = (outputs, metrics)\n","\n","        for metric_name, metric_value in metrics.items():\n","            print(f\"{dataset_name}_{metric_name}: {metric_value:.4f}\")\n","            wandb.log({f\"evaluation/{dataset_name}_{metric_name}\": metric_value})\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["# Run"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-30T08:05:59.777823Z","iopub.status.busy":"2023-08-30T08:05:59.777451Z"},"trusted":true},"outputs":[],"source":["class PropertyDict(dict):\n","    def __getattr__(self, key):\n","        if key in self:\n","            return self[key]\n","        raise AttributeError(\n","            f\"'{self.__class__.__name__}' object has no attribute '{key}'\"\n","        )\n","\n","    def __setattr__(self, key, value):\n","        self[key] = value\n","\n","\n","hyperparameters = PropertyDict(\n","    seed=2022,\n","    add_history=True,\n","    checkpoint_name=\"distil_roberta\",\n","    model_name=\"distil_roberta\",\n","    model_type=\"encoder_decoder\",\n","    initialize_cross_attention=True,\n","    yng_loss_weight=0.6,\n","    rationale_loss_weight=0.8,\n","    generative_loss_weight=0.2,\n","    batch_size=32,\n","    val_batch_size=64,\n","    generate_batch_size=32,\n","    num_workers=2,\n","    num_epochs=3,\n","    optimizer_name=\"AdamW\",\n","    learning_rate=2e-4,\n","    scheduler=\"linear\",\n","    warmup_fraction=0.1,\n","    teacher_force_scheduler=\"linear\",\n","    tf_start = 1.,\n","    tf_end = 0.,\n","    tf_fraction = 0.6,\n","    accumulation_steps=1,\n","    gradient_clip=1.0,\n","    mixed_precision=\"fp16\",\n","    checkpoint_interval=700,\n","    log_interval=700,\n","    cpu=False,\n",")\n","\n","with wandb.init(project=CONFIG.wandbConfig.project, config=hyperparameters):\n","    config = wandb.config\n","\n","    set_seed(config.seed)\n","\n","    # Make the model\n","    tokenizer, model = make_model(config)\n","\n","    # Make the data\n","    train_data = get_data(\"train\", config)\n","    val_data = get_data(\"validation\", config)\n","    train_dataloader = make_dataloader(train_data, tokenizer, config, split=\"train\")\n","    val_dataloader = make_dataloader(val_data, tokenizer, config, split=\"validation\")\n","\n","    # Make the loss, the optimizer and the scheduler\n","    loss_fn = make_loss(config)\n","    optimizer = make_optimizer(model, loss_fn, config)\n","    scheduler = make_scheduler(\n","        optimizer, steps_per_epoch=len(train_dataloader), config=config\n","    )\n","    tf_scheduler = make_teacher_force_scheduler(steps_per_epoch=len(train_dataloader), config=config)\n","\n","    # model, train_dataloader, val_dataloader, loss_fn, optimizer, scheduler, metrics = make(config)\n","    print(model)\n","\n","    train(\n","        model,\n","        train_dataloader,\n","        val_dataloader,\n","        loss_fn,\n","        optimizer,\n","        scheduler,\n","        config,\n","        teacher_force_scheduler=tf_scheduler,\n","    )\n","\n","#     results = evaluate(model, tokenizer, train_data, val_data, config)\n","\n","# pipeline(hyperparameters)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
