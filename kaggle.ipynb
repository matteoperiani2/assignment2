{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:53:31.627490Z","iopub.status.busy":"2023-08-02T15:53:31.627180Z","iopub.status.idle":"2023-08-02T15:53:58.780541Z","shell.execute_reply":"2023-08-02T15:53:58.779222Z","shell.execute_reply.started":"2023-08-02T15:53:31.627461Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install accelerate==0.19.0 datasets==2.12.0 transformers==4.29.2 evaluate==0.4.0 scikit-learn text2num tokenizers torch>=2.0.0 torchmetrics tqdm wandb "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:53:58.783332Z","iopub.status.busy":"2023-08-02T15:53:58.782923Z","iopub.status.idle":"2023-08-02T15:54:12.037266Z","shell.execute_reply":"2023-08-02T15:54:12.036268Z","shell.execute_reply.started":"2023-08-02T15:53:58.783293Z"},"trusted":true},"outputs":[],"source":["import os, urllib.request, inspect, functools, collections, gc\n","from tqdm.auto import tqdm\n","from typing import List, Optional, Union\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","from torch.optim import AdamW\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","\n","import transformers, datasets\n","from transformers import AutoTokenizer, EncoderDecoderModel\n","\n","import wandb\n","\n","# keep datasets in memory if < 8 GB\n","datasets.config.IN_MEMORY_MAX_SIZE = 8 * 1024**3"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:54:12.039135Z","iopub.status.busy":"2023-08-02T15:54:12.038781Z","iopub.status.idle":"2023-08-02T15:57:46.700132Z","shell.execute_reply":"2023-08-02T15:57:46.699077Z","shell.execute_reply.started":"2023-08-02T15:54:12.039083Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimonemele999\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:46.703879Z","iopub.status.busy":"2023-08-02T15:57:46.703112Z","iopub.status.idle":"2023-08-02T15:57:46.711594Z","shell.execute_reply":"2023-08-02T15:57:46.710417Z","shell.execute_reply.started":"2023-08-02T15:57:46.703848Z"},"trusted":true},"outputs":[],"source":["from enum import Enum, auto\n","\n","\n","class AnswerType(Enum):\n","    UNKNOWN = auto()\n","    SPAN = auto()\n","    YES_NO = auto()\n","    FLUENCY = auto()\n","    COUNTING = auto()\n","    MULTIPLE_CHOICE = auto()\n","\n","    def __str__(self):\n","        return self.name.lower()\n","\n","    @classmethod\n","    def list(cls, return_unknown=True):\n","        return [str(c) for c in cls if return_unknown or c != AnswerType.UNKNOWN]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:46.714260Z","iopub.status.busy":"2023-08-02T15:57:46.713486Z","iopub.status.idle":"2023-08-02T15:57:46.736904Z","shell.execute_reply":"2023-08-02T15:57:46.735542Z","shell.execute_reply.started":"2023-08-02T15:57:46.714224Z"},"trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","import os\n","from typing import Optional\n","\n","\n","@dataclass\n","class Config:\n","    @dataclass\n","    class Dataset:\n","        train_url: str = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n","        test_url: str = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n","\n","        # data_dir: str = os.path.join(\"..\", \"input\", \"dataset\", \"data\")\n","        data_dir: str = \"data\"\n","\n","        raw_dir: str = os.path.join(data_dir, \"raw\")\n","        train_data_raw: str = os.path.join(raw_dir, \"train.json\")\n","        test_data_raw: str = os.path.join(raw_dir, \"test.json\")\n","\n","        annotated_dir: str = os.path.join(data_dir, \"annotated\")\n","        train_data_annotated: str = os.path.join(annotated_dir, \"train.json\")\n","        test_data_annotated: str = os.path.join(annotated_dir, \"test.json\")\n","\n","        readable_dir: str = os.path.join(data_dir, \"readable\")\n","        train_data_readable: str = os.path.join(readable_dir, \"train.txt\")\n","        test_data_readable: str = os.path.join(readable_dir, \"test.txt\")\n","\n","        filtered_dir: str = os.path.join(data_dir, \"filtered\")\n","        splitted_dir: str = os.path.join(data_dir, \"splitted\")\n","        processed_dir: str = os.path.join(data_dir, \"processed\")\n","        train_dir: str = os.path.join(data_dir, \"train\")\n","\n","        def train_no_history(self, model_name: str, split=\"\") -> str:\n","            return os.path.join(self.train_dir, \"train_no_history\", model_name, split)\n","\n","        def train_with_history(self, model_name: str, split=\"\") -> str:\n","            return os.path.join(self.train_dir, \"train_with_history\", model_name, split)\n","\n","    class Checkpoints:\n","        def __init__(\n","            self, distil_roberta=\"distilroberta-base\", bert_tiny=\"prajjwal1/bert-tiny\"\n","        ) -> None:\n","            self.distil_roberta = distil_roberta\n","            self.bert_tiny = bert_tiny\n","\n","    class Models:\n","        def __init__(\n","            self,\n","            model_dir_name=\"models\",\n","            checkpoint_dir_name=\"checkpoints\",\n","            final_checkpoint_name=\"final.pt\",\n","        ) -> None:\n","            self.__model_dir = model_dir_name\n","            self.__checkpoints_dir_name = checkpoint_dir_name\n","            self.__final_checkpoint_name = final_checkpoint_name\n","\n","        def model_dir(self, model_name, history: Optional[bool] = None):\n","            if history is None:\n","                history_str = \"\"\n","            elif history:\n","                history_str = \"history\"\n","            else:\n","                history_str = \"no_history\"\n","            return os.path.join(self.__model_dir, model_name, history_str)\n","\n","        def checkpoints_dir(self, model_name, history: Optional[bool]):\n","            return os.path.join(\n","                self.model_dir(model_name, history=history), self.__checkpoints_dir_name\n","            )\n","\n","        def checkpoint(self, model_name, history: Optional[bool]):\n","            return os.path.join(\n","                self.model_dir(model_name, history=history),\n","                self.__final_checkpoint_name,\n","            )\n","\n","    @dataclass\n","    class Preprocessing:\n","        encoder_max_length: int\n","        decoder_max_length: int\n","        stride: int = 196\n","        use_window: bool = False\n","        max_history_length: int = 4\n","\n","    @dataclass\n","    class WandbConfig:\n","        \"\"\"Specify the parameters of `wandb.init`\"\"\"\n","\n","        project: str = \"nlp_assignment2\"\n","        entity: str = \"nlp_assignment2\"\n","\n","    dataset: Dataset = Dataset()\n","    checkpoints: Checkpoints = Checkpoints()\n","    models: Models = Models()\n","\n","    # remove all span answers longer than span_max_length words\n","    span_max_length: int = 37\n","    # ignore loss of rationales longer than rationale_max_length\n","    rationale_max_length: int = 150\n","\n","    encoder_max_length = 512\n","    # decoder_max_length = 350\n","    decoder_max_length = 64\n","\n","    preprocessing = Preprocessing(encoder_max_length, decoder_max_length)\n","    generation = dict(penalty_alpha=0.6, top_k=6)\n","\n","    wandbConfig = WandbConfig()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:46.738968Z","iopub.status.busy":"2023-08-02T15:57:46.738542Z","iopub.status.idle":"2023-08-02T15:57:46.751005Z","shell.execute_reply":"2023-08-02T15:57:46.750025Z","shell.execute_reply.started":"2023-08-02T15:57:46.738935Z"},"trusted":true},"outputs":[],"source":["CONFIG: Config = Config()"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:46.753196Z","iopub.status.busy":"2023-08-02T15:57:46.752763Z","iopub.status.idle":"2023-08-02T15:57:47.087220Z","shell.execute_reply":"2023-08-02T15:57:47.086277Z","shell.execute_reply.started":"2023-08-02T15:57:46.753161Z"},"trusted":true},"outputs":[],"source":["import inspect\n","import itertools\n","import os\n","import random\n","from typing import Dict, Literal\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import datasets\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","from text_to_num import text2num\n","\n","\n","\n","class AvgValue:\n","    def __init__(self, initial_value=0.0) -> None:\n","        self.__value = initial_value\n","        self.__last_value = initial_value\n","        self.__n = 0\n","\n","    def update(self, value, n=1):\n","        self.__last_value = value\n","        old_n = self.__n\n","        self.__n += n\n","        self.__value = old_n / self.__n * self.__value + n / self.__n * value\n","\n","    def value(self):\n","        return self.__value\n","\n","    @property\n","    def last_value(self):\n","        return self.__last_value\n","\n","    @property\n","    def n(self):\n","        return self.__n\n","\n","\n","################# Reproducibility ######################Ã \n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","\n","# Using a generator and the following function as `worker_init_fn` preserves reproducibility when using DataLoader\n","def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","\n","def create_reproducible_dataloader(*args, **kwargs):\n","    generator = torch.Generator()\n","    return DataLoader(\n","        *args,\n","        **kwargs,\n","        #   worker_init_fn=seed_worker,\n","        #   generator=generator\n","    )\n","\n","\n","###############################################################\n","\n","\n","def create_dirs_for_file(file_path):\n","    dir = os.path.dirname(file_path)\n","    ensure_dir_exists(dir)\n","\n","\n","def ensure_dir_exists(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","\n","def is_number(string: str) -> bool:\n","    \"\"\"\n","    Check whether a string is a number, both written or numeric.\n","\n","    Args:\n","    - string (str): The string to be checked.\n","\n","    Returns:\n","    - True if the string is a number, False otherwise.\n","    \"\"\"\n","    if string.isdigit():\n","        return True\n","    try:\n","        # Try to convert written number to integer\n","        text2num(string, \"en\", relaxed=True)\n","        return True\n","    except ValueError:\n","        return False\n","\n","\n","def batched_function(fn, scalar_output=True):\n","    def execute_on_batch(batch):\n","        examples = [\n","            fn(dict(zip(batch.keys(), values))) for values in zip(*batch.values())\n","        ]\n","\n","        if scalar_output:\n","            return {\n","                key: [example[key] for example in examples]\n","                for key in examples[0].keys()\n","            }\n","\n","        return {\n","            key: list(itertools.chain(*(example[key] for example in examples)))\n","            for key in examples[0].keys()\n","        }\n","\n","    return execute_on_batch\n","\n","\n","def create_dataframe(dataset: datasets.DatasetDict):\n","    dataset.set_format(\"pandas\")\n","\n","    dataset_ = []\n","    for split, ds in dataset.items():\n","        split_df = ds[:]\n","        split_df[\"split\"] = split\n","        dataset_.append(split_df)\n","    dataset_ = pd.concat(dataset_)\n","    dataset_.reset_index(drop=True, inplace=True)\n","    dataset.reset_format()\n","\n","    return dataset_\n","\n","\n","def explode_qa(dataset: pd.DataFrame):\n","    dataset = dataset.explode([\"questions\", \"answers\"])\n","    dataset.rename(columns={\"questions\": \"question\", \"answers\": \"answer\"}, inplace=True)\n","\n","    questions = pd.json_normalize(dataset[\"question\"])\n","    questions = questions[[\"turn_id\", \"input_text\"]]\n","    questions.rename(\n","        columns={\"input_text\": \"question\", \"turn_id\": \"turn\"}, inplace=True\n","    )\n","\n","    answers = pd.json_normalize(dataset[\"answer\"])\n","    answers = answers[\n","        [\"input_text\", \"span_text\", \"span_start\", \"span_end\", \"answer_type\"]\n","    ]\n","    answers.rename(\n","        columns={\"input_text\": \"answer\", \"span_text\": \"rationale\"}, inplace=True\n","    )\n","\n","    dataset.reset_index(inplace=True)\n","    dataset.drop([\"index\", \"question\", \"answer\"], axis=1, inplace=True)\n","    dataset = dataset.join(questions)\n","    dataset = dataset.join(answers)\n","\n","    cols = dataset.columns.tolist()\n","    cols.append(cols.pop(cols.index(\"last_turn\")))\n","    cols.append(cols.pop(cols.index(\"qa_length\")))\n","    cols.append(cols.pop(cols.index(\"split\")))\n","    return dataset[cols]\n","\n","\n","def plot_answer_type_distribution(qa_dataset: pd.DataFrame):\n","    plot_distribution(qa_dataset, field=\"answer_type\", hue=\"split\")\n","\n","\n","def plot_distribution(dataset: pd.DataFrame, field: str, hue: str = None):\n","    if hue is not None:\n","        dataset = dataset.groupby(hue)\n","\n","    distribution = dataset[field].value_counts(normalize=True)\n","    distribution = distribution.apply(lambda x: np.round(x, decimals=3) * 100)\n","    distribution = distribution.rename(\"frequency\").reset_index()\n","    ax = sns.barplot(distribution, x=field, y=\"frequency\", hue=hue)\n","\n","    for i in ax.containers:\n","        ax.bar_label(\n","            i,\n","        )\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def show_inputs(tokenizer, data, inputs):\n","    for k, v in inputs.items():\n","        print(f\"{k:<27}: {v}\")\n","    print()\n","\n","    for idx in range(len(inputs[\"input_ids\"])):\n","        show_input(tokenizer, data, inputs, idx)\n","        print()\n","\n","\n","def show_input(tokenizer, data, inputs, idx):\n","    sample_idx = (\n","        inputs[\"overflow_to_sample_mapping\"][idx]\n","        if \"overflow_to_sample_mapping\" in inputs\n","        else idx\n","    )\n","\n","    input_ids = np.asarray(inputs[\"input_ids\"][idx])\n","    passage_mask = np.asarray(inputs[\"passage_mask\"][idx])\n","    rationale_labels = np.asarray(inputs[\"rationale_labels\"][idx])\n","    rationale_start = inputs[\"rationale_start\"][idx]\n","    rationale_end = inputs[\"rationale_end\"][idx]\n","    labels = np.asarray(inputs[\"labels\"][idx])\n","    decoder_attention_mask = np.asarray(inputs[\"decoder_attention_mask\"][idx])\n","\n","    passage = input_ids[passage_mask.astype(np.bool_)]\n","    rationale = input_ids[rationale_labels > 0]\n","    assert np.all(rationale == input_ids[rationale_start:rationale_end])\n","    answer = labels[decoder_attention_mask.astype(np.bool_)]\n","\n","    print(\"Input:\", tokenizer.decode(input_ids))\n","    print(\"Q:\", data[\"question\"][sample_idx])\n","    print(\"P (-):\", data[\"passage\"][sample_idx])\n","    print(\"P (+):\", tokenizer.decode(passage))\n","    print(\"R (-):\", data[\"rationale\"][sample_idx])\n","    print(\"R (+):\", tokenizer.decode(rationale))\n","    print(\"A (-):\", data[\"answer\"][sample_idx])\n","    print(\"A (+):\", tokenizer.decode(answer))\n","    print(\"History:\", data[\"history\"][sample_idx])\n","\n","\n","def logits_to_class(logits, task: Literal[\"binary\", \"multiclass\"]) -> torch.LongTensor:\n","    if task == \"binary\":\n","        return (logits > 0.0).long()\n","    elif task == \"multiclass\":\n","        return torch.argmax(logits, dim=-1).long()\n","    else:\n","        raise ValueError(\n","            \"Invalid task. Supported values are 'binary' and 'multiclass'.\"\n","        )\n","\n","\n","def prepare_model_inputs(\n","    model: nn.Module, inputs: Dict[str, torch.Tensor]\n",") -> Dict[str, torch.Tensor]:\n","    forward_signature = set(inspect.signature(model.forward).parameters)\n","    inputs = {\n","        argument: value.to(model.device)\n","        for argument, value in inputs.items()\n","        if argument in forward_signature\n","    }\n","    return inputs\n"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import itertools\n","import re\n","import string\n","import transformers\n","\n","import numpy as np\n","\n","from typing import List, Tuple\n","\n","\n","def answer_to_idx(answer: str) -> int:\n","    if answer.lower() == \"yes\":\n","        return 0\n","    if answer.lower() == \"no\":\n","        return 1\n","    return 2\n","\n","\n","def idx_to_answer(idx: int) -> str:\n","    if idx == 0:\n","        return \"yes\"\n","    if idx == 1:\n","        return \"no\"\n","    return None\n","\n","\n","class CoQADatasetPreprocessing:\n","    def __init__(\n","        self,\n","        tokenizer: transformers.PreTrainedTokenizer = None,\n","        label_pad_token_id=-100,\n","        encoder_max_length=512,\n","        decoder_max_length=350,\n","        stride=196,\n","        use_window=False,\n","        max_history_length=4,\n","    ) -> None:\n","        self.tokenizer = tokenizer\n","        self.label_pad_token_id = label_pad_token_id\n","        self.encoder_max_length = encoder_max_length\n","        self.decoder_max_length = decoder_max_length\n","        self.stride = stride\n","        self.use_window = use_window\n","        self.max_history_length = max_history_length\n","\n","    def explode_questions(self, example):\n","        questions = example[\"questions\"]\n","        answers = example[\"answers\"]\n","        histories = []\n","\n","        for idx in range(len(questions)):\n","            history = self.__create_history(idx, questions, answers)\n","            histories.append(history)\n","\n","        output = {\n","            \"id\": [example[\"id\"]] * example[\"qa_length\"],\n","            \"turn\": [question_item[\"turn_id\"] for question_item in questions],\n","            \"question\": [question_item[\"input_text\"] for question_item in questions],\n","            \"answer\": [answer_item[\"input_text\"] for answer_item in answers],\n","            \"rationale\": [answer_item[\"span_text\"] for answer_item in answers],\n","            \"span_start\": [answer_item[\"span_start\"] for answer_item in answers],\n","            \"span_end\": [answer_item[\"span_end\"] for answer_item in answers],\n","            \"answer_type\": [answer_item[\"answer_type\"] for answer_item in answers],\n","            \"history\": histories,\n","            \"history_length\": [len(history) for history in histories],\n","        }\n","        for key, value in example.items():\n","            if key not in output:\n","                output[key] = [value] * example[\"qa_length\"]\n","        return output\n","\n","    def __create_history(self, current_index, questions, answers):\n","        history = [\n","            {\n","                \"question\": questions[i][\"input_text\"],\n","                \"answer\": answers[i][\"input_text\"],\n","                \"turn\": questions[i][\"turn_id\"],\n","            }\n","            for i in range(current_index)\n","        ]\n","        return history\n","\n","    def preprocess_texts(self, example):\n","        handle_rationale = \"rationale\" in example\n","        if handle_rationale:\n","            example = self.__fix_rationale(example)\n","\n","        example = self.__preprocess_passage(example, handle_rationale=handle_rationale)\n","        example = self.__preprocess_questions(example)\n","        example = self.__preprocess_answers(example)\n","\n","        return example\n","\n","    def __fix_rationale(self, example):\n","        rationale, span_start, span_end = fix_rationale(\n","            example[\"passage\"],\n","            example[\"rationale\"],\n","            example[\"span_start\"],\n","            example[\"span_end\"],\n","        )\n","\n","        example[\"rationale\"] = rationale\n","        example[\"span_start\"] = span_start\n","        example[\"span_end\"] = span_end\n","\n","        return example\n","\n","    def __preprocess_passage(self, example, handle_rationale=True):\n","        return self.__fix_passage_white_space(\n","            example, handle_rationale=handle_rationale\n","        )\n","\n","    def __preprocess_questions(self, example):\n","        example[\"question\"] = white_space_fix(example[\"question\"])\n","        for item in example.get(\"history\", []):\n","            item[\"question\"] = white_space_fix(item[\"question\"])\n","        return example\n","\n","    def __preprocess_answers(self, example):\n","        if \"answer\" in example:\n","            example[\"answer\"] = self.__preprocess_answer(example[\"answer\"])\n","        for item in example.get(\"history\", []):\n","            item[\"answer\"] = self.__preprocess_answer(item[\"answer\"])\n","        return example\n","\n","    def __preprocess_answer(self, answer):\n","        answer = white_space_fix(answer)\n","        answer = strip_non_alphanumeric_chars(answer)\n","        return answer\n","\n","    def __fix_passage_white_space(self, example, handle_rationale=True):\n","        passage = example[\"passage\"]\n","        if handle_rationale:\n","            span_start = example[\"span_start\"]\n","            span_end = example[\"span_end\"]\n","\n","            if span_end - span_start > 0:\n","                # assert rationale has already been fixed\n","                assert (\n","                    passage[span_start].isalnum() and passage[span_end - 1].isalnum()\n","                ), \"Rationale must start and end with alphanumeric characters. You must fix it before.\"\n","\n","            passage_start = white_space_fix(passage[:span_start])\n","            rationale = white_space_fix(passage[span_start:span_end])\n","            passage_end = white_space_fix(passage[span_end:])\n","\n","            passage = \" \".join((passage_start, rationale, passage_end))\n","            span_start = len(passage_start) + 1\n","            span_end = span_start + len(rationale)\n","\n","            assert rationale == passage[span_start:span_end]\n","\n","            example[\"passage\"] = passage\n","            example[\"rationale\"] = rationale\n","            example[\"span_start\"] = span_start\n","            example[\"span_end\"] = span_end\n","        else:\n","            example[\"passage\"] = white_space_fix(passage)\n","\n","        return example\n","\n","    def process_data_to_model_inputs(\n","        self,\n","        examples,\n","        add_history=False,\n","        padding=False,\n","    ) -> transformers.BatchEncoding:\n","        assert (\n","            self.tokenizer is not None\n","        ), \"A tokenizer is required to prepare the inputs for the model\"\n","        process_rationale = \"rationale\" in examples\n","        process_answer = \"answer\" in examples\n","\n","        sentences = [examples[\"question\"], examples[\"passage\"]]\n","\n","        if add_history:\n","            sentences[0] = self.__concat_history_and_question(\n","                examples[\"history\"], examples[\"question\"]\n","            )\n","\n","        inputs = self.tokenizer(\n","            *sentences,\n","            padding=padding,\n","            truncation=\"only_second\",\n","            max_length=self.encoder_max_length,\n","            stride=self.stride,\n","            return_overflowing_tokens=self.use_window,\n","            return_offsets_mapping=True,\n","        )\n","        if process_answer:\n","            outputs = self.tokenizer(\n","                examples[\"answer\"],\n","                padding=padding,\n","                truncation=True,\n","                max_length=self.decoder_max_length,\n","            )\n","\n","        offset_mapping = inputs[\"offset_mapping\"]\n","        if self.use_window:\n","            sample_map = lambda i: inputs[\"overflow_to_sample_mapping\"][i]\n","        else:\n","            sample_map = lambda i: i\n","\n","        yes_no_types = []\n","        yng_labels = []\n","\n","        passage_masks = []\n","        rationale_starts = []\n","        rationale_ends = []\n","        rationale_labels = []\n","        decoder_input_ids = []\n","        labels = []\n","        decoder_attention_masks = []\n","\n","        ids = []\n","        turns = []\n","\n","        # # store the presence of the rationale in the passage for at least one row\n","        # rationale_in_passage = [False] * len(examples[\"question\"])\n","        for i, offset in enumerate(offset_mapping):\n","            sample_idx = sample_map(i)\n","            sequence_ids = inputs.sequence_ids(i)\n","\n","            passage_start, passage_end = self.__find_passage(sequence_ids)\n","            passage_masks.append(\n","                self.__create_mask(sequence_ids, passage_start, passage_end + 1)\n","            )\n","\n","            if process_rationale:\n","                start_char = examples[\"span_start\"][sample_idx]\n","                end_char = examples[\"span_end\"][sample_idx]\n","                rationale_start, rationale_end = self.__char2token_rationale_span(\n","                    offset, (passage_start, passage_end), (start_char, end_char)\n","                )\n","                rationale_starts.append(rationale_start)\n","                rationale_ends.append(rationale_end)\n","                rationale_labels_ = self.__create_mask(\n","                    sequence_ids, rationale_start, rationale_end, dtype=np.float32\n","                )\n","                rationale_labels_[passage_masks[-1] == 0] = self.label_pad_token_id\n","                rationale_labels.append(rationale_labels_)\n","\n","                # rationale_in_passage[sample_idx] |= rationale_start != -1\n","\n","            if process_answer:\n","                # Remove <eos> from decoder_input_ids\n","                decoder_input_ids_ = outputs.input_ids[sample_idx][:-1]\n","                # Remove <bos> from labels\n","                labels_ = outputs.input_ids[sample_idx].copy()[1:]\n","                labels_ = [\n","                    self.label_pad_token_id\n","                    if token == self.tokenizer.pad_token_id\n","                    else token\n","                    for token in labels_\n","                ]\n","                decoder_attention_mask = outputs.attention_mask[sample_idx][:-1]\n","\n","                decoder_input_ids.append(decoder_input_ids_)\n","                labels.append(labels_)\n","                decoder_attention_masks.append(decoder_attention_mask)\n","\n","                yng_label = answer_to_idx(examples[\"answer\"][sample_idx])\n","                is_yes_no = yng_label < 2\n","                assert is_yes_no == (examples[\"answer_type\"][sample_idx] == \"yes_no\")\n","                yng_labels.append(yng_label)\n","                yes_no_types.append(int(is_yes_no))\n","\n","            ids.append(examples[\"id\"][sample_idx])\n","            if \"turn\" in examples:\n","                turns.append(examples[\"turn\"][sample_idx])\n","\n","        # if process_rationale:\n","        #     for sample_idx, is_rationale_in_passage in enumerate(rationale_in_passage):\n","        #         if not is_rationale_in_passage:\n","        #             warnings.warn(f\"The rationale is never contained in the passage. Id: {examples['id'][sample_idx]}, turn:{examples['turn'][sample_idx]}\")\n","\n","        inputs[\"passage_mask\"] = passage_masks\n","        if process_rationale:\n","            inputs[\"rationale_start\"] = rationale_starts\n","            inputs[\"rationale_end\"] = rationale_ends\n","            inputs[\"rationale_labels\"] = rationale_labels\n","        if process_answer:\n","            inputs[\"decoder_input_ids\"] = decoder_input_ids\n","            inputs[\"labels\"] = labels\n","            inputs[\"decoder_attention_mask\"] = decoder_attention_masks\n","            inputs[\"yng_label\"] = yng_labels\n","            inputs[\"yes_no\"] = yes_no_types\n","        inputs[\"id\"] = ids\n","        if len(turns) > 0:\n","            inputs[\"turn\"] = turns\n","\n","        return inputs\n","\n","    def __concat_history_and_question(self, histories, questions):\n","        outputs = []\n","        for history, question in zip(histories, questions):\n","            history_str = self.__create_history_str(history)\n","            history_question = self.tokenizer.sep_token.join((history_str, question))\n","            outputs.append(history_question)\n","        return outputs\n","\n","    def __create_history_str(self, history):\n","        history_items = reversed(\n","            tuple(itertools.islice((reversed(history)), self.max_history_length))\n","        )\n","        qa_pairs = []\n","        for item in history_items:\n","            qa = (item[\"question\"], item[\"answer\"])\n","            qa = self.tokenizer.sep_token.join(qa)\n","            qa_pairs.append(qa)\n","        return self.tokenizer.sep_token.join(qa_pairs)\n","\n","    def __find_passage(self, sequence_ids: List[int]) -> Tuple[int, int]:\n","        \"\"\"\n","        Find the start and the end of the passage w.r.t. the tokens.\n","        \"\"\"\n","\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        passage_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        passage_end = idx - 1\n","\n","        return passage_start, passage_end\n","\n","    def __char2token_rationale_span(\n","        self,\n","        token_to_span: List[Tuple[int, int]],\n","        passage_token_span: Tuple[int, int],\n","        rationale_char_span: Tuple[int, int],\n","    ) -> Tuple[int, int]:\n","        \"\"\"\n","        Map the rationale span from char indexes to token indexes\n","        \"\"\"\n","        passage_start, passage_end = passage_token_span\n","        start_char, end_char = rationale_char_span\n","\n","        # If the rationale is not fully inside the passage, returns (-1, -1)\n","        if (\n","            token_to_span[passage_start][0] > start_char\n","            or token_to_span[passage_end][1] < end_char\n","        ):\n","            return (-1, -1)\n","\n","        # Otherwise it's the start and end token positions\n","        idx = passage_start\n","        while idx <= passage_end and token_to_span[idx][0] <= start_char:\n","            idx += 1\n","        start_position = idx - 1\n","\n","        idx = passage_end\n","        while idx >= passage_start and token_to_span[idx][1] >= end_char:\n","            idx -= 1\n","        end_position = idx + 1\n","\n","        return start_position, end_position + 1\n","\n","    def __create_mask(self, arr, start, end, dtype=np.int8):\n","        mask = np.zeros_like(arr, dtype=dtype)\n","        mask[start:end] = 1\n","        return mask\n","\n","\n","def remove_articles_(text):\n","    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n","    return re.sub(regex, \" \", text)\n","\n","\n","def white_space_fix(text: str):\n","    return \" \".join(text.split())\n","\n","\n","def remove_punc(text):\n","    exclude = set(string.punctuation)\n","    return \"\".join(ch for ch in text if ch not in exclude)\n","\n","\n","def lower(text):\n","    return text.lower()\n","\n","\n","def normalize_text(text, remove_articles=False):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    text = remove_punc(lower(text))\n","    if remove_articles:\n","        text = remove_articles_(text)\n","    return white_space_fix(text)\n","\n","\n","def normalize_answer(text):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    return normalize_text(text, remove_articles=True)\n","\n","\n","def strip_non_alphanumeric_chars(text: str):\n","    \"\"\"\n","    Removes trailing and leading non alpha-numeric characters from a given string.\n","    \"\"\"\n","    start_index = 0\n","    while start_index < len(text) and not text[start_index].isalnum():\n","        start_index += 1\n","\n","    end_index = len(text) - 1\n","    while end_index > start_index and not text[end_index].isalnum():\n","        end_index -= 1\n","\n","    return text[start_index : end_index + 1]\n","\n","\n","def find_span(passage: str, text: str, span_start: int = None, span_end: int = None):\n","    if len(text) == 0:\n","        return (span_start, span_start)\n","    assert (\n","        text[0].isalnum() and text[-1].isalnum()\n","    ), \"Text must begin and end with an alphanumeric character.\"\n","\n","    start_idx = passage.find(text, span_start, span_end)\n","    end_idx = start_idx + len(text) - 1\n","\n","    if start_idx == -1:\n","        raise ValueError(\"The text is not present in the passage.\")\n","\n","    # Find the beginning of the word in the passage\n","    while start_idx > 0 and passage[start_idx - 1].isalnum():\n","        start_idx -= 1\n","\n","    # Find the end of the word in the passage\n","    while end_idx < len(passage) - 1 and passage[end_idx + 1].isalnum():\n","        end_idx += 1\n","\n","    return start_idx, end_idx + 1\n","\n","\n","def fix_rationale(passage: str, rationale: str, span_start: int, span_end: int):\n","    rationale = strip_non_alphanumeric_chars(rationale)\n","    span_start, span_end = find_span(\n","        passage, rationale, span_start=span_start, span_end=span_end\n","    )\n","    return passage[span_start:span_end], span_start, span_end\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model definition"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.090027Z","iopub.status.busy":"2023-08-02T15:57:47.089361Z","iopub.status.idle":"2023-08-02T15:57:47.149413Z","shell.execute_reply":"2023-08-02T15:57:47.148289Z","shell.execute_reply.started":"2023-08-02T15:57:47.089990Z"},"trusted":true},"outputs":[],"source":["from typing import List, Optional, Union, Tuple\n","from dataclasses import dataclass\n","import warnings\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import transformers\n","from transformers import AutoTokenizer, AutoModel, EncoderDecoderModel\n","\n","\n","@dataclass\n","class QAEncoderModelOutput(transformers.utils.ModelOutput):\n","    \"\"\"\n","    Base class for outputs of question answering with rationale models.\n","    Args:\n","        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n","            Total rationale extraction loss is the sum of a Binary Cross-Entropy for the tokens in the passage.\n","        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n","            Sequence of hidden-states at the output of the last layer of the model.\n","        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n","            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n","            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n","            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n","        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n","            heads.\n","        rationale_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n","            Rationale classification scores (before Sigmoid).\n","        yng_logits (`torch.FloatTensor` of shape `(batch_size, 3)`):\n","            Yes/No/Generative scores (before Sigmoid).\n","    \"\"\"\n","\n","    loss: Optional[torch.FloatTensor] = None\n","    last_hidden_state: torch.FloatTensor = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    rationale_logits: torch.FloatTensor = None\n","    yng_logits: torch.FloatTensor = None\n","\n","\n","@dataclass\n","class QAEncoderDecoderModelOutput(transformers.utils.ModelOutput):\n","    \"\"\"\n","    Class for [`QAEncoderDecoderModel`] outputs.\n","    Args:\n","        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n","            Language modeling loss.\n","        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n","            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n","        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n","            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n","            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n","            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n","            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n","            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n","        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n","            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n","            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n","            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n","        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n","            self-attention heads.\n","        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n","            weighted average in the cross-attention heads.\n","        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n","            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n","        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n","            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n","            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n","            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n","        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n","            self-attention heads.\n","        encoder_rationale_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n","            Encoder rationale classification scores (before Sigmoid).\n","        encoder_yng_logits (`torch.FloatTensor` of shape `(batch_size, 3)`):\n","            Encoder Yes/No/Generative scores (before Sigmoid).\n","    \"\"\"\n","\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n","    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n","    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_rationale_logits: Optional[torch.FloatTensor] = None\n","    encoder_yng_logits: torch.FloatTensor = None\n","\n","\n","class QAEncoderDecoderModel(transformers.EncoderDecoderModel):\n","    def __init__(\n","        self,\n","        encoder: transformers.PreTrainedModel,\n","        decoder: transformers.PreTrainedModel,\n","        config: Optional[transformers.EncoderDecoderConfig] = None,\n","    ):\n","        super(QAEncoderDecoderModel, self).__init__(\n","            encoder=encoder, decoder=decoder, config=config\n","        )\n","\n","    def tie_weights(self):\n","        # tie encoder & decoder if needed\n","        if self.config.tie_encoder_decoder:\n","            # tie encoder and decoder base model\n","            encoder_base_model_prefix = self.encoder.base_model_prefix\n","            decoder_base_model_prefix = self.decoder.base_model_prefix\n","            self._tie_encoder_decoder_weights(\n","                self.encoder._modules[encoder_base_model_prefix],\n","                self.decoder._modules[decoder_base_model_prefix],\n","                self.decoder.base_model_prefix,\n","            )\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        token_type_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        passage_mask: Optional[torch.BoolTensor] = None,\n","        decoder_input_ids: Optional[torch.LongTensor] = None,\n","        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n","        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n","        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","        # labels: Optional[torch.LongTensor] = None,\n","        use_cache: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","        teacher_force: Optional[float] = None,\n","        rationale_labels: Optional[torch.BoolTensor] = None,\n","        **kwargs,\n","    ) -> Union[Tuple, transformers.modeling_outputs.Seq2SeqLMOutput]:\n","        kwargs_encoder = {\n","            argument: value\n","            for argument, value in kwargs.items()\n","            if not argument.startswith(\"decoder\")\n","        }\n","\n","        if encoder_outputs is None:\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","                passage_mask=passage_mask,\n","                return_dict=return_dict,\n","                teacher_force=teacher_force,\n","                rationale_labels=rationale_labels,\n","                **kwargs_encoder,\n","            )\n","\n","        outputs = super().forward(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            encoder_outputs=encoder_outputs,\n","            past_key_values=past_key_values,\n","            inputs_embeds=inputs_embeds,\n","            decoder_inputs_embeds=decoder_inputs_embeds,\n","            # labels=labels,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            token_type_ids=token_type_ids,\n","            **kwargs,\n","        )\n","\n","        if not return_dict:\n","            return outputs + encoder_outputs[-2:]\n","\n","        return QAEncoderDecoderModelOutput(\n","            **outputs,\n","            encoder_rationale_logits=encoder_outputs.rationale_logits,\n","            encoder_yng_logits=encoder_outputs.yng_logits,\n","        )\n","\n","\n","class QAEncoder(transformers.PreTrainedModel):\n","    base_model_prefix = \"encoder\"\n","\n","    def __init__(self, encoder, config) -> None:\n","        super().__init__(config)\n","        self.config = config\n","\n","        self.encoder = encoder\n","        self.rationale_head = TokenSelectionHead(config)\n","        self.yes_no_gen_head = YesNoGenHead(config)\n","\n","        self.post_init()\n","\n","    def _init_weights(self, module):\n","        \"\"\"Initialize the weights\"\"\"\n","        if isinstance(module, nn.Linear):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.Tensor] = None,\n","        token_type_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        passage_mask: Optional[torch.Tensor] = None,\n","        # labels: Optional[torch.FloatTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","        teacher_force: Optional[float] = None,\n","        rationale_labels: Optional[torch.BoolTensor] = None,\n","        **kwargs,\n","    ) -> Union[Tuple, QAEncoderModelOutput]:\n","        assert passage_mask is not None, \"Passage mask is required\"\n","\n","        outputs = self.encoder(\n","            input_ids=input_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask,\n","            inputs_embeds=inputs_embeds,\n","            # labels=labels,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            **kwargs,\n","        )\n","\n","        last_hidden_state = outputs[0]\n","        pooled_output = outputs[1]\n","\n","        rationale_logits = self.rationale_head(last_hidden_state)\n","\n","        passage_mask = passage_mask.unsqueeze(-1)\n","        p_rationale = torch.sigmoid(rationale_logits)\n","        # substitute the last_hidden_state[passage] with p_rationale * last_hidden_state[passage]\n","        # ideally, our network keeps only the span of the passage which represents the rationale\n","        if self.training:\n","            if teacher_force is not None:\n","                use_labels = torch.rand(rationale_labels.shape[0]) < teacher_force\n","                use_labels = use_labels.to(p_rationale.device)\n","                true_labels = rationale_labels.unsqueeze(-1).type(p_rationale.dtype)\n","                p_rationale = torch.where(use_labels.reshape(-1, 1, 1), true_labels, p_rationale)\n","        else:\n","            p_rationale = (p_rationale > self.config.p_rationale_threshold).type(p_rationale.dtype)\n","\n","        weighted_passage_hidden_state = passage_mask * p_rationale * last_hidden_state\n","        qa_seq_hidden_state = (1 - passage_mask) * last_hidden_state\n","        last_hidden_state = weighted_passage_hidden_state + qa_seq_hidden_state\n","\n","        yng_logits = self.yes_no_gen_head(weighted_passage_hidden_state, pooled_output)\n","\n","        rationale_logits = rationale_logits.squeeze(-1)\n","\n","        loss = None\n","        if not return_dict:\n","            output = (last_hidden_state,) + outputs[2:] + (rationale_logits,)\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return QAEncoderModelOutput(\n","            loss=loss,\n","            last_hidden_state=last_hidden_state,\n","            rationale_logits=rationale_logits,\n","            yng_logits=yng_logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","\n","class TokenSelectionHead(nn.Module):\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n","        self.act_fn = nn.ReLU()\n","        self.hidden_to_logit = nn.Linear(config.hidden_size, 1, bias=False)\n","\n","    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.act_fn(hidden_states)\n","        logits = self.hidden_to_logit(hidden_states)\n","        return logits\n","\n","\n","class YesNoGenHead(nn.Module):\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n","        self.act_fn = nn.ReLU()\n","        self.hidden_to_logit = nn.Linear(config.hidden_size, 1, bias=False)\n","        self.softmax = nn.Softmax(dim=-2)\n","\n","        classifier_dropout = (\n","            config.classifier_dropout\n","            if config.classifier_dropout is not None\n","            else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(2 * config.hidden_size, 3)\n","\n","    def forward(\n","        self,\n","        rationale_weighted_hidden_states: torch.FloatTensor,\n","        pooled_output: torch.FloatTensor,\n","    ) -> torch.FloatTensor:\n","        rationale_weighted_hidden_states = self.dense(rationale_weighted_hidden_states)\n","        rationale_weighted_hidden_states = self.act_fn(\n","            rationale_weighted_hidden_states\n","        )  # BxTxD\n","        logits = self.hidden_to_logit(rationale_weighted_hidden_states)  # BxTx1\n","        attention_scores = self.softmax(logits)  # BxTx1\n","        weighted_tokens = attention_scores * rationale_weighted_hidden_states  # BxTxD\n","        weighted_pooled_output = torch.sum(weighted_tokens, dim=-2)  # BxD\n","        pooled_output = torch.cat(\n","            (weighted_pooled_output, pooled_output), dim=-1\n","        )  # Bx2D\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        return logits\n","\n","\n","def initialize_cross_attention_layer_with_self_attention_layer(\n","    self_attention: nn.Module,\n","    cross_attention: nn.Module,\n","    cross_attention_layer_prefix: str,\n","):\n","    uninitialized_cross_attention_weights: List[str] = []\n","    if cross_attention.__class__ != self_attention.__class__:\n","        print(\n","            f\"{cross_attention.__class__} and {self_attention.__class__} are not equal. In this case make sure that all encoder\"\n","            \" weights are correctly initialized.\"\n","        )\n","\n","    def initialize_cross_attention_with_self_attention_recursively(\n","        self_attention_pointer: nn.Module,\n","        cross_attention_pointer: nn.Module,\n","        module_name: str,\n","        uninitialized_cross_attention_weights: List[str],\n","        depth=0,\n","    ):\n","        assert isinstance(self_attention_pointer, nn.Module) and isinstance(\n","            cross_attention_pointer, nn.Module\n","        ), f\"{self_attention_pointer} and {cross_attention_pointer} have to be of type nn.Module\"\n","        if hasattr(self_attention_pointer, \"weight\"):\n","            assert hasattr(cross_attention_pointer, \"weight\")\n","            cross_attention_pointer.weight.data = (\n","                self_attention_pointer.weight.data.clone().detach()\n","            )\n","            if hasattr(self_attention_pointer, \"bias\"):\n","                assert hasattr(cross_attention_pointer, \"bias\")\n","                cross_attention_pointer.bias.data = (\n","                    self_attention_pointer.bias.data.clone().detach()\n","                )\n","            return\n","\n","        cross_attention_modules = cross_attention_pointer._modules\n","        self_attention_modules = self_attention_pointer._modules\n","        if len(self_attention_modules) > 0:\n","            assert (\n","                len(cross_attention_modules) > 0\n","            ), f\"Cross-attention module {cross_attention_pointer} does not match self-attention module {self_attention_pointer}\"\n","\n","            all_cross_attention_weights = {\n","                module_name + \"/\" + sub_name\n","                for sub_name in cross_attention_modules.keys()\n","            }\n","            cross_attention_layer_pos = 0\n","            for name, module in self_attention_modules.items():\n","                if name.isdigit():\n","                    cross_attention_name = str(int(name) + cross_attention_layer_pos)\n","                    self_attention_name = name\n","                    if not isinstance(\n","                        self_attention_modules[self_attention_name],\n","                        type(cross_attention_modules[cross_attention_name]),\n","                    ) and len(cross_attention_modules) != len(self_attention_modules):\n","                        # this can happen if the name corresponds to the position in a list module list of layers\n","                        # in this case the decoder has added a cross-attention that the encoder does not have\n","                        # thus skip this step and subtract one layer pos from encoder\n","                        cross_attention_layer_pos -= 1\n","                        continue\n","                elif name not in cross_attention_modules:\n","                    continue\n","                elif depth > 500:\n","                    raise ValueError(\n","                        \"Max depth of recursive function `initialize_cross_attention_with_self_attention` reached. It seems that there is\"\n","                        \" a circular dependency between two or more `nn.Modules` of your model.\"\n","                    )\n","                else:\n","                    self_attention_name = cross_attention_name = name\n","                initialize_cross_attention_with_self_attention_recursively(\n","                    self_attention_modules[self_attention_name],\n","                    cross_attention_modules[cross_attention_name],\n","                    module_name + \"/\" + name,\n","                    uninitialized_cross_attention_weights,\n","                    depth=depth + 1,\n","                )\n","                all_cross_attention_weights.remove(\n","                    module_name + \"/\" + cross_attention_name\n","                )\n","\n","            uninitialized_cross_attention_weights += list(all_cross_attention_weights)\n","\n","    # initialize weights recursively\n","    initialize_cross_attention_with_self_attention_recursively(\n","        self_attention,\n","        cross_attention,\n","        cross_attention_layer_prefix,\n","        uninitialized_cross_attention_weights,\n","    )\n","    if len(uninitialized_cross_attention_weights) > 0:\n","        warnings.warn(\n","            f\"The following cross_attention weights were not initialized with self_attention weights: {uninitialized_cross_attention_weights}\"\n","        )\n","\n","\n","def initialize_cross_attention_with_self_attention(model: EncoderDecoderModel):\n","    decoder_base_model_prefix = model.decoder.base_model_prefix\n","    for layer_idx in range(model.config.decoder.num_hidden_layers):\n","        decoder_layer = model.decoder._modules[decoder_base_model_prefix].encoder.layer[\n","            layer_idx\n","        ]\n","        cross_attention = decoder_layer.crossattention\n","        self_attention = decoder_layer.attention\n","        cross_attention_name = f\"layer.{layer_idx}.crossattention\"\n","        initialize_cross_attention_layer_with_self_attention_layer(\n","            self_attention, cross_attention, cross_attention_name\n","        )\n","    print(\"Cross-attention has been initialized with self-attention weights.\")\n","\n","\n","def make_encoder_decoder_model(\n","    checkpoint,\n","    decoder_max_length,\n","    generation_kwargs,\n","    tokenizer: Optional[transformers.PreTrainedTokenizer] = None,\n","    initialize_cross_attention=True,\n","):\n","    tokenizer, encoder = make_qa_encoder(checkpoint, tokenizer=tokenizer)\n","    decoder = transformers.AutoModelForCausalLM.from_pretrained(\n","        checkpoint,\n","        is_decoder=True,\n","        add_cross_attention=True,\n","        decoder_start_token_id=tokenizer.bos_token_id,\n","    )\n","\n","    config = transformers.EncoderDecoderConfig.from_encoder_decoder_configs(\n","        encoder.config,\n","        decoder.config,\n","        tie_encoder_decoder=True,\n","        decoder_start_token_id=tokenizer.cls_token_id,\n","        eos_token_id=tokenizer.sep_token_id,\n","        pad_token_id=tokenizer.pad_token_id,\n","        # sensible parameters for generation\n","        vocab_size=decoder.config.vocab_size,\n","        max_new_tokens=decoder_max_length,\n","        **generation_kwargs,\n","    )\n","\n","    model = QAEncoderDecoderModel(encoder=encoder, decoder=decoder, config=config)\n","    if initialize_cross_attention:\n","        initialize_cross_attention_with_self_attention(model)\n","\n","    return tokenizer, model\n","\n","\n","def make_qa_encoder(\n","    checkpoint, tokenizer: Optional[transformers.PreTrainedTokenizer] = None\n","):\n","    if tokenizer is None:\n","        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","    encoder = AutoModel.from_pretrained(checkpoint)\n","    encoder.config.p_rationale_threshold = 0.5\n","    encoder = QAEncoder(encoder, encoder.config)\n","    return tokenizer, encoder\n"]},{"cell_type":"markdown","metadata":{},"source":["# Losses"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.151673Z","iopub.status.busy":"2023-08-02T15:57:47.151229Z","iopub.status.idle":"2023-08-02T15:57:47.180257Z","shell.execute_reply":"2023-08-02T15:57:47.179183Z","shell.execute_reply.started":"2023-08-02T15:57:47.151636Z"},"trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Dict, Optional, Protocol, Tuple\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","\n","_EPSILON = 1e-7\n","\n","\n","def apply_reduction(input: torch.Tensor, reduction: str, dim=0):\n","    if reduction == \"none\":\n","        return input\n","\n","    if input.shape[0] == 0:\n","        input = torch.Tensor([0]).type(dtype=input.dtype).to(device=input.device)\n","    if reduction == \"mean\":\n","        return torch.mean(input, dim=dim)\n","    if reduction == \"sum\":\n","        return torch.sum(input, dim=dim)\n","\n","    raise ValueError(\n","        \"Invalid reduction. Supported values are 'none', 'mean' and 'sum'.\"\n","    )\n","\n","def categorical_focal_loss_with_logits(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor]=None, alpha=1., gamma=2., reduction: str = \"mean\"):\n","    ce_loss = F.cross_entropy(input, target, reduction=\"none\")\n","    pt = torch.exp(-ce_loss)\n","    loss = alpha * (1-pt)**gamma * ce_loss\n","\n","    if weight is not None:\n","        weight = weight[target.long()]\n","        loss *= weight\n","\n","    return apply_reduction(loss, reduction=reduction)\n","\n","\n","class Loss(Protocol):\n","    def __call__(self, outputs, targets: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n","        pass\n","\n","\n","class ComputeLoss(Protocol):\n","    def __call__(\n","        outputs, targets: Dict[str, torch.Tensor]\n","    ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","        pass\n","\n","\n","def wrap_loss_fn(\n","    name: str, loss_fn: Loss\n",") -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","    def loss(outputs, targets):\n","        loss_value = loss_fn(outputs, targets)\n","        return loss_value, {name: loss_value.item()}\n","\n","    return loss\n","\n","\n","@dataclass\n","class Criterion:\n","    name: str\n","    loss_fn: Loss\n","    weight: float = 1.0\n","\n","\n","class UncertaintyLoss(nn.Module, ComputeLoss):\n","    def __init__(self, name: str, loss_fn: Loss, initial_weight: float = 1.0) -> None:\n","        super(UncertaintyLoss, self).__init__()\n","        self.name = name\n","        self.loss_fn = loss_fn\n","        log_sigma_square = -np.log(initial_weight)\n","        self.log_sigma_square = nn.Parameter(\n","            torch.tensor(log_sigma_square, requires_grad=True, dtype=torch.float32)\n","        )\n","\n","    def forward(\n","        self, outputs, targets: Dict[str, torch.Tensor]\n","    ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","        inner_loss = self.loss_fn(outputs, targets)\n","        # 1/sigma^2 * L + 2 log sigma\n","        weight = torch.exp(-self.log_sigma_square)\n","        loss = weight * inner_loss + self.log_sigma_square\n","\n","        return loss, {\n","            self.name: inner_loss.item(),\n","            f\"{self.name}_weight\": weight.item(),\n","        }\n","\n","\n","# class UncertaintyLoss(nn.Module):\n","#     def __init__(self, *criteria: Criterion) -> None:\n","#         super(UncertaintyLoss, self).__init__()\n","#         self.criteria = criteria\n","#         weights = [criterion.weight for criterion in self.criteria]\n","#         log_square_sigmas = -np.log(weights)\n","#         self.log_square_sigmas = nn.Parameter(\n","#             torch.tensor(log_square_sigmas, requires_grad=True, dtype=torch.float32)\n","#         )\n","\n","#     def forward(\n","#         self, outputs, targets: Dict[str, torch.Tensor]\n","#     ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","#         losses = {}\n","#         total_loss = 0.0\n","#         for criterion, log_sigma_square in zip(self.criteria, self.log_square_sigmas):\n","#             loss = criterion.loss_fn(outputs, targets)\n","#             # 1/sigma^2 * L + 2 log sigma\n","#             weight = torch.exp(-log_sigma_square)\n","#             total_loss += weight * loss + log_sigma_square\n","#             losses[f\"{criterion.name}_weight\"] = weight.item()\n","#             losses[criterion.name] = loss.item()\n","\n","#         return total_loss, losses\n","\n","\n","def generative_loss(\n","    logits: torch.FloatTensor,\n","    labels: torch.IntTensor,\n","    reduction: str = \"mean\",\n","    mask: torch.Tensor = None,\n",") -> torch.FloatTensor:\n","    if mask is not None:\n","        logits = logits[mask.bool()]\n","        labels = labels[mask.bool()]\n","\n","    # swap seq_length with vocabulary dimension\n","    logits = torch.transpose(logits, 1, 2)  # batch_size x seq_length x vocab\n","    loss = F.cross_entropy(\n","        input=logits, target=labels, reduction=\"none\"\n","    )  # batch_size x seq_length\n","    n_tokens_per_sample = torch.sum(labels != -100, dim=-1)  # batch_size\n","    n_tokens_per_sample = torch.clamp(n_tokens_per_sample, min=_EPSILON)\n","    loss = torch.sum(loss, dim=-1) / n_tokens_per_sample  # batch_size\n","    loss = apply_reduction(loss, reduction=reduction)\n","    return loss\n","\n","\n","class EncoderDecoderGenerativeLoss(Loss):\n","    def __init__(self, reduction: str = \"mean\") -> None:\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"logits\"]\n","        labels = targets[\"labels\"]\n","\n","        return generative_loss(logits, labels, reduction=self.reduction, mask=mask)\n","\n","\n","# def rationale_loss(self, outputs, targets: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n","def rationale_loss(\n","    logits: torch.FloatTensor,\n","    labels: torch.IntTensor,\n","    passage_mask: torch.IntTensor,\n","    max_rationale_length: int,\n","    reduction=\"mean\",\n","    mask: torch.Tensor = None,\n",") -> torch.FloatTensor:\n","    \"\"\"\n","    li = w * BCE(y_pred_i, y_true_i)\n","    , where w = w_positive if y_true_i is positive\n","            w = w_negative if y_true_i is negative\n","    w_positive = totals / positives\n","    w_negative = totals / negatives\n","    , where totals, positives and negatives are computed for each sequence\n","\n","    Ls = sum_i=1..seq_length li / sum(w_i)\n","    L = sum_s=1..N Ls / N,\n","    , where N is the #sequences whose rationale length is <= max_rationale_length\n","    \"\"\"\n","\n","    # rationale_logits = outputs[self.rationale_logits_name]\n","    # rationale_labels = targets[self.rationale_labels_name]\n","    # passage_mask = targets[self.passage_mask_name]\n","\n","    labels = labels * passage_mask\n","\n","    rationale_lengths = torch.sum(labels, dim=-1)  # batch_size\n","    valid_rationales = rationale_lengths <= max_rationale_length\n","    if mask is not None:\n","        valid_rationales = valid_rationales & mask.bool()\n","\n","    labels = labels[valid_rationales]\n","    passage_mask = passage_mask[valid_rationales]\n","    logits = logits[valid_rationales]\n","\n","    # n_sequences = torch.sum(valid_rationales)\n","\n","    totals = torch.sum(passage_mask, -1, keepdim=True)  # N x 1\n","    positives = torch.sum(labels, -1, keepdim=True)  # N x 1\n","    negatives = totals - positives  # N x 1\n","    totals = torch.clamp(totals, min=_EPSILON).float()\n","    weights = torch.where(\n","        labels == 1.0, totals / positives, totals / negatives\n","    )  # N x seq_length\n","    weights = torch.where(weights != torch.inf, weights, 0.0)  # N x seq_length\n","    weights = weights * passage_mask  # N x seq_length\n","    normalize_factor = torch.clamp(\n","        torch.sum(weights, dim=-1, keepdim=True), min=_EPSILON\n","    )\n","    weights = weights / normalize_factor  # N x seq_length\n","    # weights = weights * valid_rationales / n_sequences\n","\n","    # N x seq_length\n","    per_token_loss = F.binary_cross_entropy_with_logits(\n","        input=logits,\n","        target=labels,\n","        weight=weights,\n","        reduction=\"none\",\n","    )\n","\n","    loss = torch.sum(per_token_loss, dim=-1)  # N\n","    return apply_reduction(loss, reduction=reduction)\n","\n","\n","class EncoderDecoderRationaleLoss(Loss):\n","    def __init__(self, max_rationale_length: int, reduction: str = \"mean\") -> None:\n","        self.max_rationale_length = max_rationale_length\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"encoder_rationale_logits\"]\n","        labels = targets[\"rationale_labels\"]\n","        passage_mask = targets[\"passage_mask\"]\n","\n","        return rationale_loss(\n","            logits,\n","            labels,\n","            passage_mask,\n","            self.max_rationale_length,\n","            reduction=self.reduction,\n","            mask=mask,\n","        )\n","\n","\n","class EncoderRationaleLoss(Loss):\n","    def __init__(self, max_rationale_length: int, reduction: str = \"mean\") -> None:\n","        self.max_rationale_length = max_rationale_length\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"rationale_logits\"]\n","        labels = targets[\"rationale_labels\"]\n","        passage_mask = targets[\"passage_mask\"]\n","\n","        return rationale_loss(\n","            logits,\n","            labels,\n","            passage_mask,\n","            self.max_rationale_length,\n","            reduction=self.reduction,\n","            mask=mask,\n","        )\n","\n","\n","def yes_no_gen_loss(\n","    logits: torch.FloatTensor,\n","    labels: torch.IntTensor,\n","    weight: Optional[torch.FloatTensor] = None,\n","    reduction=\"mean\",\n","    mask: torch.Tensor = None,\n",") -> torch.FloatTensor:\n","    if mask is not None:\n","        logits = logits[mask.bool()]\n","        labels = labels[mask.bool()]\n","\n","    if weight is not None:\n","        weight.to(logits.device)\n","\n","    # loss = F.cross_entropy(logits, labels, weight=weight, reduction=reduction)\n","    loss = categorical_focal_loss_with_logits(logits, labels, weight=weight, reduction=reduction)\n","    return loss\n","\n","\n","class EncoderDecoderYNGLoss(Loss):\n","    def __init__(\n","        self, weight: Optional[torch.FloatTensor] = None, reduction: str = \"mean\"\n","    ) -> None:\n","        self.weight = weight\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"encoder_yng_logits\"]\n","        labels = targets[\"yng_label\"]\n","\n","        return yes_no_gen_loss(\n","            logits, labels, weight=self.weight, reduction=self.reduction, mask=mask\n","        )\n","\n","\n","class EncoderYNGLoss(Loss):\n","    def __init__(\n","        self, weight: Optional[torch.FloatTensor] = None, reduction: str = \"mean\"\n","    ) -> None:\n","        self.weight = weight\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"yng_logits\"]\n","        labels = targets[\"yng_label\"]\n","\n","        return yes_no_gen_loss(\n","            logits, labels, weight=self.weight, reduction=self.reduction, mask=mask\n","        )\n","\n","\n","class EncoderDecoderLoss(nn.Module):\n","    def __init__(\n","        self,\n","        max_rationale_length,\n","        yng_loss_weight=1.0,\n","        rationale_loss_weight=1.0,\n","        generative_loss_weight=1.0,\n","    ) -> None:\n","        super().__init__()\n","\n","        self.yng_loss_weight = yng_loss_weight\n","        self.rationale_loss_weight = rationale_loss_weight\n","        self.generative_loss_weight = generative_loss_weight\n","\n","        # weight = torch.Tensor([1 / 11.0, 1 / 9.0, 1 / 80.0])\n","        # weight = weight / torch.sum(weight)\n","        # weight = None\n","        self.yes_no_gen_loss_fn = EncoderDecoderYNGLoss()\n","        self.yes_no_gen_loss_fn = EncoderDecoderYNGLoss()\n","        self.rationale_loss_fn = EncoderDecoderRationaleLoss(\n","            max_rationale_length=max_rationale_length\n","        )\n","        self.generative_loss_fn = EncoderDecoderGenerativeLoss()\n","\n","    def forward(\n","        self, outputs, targets: Dict[str, torch.Tensor]\n","    ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","        is_generative = ~targets[\"yes_no\"].bool()\n","\n","        yng_loss = self.yes_no_gen_loss_fn(outputs, targets)\n","        rationale_loss = self.rationale_loss_fn(outputs, targets, mask=is_generative)\n","        generative_loss = self.generative_loss_fn(outputs, targets, mask=is_generative)\n","\n","        total_loss = (\n","            self.yng_loss_weight * yng_loss\n","            + self.rationale_loss_weight * rationale_loss\n","            + self.generative_loss_weight * generative_loss\n","        )\n","        loss_logs = {\n","            \"yng_loss\": yng_loss.item(),\n","            \"rationale_loss\": rationale_loss.item(),\n","            \"generative_loss\": generative_loss.item(),\n","        }\n","\n","        return total_loss, loss_logs\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","\n","import numpy as np\n","\n","\n","import transformers\n","import datasets\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, Optional, Union\n","\n","class LinearScheduler:\n","    def __init__(\n","        self, total_iters:int, start_value:float=1., end_value:float=0., fraction=0.7\n","    ):\n","\n","        self.start_value = float(start_value)\n","        self.end_value = float(end_value)\n","        self.total_iters = total_iters\n","        self.fraction = fraction\n","        self._total_iters = fraction * total_iters\n","        self.current_step = 0\n","\n","    def step(self):\n","        self.current_step += 1\n","\n","    def get_value(self):\n","        if self.current_step > self._total_iters:\n","            return self.end_value\n","         \n","        return (\n","            self.start_value\n","            + (self.end_value - self.start_value)\n","            / self._total_iters\n","            * self.current_step\n","        )\n","\n","class DummyScheduler:\n","    def step(self):\n","        pass\n","\n","    def get_value(self):\n","        return 0.\n","\n","class DummyLRScheduler:\n","    def __init__(self, optimizer: torch.optim.Optimizer) -> None:\n","        self.optimizer = optimizer\n","\n","    def step(self):\n","        None\n","\n","    def get_last_lr(self):\n","        return [group[\"lr\"] for group in self.optimizer.param_groups]\n","\n","    def state_dict(self):\n","        return {}\n","\n","\n","@dataclass\n","class DynamicPaddingCollatorForSeq2Seq:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs received, as well as the labels.\n","\n","    Args:\n","        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n","            The tokenizer used for encoding the data.\n","        model ([`PreTrainedModel`]):\n","            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n","            prepare the *decoder_input_ids*\n","\n","            This is useful when using *label_smoothing* to avoid calculating loss twice.\n","        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","\n","            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n","              sequence is provided).\n","            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n","              acceptable input length for the model if that argument is not provided.\n","            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n","        max_length (`int`, *optional*):\n","            Maximum length of the returned list and optionally padding length (see above).\n","        pad_to_multiple_of (`int`, *optional*):\n","            If set will pad the sequence to a multiple of the provided value.\n","\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","        label_pad_token_id (`int`, *optional*, defaults to -100):\n","            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n","        return_tensors (`str`):\n","            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n","    \"\"\"\n","\n","    tokenizer: transformers.PreTrainedTokenizerBase\n","    model: Optional[Any] = None\n","    padding: Union[bool, str, transformers.utils.PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    label_pad_token_id: int = -100\n","    return_tensors: str = \"pt\"\n","\n","    def __call__(self, features, return_tensors=None):\n","        if return_tensors is None:\n","            return_tensors = self.return_tensors\n","\n","        # We have to pad the labels and other features not in  `tokenizer.model_input_names` before calling `tokenizer.pad`\n","        # as `tokenizer.pad` method will pad only features in `tokenizer.model_input_names`\n","        tokenizer_input_names = set(self.tokenizer.model_input_names)\n","        for feature_name in features[0].keys():\n","            if feature_name not in tokenizer_input_names and isinstance(\n","                features[0][feature_name], list\n","            ):\n","                if feature_name.endswith(\"labels\"):\n","                    self.pad_feature(feature_name, features, self.label_pad_token_id)\n","                else:\n","                    self.pad_feature(feature_name, features)\n","\n","        features = self.tokenizer.pad(\n","            features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=return_tensors,\n","        )\n","\n","        # prepare decoder_input_ids\n","        if (\n","            \"labels\" in features\n","            and \"decoder_input_ids\" not in features\n","            and self.model is not None\n","            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n","        ):\n","            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(\n","                labels=features[\"labels\"]\n","            )\n","            features[\"decoder_input_ids\"] = decoder_input_ids\n","\n","        return features\n","\n","    def pad_feature(self, feature_name, features, pad_id=0):\n","        items = (\n","            [feature[feature_name] for feature in features]\n","            if feature_name in features[0].keys()\n","            else None\n","        )\n","        # We have to pad the feature before calling `tokenizer.pad` as this method won't pad them and needs them of the\n","        # same length to return tensors.\n","        if items is not None:\n","            max_item_length = max(len(l) for l in items)\n","            if self.pad_to_multiple_of is not None:\n","                max_item_length = (\n","                    (max_item_length + self.pad_to_multiple_of - 1)\n","                    // self.pad_to_multiple_of\n","                    * self.pad_to_multiple_of\n","                )\n","\n","            padding_side = self.tokenizer.padding_side\n","            for feature in features:\n","                remainder = [pad_id] * (max_item_length - len(feature[feature_name]))\n","                if isinstance(feature[feature_name], list):\n","                    feature[feature_name] = (\n","                        feature[feature_name] + remainder\n","                        if padding_side == \"right\"\n","                        else remainder + feature[feature_name]\n","                    )\n","                elif padding_side == \"right\":\n","                    feature[feature_name] = np.concatenate(\n","                        [feature[feature_name], remainder]\n","                    )\n","                else:\n","                    feature[feature_name] = np.concatenate(\n","                        [remainder, feature[feature_name]]\n","                    )\n","\n","\n","def save_checkpoint(\n","    model, optimizer, scheduler, epoch, step, checkpoint_counter, checkpoint_path\n","):\n","    checkpoint = {\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","        \"scheduler_state_dict\": scheduler.state_dict(),\n","        \"epoch\": epoch,\n","        \"step\": step,\n","        \"checkpoint_counter\": checkpoint_counter,\n","    }\n","\n","    torch.save(checkpoint, checkpoint_path)\n","\n","\n","def load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None):\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","    if scheduler is not None:\n","        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","    epoch = checkpoint[\"epoch\"]\n","    step = checkpoint[\"step\"]\n","    checkpoint_counter = checkpoint[\"checkpoint_counter\"]\n","\n","    print(f\"Loaded checkpoint from '{checkpoint_path}'\")\n","\n","    return model, optimizer, scheduler, epoch, step, checkpoint_counter\n","\n","def prepare_inputs_for_train(\n","    dataset: datasets.DatasetDict,\n","    checkpoints: Dict[str, str],\n","    filename_fn,\n","    add_history=False,\n","    num_processes=None,\n","    verbose=True,\n","    **preprocessing_kwargs,\n","):\n","    for name, checkpoint in checkpoints.items():\n","        tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n","        preprocessing = CoQADatasetPreprocessing(tokenizer, **preprocessing_kwargs)\n","\n","        if verbose:\n","            print(\"Preparing inputs for\", name, \"...\")\n","\n","        if not os.path.exists(filename_fn(name)):\n","            dataset_ = dataset.map(\n","                preprocessing.process_data_to_model_inputs,\n","                fn_kwargs={\"add_history\": add_history},\n","                batched=True,\n","                remove_columns=dataset[\"train\"].column_names,\n","                num_proc=num_processes,\n","            )\n","\n","            dataset_.save_to_disk(filename_fn(name))\n","            del dataset_\n","\n","        if verbose:\n","            dataset_ = datasets.load_from_disk(filename_fn(name))\n","            print(dataset_)\n","            print()\n","            print(\"Showing some input examples:\")\n","            decoded_inputs = tokenizer.batch_decode(dataset_[\"train\"][:5][\"input_ids\"])\n","            for decoded in decoded_inputs:\n","                print(decoded)\n","            print()\n","            del dataset_"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.184314Z","iopub.status.busy":"2023-08-02T15:57:47.183878Z","iopub.status.idle":"2023-08-02T15:57:47.198916Z","shell.execute_reply":"2023-08-02T15:57:47.197893Z","shell.execute_reply.started":"2023-08-02T15:57:47.184277Z"},"trusted":true},"outputs":[],"source":["\"\"\"Functions taken from [the official evaluation script]\n","(https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/)\n","for SQuAD version 2.0.\n","\"\"\"\n","\n","import collections\n","import re\n","import string\n","\n","\n","def normalize_answer(s):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","\n","    def remove_articles(text):\n","        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n","        return re.sub(regex, \" \", text)\n","\n","    def white_space_fix(text):\n","        return \" \".join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return \"\".join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","\n","def get_tokens(s):\n","    if not s:\n","        return []\n","    return normalize_answer(s).split()\n","\n","\n","def compute_f1(a_gold, a_pred):\n","    gold_toks = get_tokens(a_gold)\n","    pred_toks = get_tokens(a_pred)\n","    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","    num_same = sum(common.values())\n","    if len(gold_toks) == 0 or len(pred_toks) == 0:\n","        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n","        return float(gold_toks == pred_toks)\n","    if num_same == 0:\n","        return 0.\n","    precision = 1.0 * num_same / len(pred_toks)\n","    recall = 1.0 * num_same / len(gold_toks)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","\n","def squad_f1(targets, predictions):\n","    f1 = 0.0\n","    for a_gold, a_pred in zip(targets, predictions):\n","        f1 += compute_f1(a_gold, a_pred)\n","    return f1 / len(targets)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.201193Z","iopub.status.busy":"2023-08-02T15:57:47.200504Z","iopub.status.idle":"2023-08-02T15:57:47.830225Z","shell.execute_reply":"2023-08-02T15:57:47.829178Z","shell.execute_reply.started":"2023-08-02T15:57:47.201159Z"},"trusted":true},"outputs":[],"source":["from typing import List, Optional, Union\n","import numpy as np\n","\n","import torch\n","from torchmetrics.classification import MulticlassF1Score\n","\n","import datasets\n","from accelerate import Accelerator\n","\n","\n","per_token_f1_metric = MulticlassF1Score(\n","    num_classes=2,\n","    average=\"macro\",\n","    multidim_average=\"samplewise\",\n","    ignore_index=-100,\n",")\n","\n","macro_f1 = MulticlassF1Score(\n","    num_classes=3,\n","    average=\"macro\",\n","    ignore_index=-100,\n",")\n","\n","\n","def labels_to_answer(labels: torch.Tensor, tokenizer, ignore_index=-100) -> str:\n","    labels[labels == ignore_index] = tokenizer.pad_token_id\n","    answer = tokenizer.decode(labels, skip_special_tokens=True)\n","    return answer\n","\n","\n","def pad_input_tensors(inputs, collator):\n","    features = [dict(zip(inputs.keys(), values)) for values in zip(*inputs.values())]\n","    features = collator(features)\n","\n","    return features\n","\n","\n","def evaluate_answer(example: dict):\n","    return {\"answer_f1\": compute_f1(example[\"answer\"], example[\"pred_answer\"])}\n","\n","\n","def evaluate_rationale_f1(example: dict):\n","    rationale_f1 = per_token_f1_metric(\n","        example[\"pred_rationale_labels\"].long(),\n","        example[\"rationale_labels\"].long(),\n","    )\n","    # Ensure it is an array, not a scalar\n","    if rationale_f1.dim() == 0:\n","        rationale_f1.unsqueeze_(dim=0)\n","    return {\"rationale_f1\": rationale_f1}\n","\n","\n","def evaluate_model(model, tokenizer, dataset: datasets.Dataset, config):\n","    accelerator = Accelerator(mixed_precision=config.mixed_precision, cpu=config.cpu)\n","    model = accelerator.prepare(model)\n","    model.eval()\n","\n","    collator = DynamicPaddingCollatorForSeq2Seq(tokenizer, model)\n","\n","    dataset = dataset.map(\n","        lambda example: pad_input_tensors(example, collator),\n","        batched=True,\n","        batch_size=config.generate_batch_size,\n","        load_from_cache_file=False,\n","    )\n","\n","    dataset = dataset.with_format(\"torch\", device=model.device)\n","\n","    dataset = dataset.map(\n","        lambda example: generate_answer(model, tokenizer, example),\n","        batched=True,\n","        batch_size=config.generate_batch_size,\n","        load_from_cache_file=False,\n","    )\n","\n","    dataset = dataset.map(\n","        lambda example: {\n","            \"answer\": labels_to_answer(example[\"labels\"], tokenizer=tokenizer)\n","        },\n","        load_from_cache_file=False,\n","    )\n","\n","    dataset = dataset.map(evaluate_answer, load_from_cache_file=False)\n","    dataset = dataset.map(\n","        evaluate_rationale_f1,\n","        batched=True,\n","        batch_size=config.generate_batch_size,\n","        load_from_cache_file=False,\n","    )\n","    macro_f1_ = macro_f1.to(model.device)\n","    yng_f1 = macro_f1_(dataset[\"pred_yng_label\"], dataset[\"yng_label\"]).item()\n","\n","    rationale_f1 = torch.mean(dataset[\"rationale_f1\"]).item()\n","    answer_squad_f1 = torch.mean(dataset[\"answer_f1\"]).item()\n","\n","    dataset.reset_format()\n","    return dataset, {\n","        \"yng_f1\": yng_f1,\n","        \"rationale_f1\": rationale_f1,\n","        \"answer_squad_f1\": answer_squad_f1,\n","    }\n","\n","\n","def evaluate_model_raw_data(model, tokenizer, data):\n","    model.eval()\n","    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    preprocessing = CoQADatasetPreprocessing(tokenizer, **CONFIG.preprocessing.__dict__)\n","\n","    outputs = data.map(\n","        lambda example: generate_answer_from_raw_data(\n","            model, tokenizer, preprocessing, example[\"passage\"], example[\"question\"]\n","        ),\n","        batched=True,\n","        batch_size=32,\n","    )\n","\n","    outputs = outputs.map(evaluate_answer)\n","\n","    return outputs.select_columns(\n","        [\n","            \"source\",\n","            \"passage\",\n","            \"question\",\n","            \"rationale\",\n","            \"answer\",\n","            \"pred_answer\",\n","            \"answer_type\",\n","            \"answer_squad_f1\",\n","        ]\n","    )\n","\n","\n","def generate_answer_from_raw_data(\n","    model,\n","    tokenizer,\n","    preprocessing,\n","    passage: Union[str, List[str]],\n","    question: Union[str, List[str]],\n","    history: Optional[Union[str, List[str]]] = None,\n",") -> List[str]:\n","    use_history = history is not None\n","    preprocess = batched_function(preprocessing.preprocess_texts)\n","    if isinstance(passage, str):\n","        passage = [passage]\n","        question = [question]\n","        history = [history]\n","\n","    inputs = {\n","        \"id\": list(range(len(passage))),\n","        \"passage\": passage,\n","        \"question\": question,\n","    }\n","    if use_history:\n","        inputs[\"history\"] = history\n","\n","    inputs = preprocess(inputs)\n","    inputs = preprocessing.process_data_to_model_inputs(\n","        inputs, add_history=use_history, padding=\"max_length\"\n","    )\n","    inputs = inputs.convert_to_tensors(\"pt\")\n","\n","    return generate_answer(model, tokenizer, inputs)\n","\n","\n","def generate_answer(model, tokenizer, inputs):\n","    encoder = model.get_encoder()\n","    encoder_inputs = prepare_model_inputs(encoder, inputs)\n","\n","    inputs = prepare_model_inputs(model, inputs)\n","    inputs.pop(\"decoder_input_ids\", None)\n","\n","    with torch.no_grad():\n","        encoder_outputs = model.encoder(**encoder_inputs, return_dict=True)\n","        # output_str = np.zeros(\n","        #     encoder_outputs[\"last_hidden_state\"].shape[0], dtype=np.string_\n","        # )\n","        outputs = model.generate(**inputs)\n","\n","    answer_types = logits_to_class(encoder_outputs[\"yng_logits\"], task=\"multiclass\")\n","    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    for i in range(len(output_str)):\n","        if answer_types[i] != 2:\n","            output_str[i] = idx_to_answer(answer_types[i])\n","\n","    return {\n","        \"pred_answer\": output_str,\n","        \"yng_logits\": encoder_outputs[\"yng_logits\"],\n","        \"pred_yng_label\": logits_to_class(\n","            encoder_outputs[\"yng_logits\"], task=\"multiclass\"\n","        ),\n","        \"rationale_logits\": encoder_outputs[\"rationale_logits\"],\n","        \"pred_rationale_labels\": logits_to_class(\n","            encoder_outputs[\"rationale_logits\"], task=\"binary\"\n","        ),\n","    }\n"]},{"cell_type":"markdown","metadata":{},"source":["# Pipeline"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.859701Z","iopub.status.busy":"2023-08-02T15:57:47.859016Z","iopub.status.idle":"2023-08-02T15:57:47.928384Z","shell.execute_reply":"2023-08-02T15:57:47.927437Z","shell.execute_reply.started":"2023-08-02T15:57:47.859673Z"},"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","import gc\n","import inspect\n","import os\n","import torch\n","import torch.nn as nn\n","\n","from tqdm import tqdm\n","\n","\n","import wandb\n","import transformers\n","import datasets\n","from accelerate import Accelerator\n","\n","from typing import Dict, Tuple, Union\n","\n","\n","def pipeline(hyperparameters: dict):\n","    with wandb.init(**CONFIG.wandbConfig.__dict__, config=hyperparameters):\n","        config = wandb.config\n","\n","        set_seed(config.seed)\n","\n","        (\n","            model,\n","            tokenizer,\n","            train_data,\n","            val_data,\n","            train_dataloader,\n","            val_dataloader,\n","            loss_fn,\n","            optimizer,\n","            scheduler,\n","            # metrics,\n","        ) = make(config)\n","        print(model)\n","\n","        train(\n","            model,\n","            train_dataloader,\n","            val_dataloader,\n","            loss_fn,\n","            optimizer,\n","            scheduler,\n","            config,\n","            # metrics=metrics,\n","        )\n","\n","        evaluate(model, tokenizer, train_data, val_data, config)\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","    return model\n","\n","\n","def make(config):\n","    # Make the model\n","    tokenizer, model = make_model(config)\n","\n","    # Make the data\n","    train_data, val_data = get_data(\"train\", config), get_data(\"validation\", config)\n","    train_dataloader = make_dataloader(train_data, tokenizer, config, split=\"train\")\n","    val_dataloader = make_dataloader(val_data, tokenizer, config, split=\"validation\")\n","\n","    # Make the loss, the optimizer and the scheduler\n","    loss_fn = make_loss(config)\n","    optimizer = make_optimizer(model, loss_fn, config)\n","    scheduler = make_scheduler(\n","        optimizer, steps_per_epoch=len(train_dataloader), config=config\n","    )\n","    tf_scheduler = make_teacher_force_scheduler(\n","        steps_per_epoch=len(train_dataloader), config=config\n","    )\n","\n","    # # Make the evaluation metrics\n","    # metrics = make_metrics(tokenizer, config)\n","\n","    return (\n","        model,\n","        tokenizer,\n","        train_data,\n","        val_data,\n","        train_dataloader,\n","        val_dataloader,\n","        loss_fn,\n","        optimizer,\n","        scheduler,\n","        tf_scheduler,\n","        # metrics,\n","    )\n","\n","\n","def make_model(config):\n","    checkpoint = CONFIG.checkpoints.__dict__[config.checkpoint_name]\n","    if config.model_type == \"encoder_decoder\":\n","        return make_encoder_decoder_model(\n","            checkpoint=checkpoint,\n","            decoder_max_length=CONFIG.decoder_max_length,\n","            generation_kwargs=CONFIG.generation,\n","            initialize_cross_attention=config.initialize_cross_attention,\n","        )\n","    if config.model_type == \"encoder\":\n","        return make_qa_encoder(checkpoint=checkpoint)\n","\n","    raise ValueError(\n","        \"Invalid model_type. Supported values are 'encoder_decoder' and 'encoder'.\"\n","    )\n","\n","\n","def get_data(split: str, config):\n","    if config.get(\"history_length\", 0) > 0:\n","        path = CONFIG.dataset.train_with_history(config.checkpoint_name, split=split)\n","    else:\n","        path = CONFIG.dataset.train_no_history(config.checkpoint_name, split=split)\n","\n","    dataset = datasets.load_from_disk(path)\n","    dataset = dataset.remove_columns(\n","        [\"id\", \"turn\", \"offset_mapping\", \"rationale_start\", \"rationale_end\"]\n","    )\n","    return dataset\n","\n","\n","def make_dataloader(dataset, tokenizer, config, split: str):\n","    data_collator = DynamicPaddingCollatorForSeq2Seq(tokenizer)\n","    dataloader = create_reproducible_dataloader(\n","        dataset,\n","        batch_size=config.val_batch_size\n","        if split != \"train\" and \"val_batch_size\" in config\n","        else config.batch_size,\n","        collate_fn=data_collator,\n","        num_workers=config.val_num_workers\n","        if split != \"train\" and \"val_num_workers\" in config\n","        else config.num_workers,\n","        pin_memory=True,\n","        shuffle=True,\n","    )\n","    return dataloader\n","\n","\n","def make_loss(config) -> ComputeLoss:\n","    if config.model_type == \"encoder_decoder\":\n","        # loss = composite_loss(\n","        #     Criterion(\n","        #         \"rationale_loss\", encoder_decoder_rationale_loss, weight=config.rationale_loss_weight\n","        #     ),\n","        #     Criterion(\n","        #         \"generative_loss\", encoder_decoder_generative_loss, weight=config.generative_loss_weight\n","        #     ),\n","        # )\n","        # loss = UncertaintyLoss(\n","        #     Criterion(\n","        #         \"rationale_loss\",\n","        #         EncoderDecoderRationaleLoss(\n","        #             max_rationale_length=CONFIG.rationale_max_length\n","        #         ),\n","        #         weight=config.rationale_loss_weight,\n","        #     ),\n","        #     Criterion(\n","        #         \"generative_loss\",\n","        #         encoder_decoder_generative_loss,\n","        #         weight=config.generative_loss_weight,\n","        #     ),\n","        # )\n","\n","        # loss = CompositeLoss(\n","        #     Criterion(\n","        #         \"rationale_loss\",\n","        #         UncertaintyLoss(\n","        #             EncoderDecoderRationaleLoss(\n","        #                 max_rationale_length=CONFIG.rationale_max_length\n","        #             ),\n","        #             initial_weight=config.rationale_loss_weight,\n","        #         ),\n","        #     ),\n","        #     Criterion(\n","        #         \"generative_loss\",\n","        #         UncertaintyLoss(\n","        #             encoder_decoder_generative_loss,\n","        #             initial_weight=config.generative_loss_weight,\n","        #         ),\n","        #     ),\n","        # )\n","\n","        loss = EncoderDecoderLoss(\n","            max_rationale_length=CONFIG.rationale_max_length,\n","            yng_loss_weight=config.yng_loss_weight,\n","            rationale_loss_weight=config.rationale_loss_weight,\n","            generative_loss_weight=config.generative_loss_weight,\n","        )\n","\n","    elif config.model_type == \"encoder\":\n","        loss = EncoderRationaleLoss(max_rationale_length=CONFIG.rationale_max_length)\n","    else:\n","        raise ValueError(\n","            \"Invalid model_type. Supported values are 'encoder_decoder' and 'encoder'.\"\n","        )\n","    return loss\n","\n","\n","def make_optimizer(model, loss_fn, config):\n","    optimizer_cls = getattr(torch.optim, config.optimizer_name)\n","    parameters = [{\"params\": model.parameters()}]\n","    # encoder = model.get_encoder()\n","    # decoder = model.get_decoder()\n","    # parameters = [\n","    #             #   {\"params\": list(decoder.cls.parameters()), \"lr\": config.learning_rate * 10.},\n","    #             #   {\"params\": list(encoder.rationale_head.parameters()), \"lr\": config.learning_rate / 5.},\n","    #             #   {\"params\": list(encoder.yes_no_gen_head.parameters()), \"lr\": config.learning_rate / 10.},\n","    #               ]\n","    # params = set(model.parameters())\n","    # for group in parameters:\n","    #     params.difference_update(group[\"params\"])\n","    # parameters.insert(0, {\"params\": list(params)})\n","\n","    if hasattr(loss_fn, \"_parameters\"):\n","        loss_params = {\"params\": loss_fn.parameters()}\n","        if \"loss_learning_rate\" in config:\n","            loss_params[\"lr\"] = config.loss_learning_rate\n","        parameters.append(loss_params)\n","\n","    return optimizer_cls(\n","        parameters,\n","        lr=config.learning_rate,\n","        **config.get(\"optimizer_args\", {}),\n","    )\n","\n","\n","def make_scheduler(optimizer, steps_per_epoch, config):\n","    total_steps = steps_per_epoch * config.num_epochs\n","    warmup_steps = int(config.warmup_fraction * total_steps)\n","    if config.get(\"scheduler\", \"none\") != \"none\":\n","        return transformers.get_scheduler(\n","            config.scheduler,\n","            optimizer=optimizer,\n","            num_warmup_steps=warmup_steps,\n","            num_training_steps=total_steps,\n","        )\n","\n","    return DummyLRScheduler(optimizer=optimizer)\n","\n","\n","def make_teacher_force_scheduler(steps_per_epoch, config):\n","    total_steps = steps_per_epoch * config.num_epochs\n","    if config.get(\"teacher_force_scheduler\", \"none\") != \"none\":\n","        return LinearScheduler(\n","            start_value=config.tf_start,\n","            end_value=config.tf_end,\n","            total_iters=total_steps,\n","            fraction=config.tf_fraction,\n","        )\n","\n","    return DummyScheduler()\n","\n","\n","# def make_metrics(tokenizer, config) -> Dict[str, Metric]:\n","#     if config.model_type not in [\"encoder_decoder\", \"encoder\"]:\n","#         raise ValueError(\n","#             \"Invalid model_type. Supported values are 'encoder_decoder' and 'encoder'.\"\n","#         )\n","\n","#     metrics = {}\n","#     metrics[\"encoder\"] = {\n","#         \"yng_accuracy\": EncoderYNGAccuracy(),\n","#         \"yng_f1\": EncoderYNGF1(),\n","#         \"rationale_accuracy\": EncoderRationaleAccuracy(),\n","#         \"rationale_f1\": EncoderRationaleF1(),\n","#     }\n","#     if config.model_type == \"encoder_decoder\":\n","#         metrics[\"encoder_decoder\"] = {\n","#             \"squad_f1\": GenerativeSquadF1(tokenizer),\n","#         }\n","\n","#     return metrics\n","\n","\n","def train(\n","    model: nn.Module,\n","    train_dataloader: torch.utils.data.DataLoader,\n","    val_dataloader: torch.utils.data.DataLoader,\n","    loss_fn: Union[ComputeLoss, nn.Module],\n","    optimizer: torch.optim.Optimizer,\n","    lr_scheduler,\n","    config,\n","    teacher_force_scheduler=None,\n","    # metrics: Dict[str, Metric] = {},\n","):\n","    watch_list = [model]\n","\n","    accelerator = Accelerator(mixed_precision=config.mixed_precision, cpu=config.cpu)\n","    (\n","        model,\n","        optimizer,\n","        train_dataloader,\n","        val_dataloader,\n","        lr_scheduler,\n","    ) = accelerator.prepare(\n","        model, optimizer, train_dataloader, val_dataloader, lr_scheduler\n","    )\n","    if isinstance(loss_fn, nn.Module):\n","        watch_list.append(loss_fn)\n","        loss_fn = accelerator.prepare(loss_fn)\n","\n","    wandb.watch(watch_list, log=\"all\", log_freq=config.log_interval)\n","\n","    # Run training and track with wandb\n","    steps_per_epoch = len(train_dataloader)\n","    total_steps = steps_per_epoch * config.num_epochs\n","\n","    checkpoint_counter = 0\n","    step = 0\n","    avg_loss = AvgValue()\n","    avg_inner_losses = defaultdict(AvgValue)\n","    model.train()\n","\n","    forward_signature = set(inspect.signature(model.forward).parameters)\n","    progress_bar = tqdm(range(total_steps))\n","    for epoch in range(config.num_epochs):\n","        for data in train_dataloader:\n","            inputs = {\n","                argument: value\n","                for argument, value in data.items()\n","                if argument in forward_signature\n","            }\n","\n","            lr = lr_scheduler.get_last_lr()[0]\n","            tf = teacher_force_scheduler.get_value() if teacher_force_scheduler is not None else 0.\n","            loss, inner_losses = train_batch(\n","                inputs=inputs,\n","                data=data,\n","                step=step,\n","                model=model,\n","                loss_fn=loss_fn,\n","                optimizer=optimizer,\n","                lr_scheduler=lr_scheduler,\n","                teacher_force_scheduler=teacher_force_scheduler,\n","                accelerator=accelerator,\n","                config=config,\n","            )\n","            progress_bar.update(1)\n","\n","            # Compute statistics\n","            n_samples = len(next(iter(data.values())))\n","            step += 1\n","            avg_loss.update(loss, n_samples)\n","            for loss_name, loss_value in inner_losses.items():\n","                avg_inner_losses[f\"avg_{loss_name}\"].update(loss_value, n_samples)\n","\n","            wandb.log(\n","                {\n","                    \"train_loss\": loss,\n","                    **inner_losses,\n","                    \"lr\": lr,\n","                    \"teacher_force\": tf,\n","                },\n","                step=step,\n","            )\n","\n","            # Evaluate the model and save checkpoints\n","            if (step % config.log_interval == 0) or (step == total_steps):\n","                # Evaluate the model\n","                val_loss, val_inner_losses, val_metrics = train_evaluation(\n","                    model,\n","                    val_dataloader,\n","                    loss_fn,\n","                    # metrics=metrics,\n","                )\n","                model.train()\n","\n","                train_log(\n","                    avg_loss,\n","                    avg_inner_losses,\n","                    val_loss,\n","                    val_inner_losses,\n","                    val_metrics,\n","                    lr=lr,\n","                    teacher_force=tf,\n","                    step=step,\n","                )\n","                avg_loss = AvgValue()\n","                avg_inner_losses = defaultdict(AvgValue)\n","\n","            if step % config.checkpoint_interval == 0:\n","                # Saving checkpoint\n","                save_model_checkpoint(\n","                    accelerator.unwrap_model(model),\n","                    optimizer,\n","                    lr_scheduler,\n","                    epoch,\n","                    step,\n","                    checkpoint_counter,\n","                    config,\n","                )\n","                wandb.log(\n","                    {\n","                        \"checkpoint_counter\": checkpoint_counter,\n","                    },\n","                    step=step,\n","                )\n","                checkpoint_counter += 1\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","    wandb.unwatch(watch_list)\n","    accelerator.free_memory()\n","\n","\n","def train_batch(\n","    inputs,\n","    data,\n","    step,\n","    model,\n","    loss_fn,\n","    optimizer,\n","    lr_scheduler,\n","    config,\n","    teacher_force_scheduler=None,\n","    accelerator=None,\n","    device=None,\n","):\n","    assert (\n","        accelerator is not None or device is not None\n","    ), \"One between accelerator and device must be set.\"\n","\n","    if accelerator is None:\n","        data = {key: value.to(device) for key, value in data.items()}\n","\n","    outputs = model(\n","        **inputs, teacher_force=teacher_force_scheduler.get_value(), return_dict=True\n","    )\n","\n","    loss, inner_losses = loss_fn(outputs, data)\n","    if accelerator is not None:\n","        accelerator.backward(loss)\n","    else:\n","        loss.backward()\n","\n","    if config.get(\"gradient_clip\", \"none\") != \"none\":\n","        if accelerator is not None and accelerator.sync_gradients:\n","            accelerator.clip_grad_norm_(model.parameters(), config.gradient_clip)\n","        else:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n","\n","    if step % config.accumulation_steps == 0:\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    lr_scheduler.step()\n","    teacher_force_scheduler.step()\n","\n","    return loss.item(), inner_losses\n","\n","\n","def train_evaluation(\n","    model,\n","    dataloader,\n","    compute_loss: ComputeLoss = None\n","    # , metrics: Dict[str, Metric] = {}\n",") -> Tuple[AvgValue, Dict[str, AvgValue], Dict[str, AvgValue]]:\n","    model.eval()\n","    avg_loss = AvgValue()\n","    avg_inner_losses = defaultdict(AvgValue)\n","    avg_metrics = defaultdict(AvgValue)\n","\n","    forward_signature = set(inspect.signature(model.forward).parameters)\n","    with torch.no_grad():\n","        for data in dataloader:\n","            inputs_kwargs = {\n","                argument: value\n","                for argument, value in data.items()\n","                if argument in forward_signature\n","            }\n","            n_samples = len(next(iter(data.values())))\n","\n","            outputs = model(**inputs_kwargs, return_dict=True)\n","            if compute_loss is not None:\n","                loss, inner_losses = compute_loss(outputs, data)\n","\n","                avg_loss.update(loss.item(), n_samples)\n","                for loss_name, loss_value in inner_losses.items():\n","                    avg_inner_losses[loss_name].update(loss_value, n_samples)\n","\n","            # for metric_name, metric in metrics.items():\n","            #     metric_value = metric(outputs, data)\n","            #     avg_metrics[metric_name].update(metric_value, n_samples)\n","\n","    return avg_loss, avg_inner_losses, avg_metrics\n","\n","\n","def train_log(\n","    train_loss: AvgValue,\n","    train_inner_losses: Dict[str, AvgValue],\n","    val_loss: AvgValue,\n","    val_inner_losses: Dict[str, AvgValue],\n","    val_metrics: Dict[str, AvgValue],\n","    lr,\n","    step,\n","    teacher_force,\n","):\n","    train_loss = train_loss.value()\n","    train_inner_losses = {\n","        f\"{loss_name}\": loss_value.value()\n","        for loss_name, loss_value in train_inner_losses.items()\n","    }\n","\n","    val_loss = val_loss.value()\n","    val_inner_losses = {\n","        f\"val_{loss_name}\": loss_value.value()\n","        for loss_name, loss_value in val_inner_losses.items()\n","    }\n","\n","    val_metrics = {\n","        f\"val_{metric_name}\": metric_value.value()\n","        for metric_name, metric_value in val_metrics.items()\n","    }\n","\n","    wandb.log(\n","        {\n","            \"avg_train_loss\": train_loss,\n","            **train_inner_losses,\n","            \"val_loss\": val_loss,\n","            **val_inner_losses,\n","            **val_metrics,\n","            \"lr\": lr,\n","            \"teacher_force\": teacher_force,\n","        },\n","        step=step,\n","    )\n","    print(\n","        f\"Iteration: {step:6}\",\n","        f\"train loss: {train_loss:.4f}\",\n","        f\"val loss: {val_loss:.4f}\",\n","        f\"lr: {lr:.6f}\",\n","        sep=\"\\t\",\n","    )\n","\n","\n","def save_model_checkpoint(\n","    model, optimizer, lr_scheduler, epoch, step, checkpoint_counter, config\n","):\n","    checkpoint_dir = os.path.join(\n","        CONFIG.models.checkpoints_dir(\n","            config.model_name, config.get(\"history_length\", 0) > 0\n","        ),\n","        str(config.seed),\n","    )\n","    filename = f\"checkpoint_{checkpoint_counter}.pt\"\n","    checkpoint_file = os.path.join(checkpoint_dir, filename)\n","\n","    create_dirs_for_file(checkpoint_file)\n","    save_checkpoint(\n","        model,\n","        optimizer,\n","        lr_scheduler,\n","        epoch,\n","        step,\n","        checkpoint_counter,\n","        checkpoint_path=checkpoint_file,\n","    )\n","    wandb.save(checkpoint_file)\n","\n","\n","def load_model_checkpoint(\n","    checkpoint_counter, config, model, optimizer=None, lr_scheduler=None\n","):\n","    checkpoint_file = os.path.join(\n","        CONFIG.models.checkpoints_dir(\n","            config.model_name, config.get(\"history_length\", 0) > 0\n","        ),\n","        config.model_name,\n","        f\"checkpoint_{checkpoint_counter}.pt\",\n","    )\n","    return load_checkpoint(\n","        checkpoint_file, model, optimizer=optimizer, scheduler=lr_scheduler\n","    )\n","\n","\n","def evaluate(\n","    model, tokenizer, train_data: datasets.Dataset, val_data: datasets.Dataset, config\n","):\n","    datasets = [(\"train\", train_data), (\"val\", val_data)]\n","    results = {}\n","    for dataset_name, dataset in datasets:\n","        outputs, metrics = evaluate_model(model, tokenizer, dataset, config)\n","        results[dataset_name] = (outputs, metrics)\n","\n","        for metric_name, metric_value in metrics.items():\n","            print(f\"{dataset_name}_{metric_name}: {metric_value:.4f}\")\n","            wandb.log({f\"evaluation/{dataset_name}_{metric_name}\": metric_value})\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","    return results\n"]},{"cell_type":"markdown","metadata":{},"source":["# Run"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.931028Z","iopub.status.busy":"2023-08-02T15:57:47.930358Z","iopub.status.idle":"2023-08-02T15:59:04.346079Z","shell.execute_reply":"2023-08-02T15:59:04.345018Z","shell.execute_reply.started":"2023-08-02T15:57:47.930991Z"},"trusted":true},"outputs":[{"data":{"text/html":["wandb version 0.15.9 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.8"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>d:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\wandb\\run-20230829_203318-mezqz1gc</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/simonemele999/nlp_assignment2/runs/mezqz1gc' target=\"_blank\">glorious-darkness-185</a></strong> to <a href='https://wandb.ai/simonemele999/nlp_assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/simonemele999/nlp_assignment2' target=\"_blank\">https://wandb.ai/simonemele999/nlp_assignment2</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/simonemele999/nlp_assignment2/runs/mezqz1gc' target=\"_blank\">https://wandb.ai/simonemele999/nlp_assignment2/runs/mezqz1gc</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following encoder weights were not tied to the decoder ['bert/pooler']\n"]},{"name":"stdout","output_type":"stream","text":["Cross-attention has been initialized with self-attention weights.\n","QAEncoderDecoderModel(\n","  (encoder): QAEncoder(\n","    (encoder): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n","        (position_embeddings): Embedding(512, 128)\n","        (token_type_embeddings): Embedding(2, 128)\n","        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-1): 2 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=128, out_features=128, bias=True)\n","                (key): Linear(in_features=128, out_features=128, bias=True)\n","                (value): Linear(in_features=128, out_features=128, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=128, out_features=128, bias=True)\n","                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=128, out_features=512, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=512, out_features=128, bias=True)\n","              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=128, out_features=128, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (rationale_head): TokenSelectionHead(\n","      (dense): Linear(in_features=128, out_features=128, bias=False)\n","      (act_fn): ReLU()\n","      (hidden_to_logit): Linear(in_features=128, out_features=1, bias=False)\n","    )\n","    (yes_no_gen_head): YesNoGenHead(\n","      (dense): Linear(in_features=128, out_features=128, bias=False)\n","      (act_fn): ReLU()\n","      (hidden_to_logit): Linear(in_features=128, out_features=1, bias=False)\n","      (softmax): Softmax(dim=-2)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (classifier): Linear(in_features=256, out_features=3, bias=True)\n","    )\n","  )\n","  (decoder): BertLMHeadModel(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n","        (position_embeddings): Embedding(512, 128)\n","        (token_type_embeddings): Embedding(2, 128)\n","        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-1): 2 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=128, out_features=128, bias=True)\n","                (key): Linear(in_features=128, out_features=128, bias=True)\n","                (value): Linear(in_features=128, out_features=128, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=128, out_features=128, bias=True)\n","                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=128, out_features=128, bias=True)\n","                (key): Linear(in_features=128, out_features=128, bias=True)\n","                (value): Linear(in_features=128, out_features=128, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=128, out_features=128, bias=True)\n","                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=128, out_features=512, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=512, out_features=128, bias=True)\n","              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (cls): BertOnlyMLMHead(\n","      (predictions): BertLMPredictionHead(\n","        (transform): BertPredictionHeadTransform(\n","          (dense): Linear(in_features=128, out_features=128, bias=True)\n","          (transform_act_fn): GELUActivation()\n","          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (decoder): Linear(in_features=128, out_features=30522, bias=True)\n","      )\n","    )\n","  )\n",")\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/8025 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","  9%|â         | 701/8025 [01:34<11:22:32,  5.59s/it]"]},{"name":"stdout","output_type":"stream","text":["Iteration:    700\ttrain loss: 1.6388\tval loss: 1.5364\tlr: 0.000435\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|ââ        | 1401/8025 [03:02<8:11:33,  4.45s/it]"]},{"name":"stdout","output_type":"stream","text":["Iteration:   1400\ttrain loss: 1.2444\tval loss: 1.4436\tlr: 0.000459\n"]},{"name":"stderr","output_type":"stream","text":[" 26%|âââ       | 2101/8025 [04:28<7:13:19,  4.39s/it]"]},{"name":"stdout","output_type":"stream","text":["Iteration:   2100\ttrain loss: 1.1495\tval loss: 1.3204\tlr: 0.000410\n"]},{"name":"stderr","output_type":"stream","text":[" 35%|ââââ      | 2801/8025 [05:53<6:39:01,  4.58s/it]"]},{"name":"stdout","output_type":"stream","text":["Iteration:   2800\ttrain loss: 1.0880\tval loss: 1.3012\tlr: 0.000362\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|âââââ     | 3501/8025 [07:16<5:33:48,  4.43s/it]"]},{"name":"stdout","output_type":"stream","text":["Iteration:   3500\ttrain loss: 1.0113\tval loss: 1.2493\tlr: 0.000313\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|ââââââ    | 4200/8025 [08:30<05:00, 12.72it/s]  "]},{"name":"stdout","output_type":"stream","text":["Iteration:   4200\ttrain loss: 1.0063\tval loss: 1.1771\tlr: 0.000265\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|ââââââ    | 4901/8025 [10:10<3:54:10,  4.50s/it]"]},{"name":"stdout","output_type":"stream","text":["Iteration:   4900\ttrain loss: 1.0017\tval loss: 1.1372\tlr: 0.000216\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|âââââââ   | 5601/8025 [11:33<2:56:35,  4.37s/it]"]},{"name":"stdout","output_type":"stream","text":["Iteration:   5600\ttrain loss: 0.9734\tval loss: 1.1234\tlr: 0.000168\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|ââââââââ  | 6300/8025 [12:41<02:14, 12.82it/s]  "]},{"name":"stdout","output_type":"stream","text":["Iteration:   6300\ttrain loss: 0.9022\tval loss: 1.0944\tlr: 0.000120\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|âââââââââ | 7001/8025 [14:20<1:12:17,  4.24s/it]"]},{"name":"stdout","output_type":"stream","text":["Iteration:   7000\ttrain loss: 0.8909\tval loss: 1.0983\tlr: 0.000071\n"]},{"name":"stderr","output_type":"stream","text":[" 96%|ââââââââââ| 7701/8025 [15:44<23:53,  4.43s/it]  "]},{"name":"stdout","output_type":"stream","text":["Iteration:   7700\ttrain loss: 0.8798\tval loss: 1.0850\tlr: 0.000023\n"]},{"name":"stderr","output_type":"stream","text":["100%|ââââââââââ| 8025/8025 [16:21<00:00, 14.43it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:   8025\ttrain loss: 0.8800\tval loss: 1.0766\tlr: 0.000000\n"]},{"name":"stderr","output_type":"stream","text":["100%|ââââââââââ| 8025/8025 [16:38<00:00,  8.03it/s]\n","Parameter 'function'=<function evaluate_model.<locals>.<lambda> at 0x000001A43B54C700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"941464c73633412f9e877f309e0e202f","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e35e6404a2844aa2a726769113d2f19a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"770294c5f10941729ee2f027f5dcaa99","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92c92982171f40d38c63053abea7a369","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"041b1acc1b4446c7bcf46dda43c39f9d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["train_yng_f1: 0.7640\n","train_rationale_f1: 0.5413\n","train_answer_squad_f1: 0.3351\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5773557a89574871aec4332ee47cde94","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"020a35a3247c48c3a56261b41eb4091b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0a054ff6e524d08940367f008dccd05","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b69e3e059a0f43e2a17ffc16b42fcbfc","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57c87ceef7c9482a8d5a87e9e4bfdda4","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["val_yng_f1: 0.7227\n","val_rationale_f1: 0.5277\n","val_answer_squad_f1: 0.2572\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3dc715933b44771a49dca2ab77fef70","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='580.152 MB of 580.152 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0,â¦"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_generative_loss</td><td>ââââââââââââ</td></tr><tr><td>avg_rationale_loss</td><td>ââââââââââââ</td></tr><tr><td>avg_train_loss</td><td>ââââââââââââ</td></tr><tr><td>avg_yng_loss</td><td>ââââââââââââ</td></tr><tr><td>checkpoint_counter</td><td>âââââââââââ</td></tr><tr><td>evaluation/train_answer_squad_f1</td><td>â</td></tr><tr><td>evaluation/train_rationale_f1</td><td>â</td></tr><tr><td>evaluation/train_yng_f1</td><td>â</td></tr><tr><td>evaluation/val_answer_squad_f1</td><td>â</td></tr><tr><td>evaluation/val_rationale_f1</td><td>â</td></tr><tr><td>evaluation/val_yng_f1</td><td>â</td></tr><tr><td>generative_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>lr</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>rationale_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>teacher_force</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>train_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val_generative_loss</td><td>ââââââââââââ</td></tr><tr><td>val_loss</td><td>ââââââââââââ</td></tr><tr><td>val_rationale_loss</td><td>ââââââââââââ</td></tr><tr><td>val_yng_loss</td><td>ââââââââââââ</td></tr><tr><td>yng_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_generative_loss</td><td>2.31834</td></tr><tr><td>avg_rationale_loss</td><td>0.45951</td></tr><tr><td>avg_train_loss</td><td>0.88001</td></tr><tr><td>avg_yng_loss</td><td>0.08123</td></tr><tr><td>checkpoint_counter</td><td>10</td></tr><tr><td>evaluation/train_answer_squad_f1</td><td>0.33511</td></tr><tr><td>evaluation/train_rationale_f1</td><td>0.54125</td></tr><tr><td>evaluation/train_yng_f1</td><td>0.764</td></tr><tr><td>evaluation/val_answer_squad_f1</td><td>0.25721</td></tr><tr><td>evaluation/val_rationale_f1</td><td>0.52766</td></tr><tr><td>evaluation/val_yng_f1</td><td>0.72274</td></tr><tr><td>generative_loss</td><td>3.52329</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>rationale_loss</td><td>0.77995</td></tr><tr><td>teacher_force</td><td>0.3</td></tr><tr><td>train_loss</td><td>1.3317</td></tr><tr><td>val_generative_loss</td><td>3.18896</td></tr><tr><td>val_loss</td><td>1.07656</td></tr><tr><td>val_rationale_loss</td><td>0.48117</td></tr><tr><td>val_yng_loss</td><td>0.08971</td></tr><tr><td>yng_loss</td><td>0.00513</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">glorious-darkness-185</strong> at: <a href='https://wandb.ai/simonemele999/nlp_assignment2/runs/mezqz1gc' target=\"_blank\">https://wandb.ai/simonemele999/nlp_assignment2/runs/mezqz1gc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 11 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>.\\wandb\\run-20230829_203318-mezqz1gc\\logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["class PropertyDict(dict):\n","    def __getattr__(self, key):\n","        if key in self:\n","            return self[key]\n","        raise AttributeError(\n","            f\"'{self.__class__.__name__}' object has no attribute '{key}'\"\n","        )\n","\n","    def __setattr__(self, key, value):\n","        self[key] = value\n","\n","seeds = [42, 2022, 1337]\n","hyperparameters = PropertyDict(\n","    seed=1337,\n","    checkpoint_name=\"bert_tiny\",\n","    model_name=\"bert_tiny\",\n","    model_type=\"encoder_decoder\",\n","    initialize_cross_attention=True,\n","    yng_loss_weight=0.6,\n","    rationale_loss_weight=0.8,\n","    generative_loss_weight=0.2,\n","    batch_size=32,\n","    val_batch_size=64,\n","    generate_batch_size=128,\n","    num_workers=0,\n","    num_epochs=3,\n","    optimizer_name=\"AdamW\",\n","    # learning_rate=5e-3,\n","    learning_rate=5e-4,\n","    loss_learning_rate=1e-1,\n","    scheduler=\"linear\",\n","    warmup_fraction=0.1,\n","    teacher_force_scheduler=\"linear\",\n","    tf_start = 1.,\n","    tf_end = 0.3,\n","    tf_fraction = 0.7,\n","    accumulation_steps=1,\n","    gradient_clip=1.0,\n","    mixed_precision=\"fp16\",\n","    checkpoint_interval=700,\n","    log_interval=700,\n","    cpu=False,\n",")\n","\n","with wandb.init(project=CONFIG.wandbConfig.project, config=hyperparameters):\n","    config = wandb.config\n","\n","    set_seed(config.seed)\n","\n","    # Make the model\n","    tokenizer, model = make_model(config)\n","\n","    # Make the data\n","    train_data = get_data(\"train\", config)\n","    # .shuffle(42).select(range(1000))\n","    val_data = get_data(\"validation\", config)\n","    # .shuffle(42).select(range(64))\n","    train_dataloader = make_dataloader(train_data, tokenizer, config, split=\"train\")\n","    val_dataloader = make_dataloader(val_data, tokenizer, config, split=\"validation\")\n","\n","    # Make the loss, the optimizer and the scheduler\n","    loss_fn = make_loss(config)\n","    optimizer = make_optimizer(model, loss_fn, config)\n","    scheduler = make_scheduler(\n","        optimizer, steps_per_epoch=len(train_dataloader), config=config\n","    )\n","    tf_scheduler = make_teacher_force_scheduler(steps_per_epoch=len(train_dataloader), config=config)\n","\n","    # model, train_dataloader, val_dataloader, loss_fn, optimizer, scheduler, metrics = make(config)\n","    print(model)\n","\n","    train(\n","        model,\n","        train_dataloader,\n","        val_dataloader,\n","        loss_fn,\n","        optimizer,\n","        scheduler,\n","        config,\n","        teacher_force_scheduler=tf_scheduler,\n","    )\n","\n","    results = evaluate(model, tokenizer, train_data, val_data, config)\n","\n","# pipeline(hyperparameters)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"No active exception to reraise","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m\n","\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"]}],"source":["raise"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import WeightedRandomSampler\n","\n","yng_labels = torch.LongTensor(train_data[\"yng_label\"])\n","_, counts = torch.unique(yng_labels, return_counts=True)\n","class_freq = 1./ counts\n","weights = class_freq[yng_labels]\n","sampler = WeightedRandomSampler(\n","    weights=weights,\n","    num_samples=weights.shape[0],\n","    replacement=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset = train_data\n","data_collator = DynamicPaddingCollatorForSeq2Seq(tokenizer)\n","dataloader = create_reproducible_dataloader(\n","        dataset,\n","        batch_size=32,\n","        collate_fn=data_collator,\n","        num_workers=0,\n","        pin_memory=True,\n","        # shuffle=True,\n","        sampler = sampler\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["2675"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["2675\n","2675\n"]},{"data":{"text/plain":["[tensor([10, 13,  9]),\n"," tensor([15,  3, 14]),\n"," tensor([10,  5, 17]),\n"," tensor([10,  6, 16]),\n"," tensor([ 8, 15,  9]),\n"," tensor([11, 10, 11]),\n"," tensor([10, 13,  9]),\n"," tensor([ 8, 13, 11]),\n"," tensor([10, 13,  9]),\n"," tensor([ 9, 11, 12]),\n"," tensor([ 5, 12, 15]),\n"," tensor([13,  6, 13]),\n"," tensor([10, 15,  7]),\n"," tensor([ 8, 15,  9]),\n"," tensor([ 7,  7, 18]),\n"," tensor([ 9,  9, 14]),\n"," tensor([13,  5, 14]),\n"," tensor([ 8, 12, 12]),\n"," tensor([10, 15,  7]),\n"," tensor([11, 12,  9]),\n"," tensor([ 8, 12, 12]),\n"," tensor([ 8, 16,  8]),\n"," tensor([ 9,  8, 15]),\n"," tensor([11,  8, 13]),\n"," tensor([12,  6, 14]),\n"," tensor([ 8, 10, 14]),\n"," tensor([11, 11, 10]),\n"," tensor([ 9, 11, 12]),\n"," tensor([10, 11, 11]),\n"," tensor([10,  9, 13]),\n"," tensor([12, 10, 10]),\n"," tensor([13,  9, 10]),\n"," tensor([13, 12,  7]),\n"," tensor([12, 10, 10]),\n"," tensor([15,  5, 12]),\n"," tensor([ 8, 15,  9]),\n"," tensor([ 7, 10, 15]),\n"," tensor([ 8, 19,  5]),\n"," tensor([13,  8, 11]),\n"," tensor([ 4, 12, 16]),\n"," tensor([15, 11,  6]),\n"," tensor([11, 13,  8]),\n"," tensor([11, 15,  6]),\n"," tensor([14,  9,  9]),\n"," tensor([14,  7, 11]),\n"," tensor([ 7, 12, 13]),\n"," tensor([ 8,  8, 16]),\n"," tensor([10, 10, 12]),\n"," tensor([13, 10,  9]),\n"," tensor([ 7, 10, 15]),\n"," tensor([12, 11,  9]),\n"," tensor([13,  6, 13]),\n"," tensor([ 6, 15, 11]),\n"," tensor([15,  8,  9]),\n"," tensor([13,  7, 12]),\n"," tensor([10, 12, 10]),\n"," tensor([ 9, 13, 10]),\n"," tensor([ 7, 12, 13]),\n"," tensor([12, 10, 10]),\n"," tensor([13,  9, 10]),\n"," tensor([14, 11,  7]),\n"," tensor([ 8, 12, 12]),\n"," tensor([15, 10,  7]),\n"," tensor([ 9, 10, 13]),\n"," tensor([13, 14,  5]),\n"," tensor([ 6, 11, 15]),\n"," tensor([15, 11,  6]),\n"," tensor([11,  5, 16]),\n"," tensor([13,  9, 10]),\n"," tensor([10, 11, 11]),\n"," tensor([ 8, 13, 11]),\n"," tensor([12,  8, 12]),\n"," tensor([ 8, 13, 11]),\n"," tensor([ 7, 13, 12]),\n"," tensor([11,  6, 15]),\n"," tensor([13,  8, 11]),\n"," tensor([12,  7, 13]),\n"," tensor([10,  8, 14]),\n"," tensor([12, 12,  8]),\n"," tensor([13,  9, 10]),\n"," tensor([11, 10, 11]),\n"," tensor([ 8, 12, 12]),\n"," tensor([10, 12, 10]),\n"," tensor([ 9, 12, 11]),\n"," tensor([ 9, 12, 11]),\n"," tensor([12, 10, 10]),\n"," tensor([10, 12, 10]),\n"," tensor([ 9,  9, 14]),\n"," tensor([14,  8, 10]),\n"," tensor([ 9, 10, 13]),\n"," tensor([13, 15,  4]),\n"," tensor([13, 11,  8]),\n"," tensor([16,  7,  9]),\n"," tensor([10, 13,  9]),\n"," tensor([10, 13,  9]),\n"," tensor([12,  7, 13]),\n"," tensor([ 8, 15,  9]),\n"," tensor([12, 10, 10]),\n"," tensor([11, 11, 10]),\n"," tensor([ 7, 16,  9]),\n"," tensor([12,  9, 11]),\n"," tensor([10, 13,  9]),\n"," tensor([ 4, 15, 13]),\n"," tensor([ 8, 11, 13]),\n"," tensor([11,  6, 15]),\n"," tensor([ 9, 14,  9]),\n"," tensor([11, 10, 11]),\n"," tensor([ 9,  9, 14]),\n"," tensor([ 9, 12, 11]),\n"," tensor([11,  9, 12]),\n"," tensor([ 6, 14, 12]),\n"," tensor([ 9, 10, 13]),\n"," tensor([16, 10,  6]),\n"," tensor([ 7, 13, 12]),\n"," tensor([13, 14,  5]),\n"," tensor([14,  7, 11]),\n"," tensor([13, 10,  9]),\n"," tensor([13, 13,  6]),\n"," tensor([ 7, 18,  7]),\n"," tensor([ 6, 13, 13]),\n"," tensor([12, 11,  9]),\n"," tensor([ 5, 15, 12]),\n"," tensor([12,  8, 12]),\n"," tensor([ 6, 11, 15]),\n"," tensor([13, 11,  8]),\n"," tensor([10, 12, 10]),\n"," tensor([10, 12, 10]),\n"," tensor([13,  8, 11]),\n"," tensor([13, 13,  6]),\n"," tensor([10, 11, 11]),\n"," tensor([12, 10, 10]),\n"," tensor([ 9, 11, 12]),\n"," tensor([ 6, 13, 13]),\n"," tensor([ 8,  8, 16]),\n"," tensor([12,  9, 11]),\n"," tensor([12,  7, 13]),\n"," tensor([ 8, 12, 12]),\n"," tensor([ 9, 12, 11]),\n"," tensor([12,  8, 12]),\n"," tensor([15,  8,  9]),\n"," tensor([11, 11, 10]),\n"," tensor([14,  8, 10]),\n"," tensor([14, 12,  6]),\n"," tensor([10, 10, 12]),\n"," tensor([ 7, 15, 10]),\n"," tensor([12, 10, 10]),\n"," tensor([ 9,  6, 17]),\n"," tensor([ 7, 11, 14]),\n"," tensor([13,  7, 12]),\n"," tensor([ 9, 16,  7]),\n"," tensor([10, 13,  9]),\n"," tensor([15, 10,  7]),\n"," tensor([10,  8, 14]),\n"," tensor([10, 11, 11]),\n"," tensor([ 8, 11, 13]),\n"," tensor([11, 11, 10]),\n"," tensor([ 9,  9, 14]),\n"," tensor([10,  8, 14]),\n"," tensor([ 7, 12, 13]),\n"," tensor([11, 10, 11]),\n"," tensor([ 9, 12, 11]),\n"," tensor([13, 10,  9]),\n"," tensor([15,  9,  8]),\n"," tensor([11, 11, 10]),\n"," tensor([13, 11,  8]),\n"," tensor([12, 11,  9]),\n"," tensor([ 9, 11, 12]),\n"," tensor([ 8,  8, 16]),\n"," tensor([11,  8, 13]),\n"," tensor([12,  7, 13]),\n"," tensor([13, 11,  8]),\n"," tensor([ 9,  8, 15]),\n"," tensor([10, 13,  9]),\n"," tensor([11,  9, 12]),\n"," tensor([12, 12,  8]),\n"," tensor([11,  7, 14]),\n"," tensor([13,  9, 10]),\n"," tensor([ 7, 14, 11]),\n"," tensor([ 7, 13, 12]),\n"," tensor([11, 13,  8]),\n"," tensor([ 9, 10, 13]),\n"," tensor([ 7, 12, 13]),\n"," tensor([11, 12,  9]),\n"," tensor([12, 11,  9]),\n"," tensor([12, 12,  8]),\n"," tensor([14, 11,  7]),\n"," tensor([11, 13,  8]),\n"," tensor([10, 11, 11]),\n"," tensor([ 9, 13, 10]),\n"," tensor([13,  8, 11]),\n"," tensor([ 8, 14, 10]),\n"," tensor([15,  7, 10]),\n"," tensor([12, 10, 10]),\n"," tensor([15,  6, 11]),\n"," tensor([10, 14,  8]),\n"," tensor([ 9, 15,  8]),\n"," tensor([12, 10, 10]),\n"," tensor([10, 10, 12]),\n"," tensor([ 5, 11, 16]),\n"," tensor([13,  8, 11]),\n"," tensor([ 9,  8, 15]),\n"," tensor([13, 11,  8]),\n"," tensor([12,  9, 11]),\n"," tensor([14, 11,  7]),\n"," tensor([15,  7, 10]),\n"," tensor([11, 12,  9]),\n"," tensor([ 9, 12, 11]),\n"," tensor([14,  8, 10]),\n"," tensor([13, 12,  7]),\n"," tensor([12, 14,  6]),\n"," tensor([11,  9, 12]),\n"," tensor([13, 11,  8]),\n"," tensor([ 8, 12, 12]),\n"," tensor([ 9, 15,  8]),\n"," tensor([10, 14,  8]),\n"," tensor([12, 10, 10]),\n"," tensor([ 9,  8, 15]),\n"," tensor([13,  8, 11]),\n"," tensor([15,  6, 11]),\n"," tensor([ 6, 16, 10]),\n"," tensor([ 7, 14, 11]),\n"," tensor([10,  7, 15]),\n"," tensor([ 8, 12, 12]),\n"," tensor([10, 10, 12]),\n"," tensor([10, 11, 11]),\n"," tensor([ 6,  9, 17]),\n"," tensor([14,  7, 11]),\n"," tensor([13,  7, 12]),\n"," tensor([11,  9, 12]),\n"," tensor([ 6, 14, 12]),\n"," tensor([12, 10, 10]),\n"," tensor([ 9,  8, 15]),\n"," tensor([16,  8,  8]),\n"," tensor([10, 11, 11]),\n"," tensor([10,  9, 13]),\n"," tensor([12, 12,  8]),\n"," tensor([11, 12,  9]),\n"," tensor([ 8, 16,  8]),\n"," tensor([10, 10, 12]),\n"," tensor([ 7, 15, 10]),\n"," tensor([15,  9,  8]),\n"," tensor([ 6, 14, 12]),\n"," tensor([ 8, 13, 11]),\n"," tensor([10,  9, 13]),\n"," tensor([10,  6, 16]),\n"," tensor([14,  7, 11]),\n"," tensor([10,  6, 16]),\n"," tensor([ 7, 12, 13]),\n"," tensor([12, 11,  9]),\n"," tensor([ 8, 13, 11]),\n"," tensor([12, 11,  9]),\n"," tensor([13,  5, 14]),\n"," tensor([ 8, 12, 12]),\n"," tensor([10, 13,  9]),\n"," tensor([13, 11,  8]),\n"," tensor([ 9,  6, 17]),\n"," tensor([13, 11,  8]),\n"," tensor([ 7, 16,  9]),\n"," tensor([11, 13,  8]),\n"," tensor([14,  8, 10]),\n"," tensor([ 9, 12, 11]),\n"," tensor([11, 11, 10]),\n"," tensor([ 9, 12, 11]),\n"," tensor([10, 12, 10]),\n"," tensor([10, 12, 10]),\n"," tensor([ 7, 12, 13]),\n"," tensor([ 7, 13, 12]),\n"," tensor([17,  7,  8]),\n"," tensor([ 9,  8, 15]),\n"," tensor([ 9, 14,  9]),\n"," tensor([17,  6,  9]),\n"," tensor([11,  8, 13]),\n"," tensor([12,  9, 11]),\n"," tensor([14,  9,  9]),\n"," tensor([16,  8,  8]),\n"," tensor([ 7, 10, 15]),\n"," tensor([11, 12,  9]),\n"," tensor([ 7, 12, 13]),\n"," tensor([11,  8, 13]),\n"," tensor([11, 16,  5]),\n"," tensor([13, 10,  9]),\n"," tensor([ 9, 14,  9]),\n"," tensor([11, 12,  9]),\n"," tensor([15, 10,  7]),\n"," tensor([17,  9,  6]),\n"," tensor([10,  9, 13]),\n"," tensor([ 9,  6, 17]),\n"," tensor([ 8,  9, 15]),\n"," tensor([ 8, 10, 14]),\n"," tensor([11, 11, 10]),\n"," tensor([13, 12,  7]),\n"," tensor([10, 13,  9]),\n"," tensor([10, 10, 12]),\n"," tensor([10, 11, 11]),\n"," tensor([ 8, 10, 14]),\n"," tensor([12, 14,  6]),\n"," tensor([ 2, 15, 15]),\n"," tensor([11, 10, 11]),\n"," tensor([ 9, 15,  8]),\n"," tensor([10, 13,  9]),\n"," tensor([11,  8, 13]),\n"," tensor([13, 11,  8]),\n"," tensor([13, 10,  9]),\n"," tensor([11, 12,  9]),\n"," tensor([ 9, 13, 10]),\n"," tensor([ 7, 16,  9]),\n"," tensor([11,  9, 12]),\n"," tensor([11, 13,  8]),\n"," tensor([10, 14,  8]),\n"," tensor([11,  9, 12]),\n"," tensor([12,  9, 11]),\n"," tensor([ 9, 15,  8]),\n"," tensor([10, 13,  9]),\n"," tensor([ 7, 16,  9]),\n"," tensor([14,  5, 13]),\n"," tensor([ 8, 13, 11]),\n"," tensor([11, 14,  7]),\n"," tensor([15,  9,  8]),\n"," tensor([13, 10,  9]),\n"," tensor([10, 10, 12]),\n"," tensor([13,  7, 12]),\n"," tensor([ 8,  9, 15]),\n"," tensor([10, 13,  9]),\n"," tensor([10, 14,  8]),\n"," tensor([13,  7, 12]),\n"," tensor([11, 12,  9]),\n"," tensor([12, 14,  6]),\n"," tensor([17, 10,  5]),\n"," tensor([13,  7, 12]),\n"," tensor([14,  7, 11]),\n"," tensor([14,  8, 10]),\n"," tensor([12, 10, 10]),\n"," tensor([11, 12,  9]),\n"," tensor([ 8, 16,  8]),\n"," tensor([11, 12,  9]),\n"," tensor([ 8, 15,  9]),\n"," tensor([15,  8,  9]),\n"," tensor([ 9,  9, 14]),\n"," tensor([ 7, 16,  9]),\n"," tensor([ 6, 13, 13]),\n"," tensor([12,  6, 14]),\n"," tensor([10, 13,  9]),\n"," tensor([ 8, 13, 11]),\n"," tensor([10,  7, 15]),\n"," tensor([ 9,  6, 17]),\n"," tensor([11,  8, 13]),\n"," tensor([12, 11,  9]),\n"," tensor([12, 13,  7]),\n"," tensor([ 8, 19,  5]),\n"," tensor([14, 11,  7]),\n"," tensor([11, 13,  8]),\n"," tensor([18,  4, 10]),\n"," tensor([ 9, 12, 11]),\n"," tensor([13, 11,  8]),\n"," tensor([11,  9, 12]),\n"," tensor([11, 13,  8]),\n"," tensor([14,  8, 10]),\n"," tensor([ 9, 12, 11]),\n"," tensor([18,  7,  7]),\n"," tensor([13,  9, 10]),\n"," tensor([10, 11, 11]),\n"," tensor([ 9, 16,  7]),\n"," tensor([10, 11, 11]),\n"," tensor([ 4, 15, 13]),\n"," tensor([11, 15,  6]),\n"," tensor([12, 12,  8]),\n"," tensor([10, 12, 10]),\n"," tensor([13,  6, 13]),\n"," tensor([12,  9, 11]),\n"," tensor([ 6,  9, 17]),\n"," tensor([11, 11, 10]),\n"," tensor([ 8, 12, 12]),\n"," tensor([ 9, 13, 10]),\n"," tensor([12,  9, 11]),\n"," tensor([10,  8, 14]),\n"," tensor([10, 11, 11]),\n"," tensor([ 7,  8, 17]),\n"," tensor([13, 13,  6]),\n"," tensor([12, 10, 10]),\n"," tensor([13,  7, 12]),\n"," tensor([10,  5, 17]),\n"," tensor([ 4, 20,  8]),\n"," tensor([13, 12,  7]),\n"," tensor([ 9, 10, 13]),\n"," tensor([14, 10,  8]),\n"," tensor([10,  8, 14]),\n"," tensor([12,  9, 11]),\n"," tensor([13, 10,  9]),\n"," tensor([ 9, 16,  7]),\n"," tensor([12,  8, 12]),\n"," tensor([ 9,  9, 14]),\n"," tensor([12,  8, 12]),\n"," tensor([ 8, 11, 13]),\n"," tensor([14,  8, 10]),\n"," tensor([11,  6, 15]),\n"," tensor([12,  8, 12]),\n"," tensor([14,  7, 11]),\n"," tensor([10,  9, 13]),\n"," tensor([16,  7,  9]),\n"," tensor([10,  7, 15]),\n"," tensor([14, 11,  7]),\n"," tensor([ 9, 12, 11]),\n"," tensor([ 7, 12, 13]),\n"," tensor([11, 11, 10]),\n"," tensor([11, 11, 10]),\n"," tensor([ 6, 17,  9]),\n"," tensor([14,  7, 11]),\n"," tensor([11, 10, 11]),\n"," tensor([ 8, 10, 14]),\n"," tensor([12,  7, 13]),\n"," tensor([12, 11,  9]),\n"," tensor([ 7, 10, 15]),\n"," tensor([15,  9,  8]),\n"," tensor([11, 13,  8]),\n"," tensor([14,  7, 11]),\n"," tensor([12,  9, 11]),\n"," tensor([11, 13,  8]),\n"," tensor([ 6, 15, 11]),\n"," tensor([ 9, 16,  7]),\n"," tensor([11,  8, 13]),\n"," tensor([ 8,  9, 15]),\n"," tensor([11, 10, 11]),\n"," tensor([ 9, 10, 13]),\n"," tensor([ 7, 11, 14]),\n"," tensor([13, 11,  8]),\n"," tensor([14,  9,  9]),\n"," tensor([ 7, 11, 14]),\n"," tensor([13, 11,  8]),\n"," tensor([11,  8, 13]),\n"," tensor([10, 13,  9]),\n"," tensor([ 9, 17,  6]),\n"," tensor([ 8, 11, 13]),\n"," tensor([ 7, 12, 13]),\n"," tensor([11,  9, 12]),\n"," tensor([10, 13,  9]),\n"," tensor([11, 10, 11]),\n"," tensor([13,  6, 13]),\n"," tensor([12,  7, 13]),\n"," tensor([14,  8, 10]),\n"," tensor([ 6, 14, 12]),\n"," tensor([11,  8, 13]),\n"," tensor([10, 11, 11]),\n"," tensor([ 9, 10, 13]),\n"," tensor([ 6, 12, 14]),\n"," tensor([13, 12,  7]),\n"," tensor([ 8, 13, 11]),\n"," tensor([ 7, 10, 15]),\n"," tensor([10, 11, 11]),\n"," tensor([10,  7, 15]),\n"," tensor([14, 10,  8]),\n"," tensor([12,  7, 13]),\n"," tensor([10, 14,  8]),\n"," tensor([12, 11,  9]),\n"," tensor([10, 11, 11]),\n"," tensor([14, 10,  8]),\n"," tensor([12, 14,  6]),\n"," tensor([13, 11,  8]),\n"," tensor([ 9, 12, 11]),\n"," tensor([11, 10, 11]),\n"," tensor([12,  9, 11]),\n"," tensor([ 9, 13, 10]),\n"," tensor([ 6, 14, 12]),\n"," tensor([14, 10,  8]),\n"," tensor([ 7,  9, 16]),\n"," tensor([ 5, 12, 15]),\n"," tensor([15, 10,  7]),\n"," tensor([12, 11,  9]),\n"," tensor([ 8, 18,  6]),\n"," tensor([ 6, 10, 16]),\n"," tensor([11,  7, 14]),\n"," tensor([10,  9, 13]),\n"," tensor([ 8, 18,  6]),\n"," tensor([13,  9, 10]),\n"," tensor([14,  7, 11]),\n"," tensor([11, 16,  5]),\n"," tensor([10, 13,  9]),\n"," tensor([11, 12,  9]),\n"," tensor([ 9, 16,  7]),\n"," tensor([ 8,  9, 15]),\n"," tensor([ 8, 17,  7]),\n"," tensor([ 7, 17,  8]),\n"," tensor([ 9, 16,  7]),\n"," tensor([ 5,  9, 18]),\n"," tensor([ 8,  9, 15]),\n"," tensor([ 7, 10, 15]),\n"," tensor([11, 10, 11]),\n"," tensor([11, 10, 11]),\n"," tensor([15,  8,  9]),\n"," tensor([14, 13,  5]),\n"," tensor([10, 12, 10]),\n"," tensor([10,  7, 15]),\n"," tensor([10, 11, 11]),\n"," tensor([ 8, 11, 13]),\n"," tensor([12, 12,  8]),\n"," tensor([ 9, 10, 13]),\n"," tensor([ 9, 11, 12]),\n"," tensor([11,  7, 14]),\n"," tensor([12, 10, 10]),\n"," tensor([14,  8, 10]),\n"," tensor([ 9, 12, 11]),\n"," tensor([12,  6, 14]),\n"," tensor([ 9, 13, 10]),\n"," tensor([13,  9, 10]),\n"," tensor([ 9, 11, 12]),\n"," tensor([ 8, 11, 13]),\n"," tensor([13,  5, 14]),\n"," tensor([ 9, 13, 10]),\n"," tensor([16,  6, 10]),\n"," tensor([11,  9, 12]),\n"," tensor([ 9, 10, 13]),\n"," tensor([ 6, 16, 10]),\n"," tensor([ 9,  8, 15]),\n"," tensor([ 9, 10, 13]),\n"," tensor([12, 13,  7]),\n"," tensor([ 9, 14,  9]),\n"," tensor([13,  9, 10]),\n"," tensor([ 6, 15, 11]),\n"," tensor([13,  8, 11]),\n"," tensor([12,  9, 11]),\n"," tensor([ 9, 13, 10]),\n"," tensor([10, 12, 10]),\n"," tensor([ 7, 14, 11]),\n"," tensor([13, 11,  8]),\n"," tensor([11,  7, 14]),\n"," tensor([ 6, 13, 13]),\n"," tensor([ 9, 12, 11]),\n"," tensor([10, 16,  6]),\n"," tensor([14, 10,  8]),\n"," tensor([13, 13,  6]),\n"," tensor([10, 10, 12]),\n"," tensor([15,  5, 12]),\n"," tensor([ 8,  9, 15]),\n"," tensor([10, 11, 11]),\n"," tensor([16,  8,  8]),\n"," tensor([12,  6, 14]),\n"," tensor([11, 10, 11]),\n"," tensor([ 5, 12, 15]),\n"," tensor([12, 11,  9]),\n"," tensor([11,  7, 14]),\n"," tensor([15,  7, 10]),\n"," tensor([ 6, 13, 13]),\n"," tensor([10, 14,  8]),\n"," tensor([11, 11, 10]),\n"," tensor([ 6, 15, 11]),\n"," tensor([15, 12,  5]),\n"," tensor([10, 10, 12]),\n"," tensor([10, 11, 11]),\n"," tensor([10, 11, 11]),\n"," tensor([ 8,  9, 15]),\n"," tensor([10, 11, 11]),\n"," tensor([15,  5, 12]),\n"," tensor([ 8, 14, 10]),\n"," tensor([16,  7,  9]),\n"," tensor([ 9, 13, 10]),\n"," tensor([ 5, 17, 10]),\n"," tensor([ 5, 11, 16]),\n"," tensor([14, 13,  5]),\n"," tensor([ 7, 12, 13]),\n"," tensor([10, 14,  8]),\n"," tensor([11, 11, 10]),\n"," tensor([ 9, 10, 13]),\n"," tensor([10, 12, 10]),\n"," tensor([16,  6, 10]),\n"," tensor([ 7,  7, 18]),\n"," tensor([ 9, 14,  9]),\n"," tensor([ 9, 12, 11]),\n"," tensor([14,  8, 10]),\n"," tensor([ 8,  8, 16]),\n"," tensor([12,  7, 13]),\n"," tensor([10,  9, 13]),\n"," tensor([ 9,  8, 15]),\n"," tensor([ 7, 10, 15]),\n"," tensor([13, 10,  9]),\n"," tensor([10, 13,  9]),\n"," tensor([ 9,  8, 15]),\n"," tensor([13,  6, 13]),\n"," tensor([12, 10, 10]),\n"," tensor([12, 11,  9]),\n"," tensor([11, 13,  8]),\n"," tensor([14,  9,  9]),\n"," tensor([17, 10,  5]),\n"," tensor([10, 10, 12]),\n"," tensor([14,  9,  9]),\n"," tensor([10, 10, 12]),\n"," tensor([13, 11,  8]),\n"," tensor([ 6, 14, 12]),\n"," tensor([ 9, 12, 11]),\n"," tensor([ 7, 14, 11]),\n"," tensor([14, 10,  8]),\n"," tensor([11,  9, 12]),\n"," tensor([17,  8,  7]),\n"," tensor([15,  9,  8]),\n"," tensor([12, 12,  8]),\n"," tensor([17,  7,  8]),\n"," tensor([14, 10,  8]),\n"," tensor([13,  7, 12]),\n"," tensor([14, 12,  6]),\n"," tensor([12, 12,  8]),\n"," tensor([12,  7, 13]),\n"," tensor([11, 10, 11]),\n"," tensor([13, 11,  8]),\n"," tensor([11, 15,  6]),\n"," tensor([ 8, 14, 10]),\n"," tensor([ 9,  8, 15]),\n"," tensor([10, 11, 11]),\n"," tensor([ 8, 11, 13]),\n"," tensor([ 8,  7, 17]),\n"," tensor([ 9,  7, 16]),\n"," tensor([14, 11,  7]),\n"," tensor([12, 13,  7]),\n"," tensor([13, 12,  7]),\n"," tensor([ 7, 13, 12]),\n"," tensor([14, 11,  7]),\n"," tensor([10,  7, 15]),\n"," tensor([11, 15,  6]),\n"," tensor([11, 12,  9]),\n"," tensor([ 7, 11, 14]),\n"," tensor([15, 10,  7]),\n"," tensor([ 8, 11, 13]),\n"," tensor([14, 10,  8]),\n"," tensor([ 9,  8, 15]),\n"," tensor([ 9, 10, 13]),\n"," tensor([13,  7, 12]),\n"," tensor([10,  8, 14]),\n"," tensor([12, 10, 10]),\n"," tensor([ 9, 11, 12]),\n"," tensor([14,  8, 10]),\n"," tensor([11, 12,  9]),\n"," tensor([10, 13,  9]),\n"," tensor([12, 10, 10]),\n"," tensor([10, 12, 10]),\n"," tensor([11,  6, 15]),\n"," tensor([12, 12,  8]),\n"," tensor([14,  9,  9]),\n"," tensor([14, 10,  8]),\n"," tensor([ 6, 13, 13]),\n"," tensor([12, 10, 10]),\n"," tensor([12,  8, 12]),\n"," tensor([ 9, 12, 11]),\n"," tensor([16,  7,  9]),\n"," tensor([ 8, 11, 13]),\n"," tensor([14, 11,  7]),\n"," tensor([10,  9, 13]),\n"," tensor([10,  9, 13]),\n"," tensor([16,  6, 10]),\n"," tensor([14, 12,  6]),\n"," tensor([ 9,  8, 15]),\n"," tensor([ 9, 11, 12]),\n"," tensor([ 7, 13, 12]),\n"," tensor([11,  8, 13]),\n"," tensor([ 8, 13, 11]),\n"," tensor([ 7, 15, 10]),\n"," tensor([11, 13,  8]),\n"," tensor([ 9,  6, 17]),\n"," tensor([10,  6, 16]),\n"," tensor([12,  9, 11]),\n"," tensor([11, 10, 11]),\n"," tensor([ 8, 11, 13]),\n"," tensor([11,  8, 13]),\n"," tensor([11,  8, 13]),\n"," tensor([16,  8,  8]),\n"," tensor([ 9, 13, 10]),\n"," tensor([ 7, 11, 14]),\n"," tensor([12, 13,  7]),\n"," tensor([ 5, 12, 15]),\n"," tensor([10, 13,  9]),\n"," tensor([11, 10, 11]),\n"," tensor([10, 10, 12]),\n"," tensor([13,  5, 14]),\n"," tensor([15,  8,  9]),\n"," tensor([11, 12,  9]),\n"," tensor([12,  9, 11]),\n"," tensor([ 9, 14,  9]),\n"," tensor([17, 11,  4]),\n"," tensor([11,  5, 16]),\n"," tensor([11, 10, 11]),\n"," tensor([ 4, 15, 13]),\n"," tensor([11,  9, 12]),\n"," tensor([11,  9, 12]),\n"," tensor([13,  6, 13]),\n"," tensor([11, 10, 11]),\n"," tensor([11, 12,  9]),\n"," tensor([ 7, 12, 13]),\n"," tensor([ 8,  8, 16]),\n"," tensor([12,  9, 11]),\n"," tensor([11, 11, 10]),\n"," tensor([12,  7, 13]),\n"," tensor([10, 13,  9]),\n"," tensor([11,  8, 13]),\n"," tensor([12,  9, 11]),\n"," tensor([10, 10, 12]),\n"," tensor([12,  8, 12]),\n"," tensor([ 8, 16,  8]),\n"," tensor([10,  7, 15]),\n"," tensor([11, 13,  8]),\n"," tensor([11,  9, 12]),\n"," tensor([13,  7, 12]),\n"," tensor([13, 12,  7]),\n"," tensor([ 7, 11, 14]),\n"," tensor([12,  7, 13]),\n"," tensor([12,  9, 11]),\n"," tensor([12, 12,  8]),\n"," tensor([ 9, 12, 11]),\n"," tensor([ 6, 14, 12]),\n"," tensor([ 9, 10, 13]),\n"," tensor([15,  6, 11]),\n"," tensor([13,  8, 11]),\n"," tensor([ 7, 18,  7]),\n"," tensor([15,  8,  9]),\n"," tensor([14, 10,  8]),\n"," tensor([11,  6, 15]),\n"," tensor([12, 10, 10]),\n"," tensor([ 9,  9, 14]),\n"," tensor([12, 11,  9]),\n"," tensor([13,  8, 11]),\n"," tensor([14,  7, 11]),\n"," tensor([ 8, 13, 11]),\n"," tensor([12,  7, 13]),\n"," tensor([ 7,  8, 17]),\n"," tensor([11,  8, 13]),\n"," tensor([13, 10,  9]),\n"," tensor([13, 10,  9]),\n"," tensor([14,  6, 12]),\n"," tensor([14,  7, 11]),\n"," tensor([13, 13,  6]),\n"," tensor([11,  8, 13]),\n"," tensor([11, 12,  9]),\n"," tensor([11, 14,  7]),\n"," tensor([ 7, 12, 13]),\n"," tensor([12, 10, 10]),\n"," tensor([13, 10,  9]),\n"," tensor([10, 12, 10]),\n"," tensor([13,  8, 11]),\n"," tensor([14, 11,  7]),\n"," tensor([ 8, 12, 12]),\n"," tensor([11, 10, 11]),\n"," tensor([10, 12, 10]),\n"," tensor([ 8, 16,  8]),\n"," tensor([13, 14,  5]),\n"," tensor([ 9, 10, 13]),\n"," tensor([13, 11,  8]),\n"," tensor([11, 15,  6]),\n"," tensor([12,  6, 14]),\n"," tensor([16,  9,  7]),\n"," tensor([14,  7, 11]),\n"," tensor([11, 12,  9]),\n"," tensor([10,  6, 16]),\n"," tensor([16,  9,  7]),\n"," tensor([13, 11,  8]),\n"," tensor([ 6, 13, 13]),\n"," tensor([ 9, 14,  9]),\n"," tensor([13, 10,  9]),\n"," tensor([ 9,  7, 16]),\n"," tensor([16,  8,  8]),\n"," tensor([11, 13,  8]),\n"," tensor([21,  5,  6]),\n"," tensor([10, 13,  9]),\n"," tensor([14, 10,  8]),\n"," tensor([10, 13,  9]),\n"," tensor([15,  6, 11]),\n"," tensor([ 7, 15, 10]),\n"," tensor([18,  6,  8]),\n"," tensor([12,  9, 11]),\n"," tensor([14,  9,  9]),\n"," tensor([12,  7, 13]),\n"," tensor([ 9, 10, 13]),\n"," tensor([13, 12,  7]),\n"," tensor([11,  8, 13]),\n"," tensor([ 8, 15,  9]),\n"," tensor([11,  7, 14]),\n"," tensor([11, 12,  9]),\n"," tensor([ 7, 12, 13]),\n"," tensor([ 8, 11, 13]),\n"," tensor([10, 11, 11]),\n"," tensor([17, 11,  4]),\n"," tensor([ 8, 13, 11]),\n"," tensor([13,  7, 12]),\n"," tensor([ 9, 12, 11]),\n"," tensor([15,  9,  8]),\n"," tensor([ 3, 13, 16]),\n"," tensor([ 9, 14,  9]),\n"," tensor([15,  6, 11]),\n"," tensor([15,  6, 11]),\n"," tensor([11,  9, 12]),\n"," tensor([10,  7, 15]),\n"," tensor([10, 14,  8]),\n"," tensor([13,  6, 13]),\n"," tensor([15,  9,  8]),\n"," tensor([ 9, 11, 12]),\n"," tensor([11, 10, 11]),\n"," tensor([10, 11, 11]),\n"," tensor([ 9, 13, 10]),\n"," tensor([ 8, 12, 12]),\n"," tensor([10, 10, 12]),\n"," tensor([ 7, 15, 10]),\n"," tensor([ 4, 15, 13]),\n"," tensor([10, 11, 11]),\n"," tensor([14,  9,  9]),\n"," tensor([14, 12,  6]),\n"," tensor([10,  8, 14]),\n"," tensor([13,  7, 12]),\n"," tensor([ 6, 14, 12]),\n"," tensor([11, 14,  7]),\n"," tensor([13,  7, 12]),\n"," tensor([12,  9, 11]),\n"," tensor([10, 15,  7]),\n"," tensor([ 7, 11, 14]),\n"," tensor([12, 10, 10]),\n"," tensor([11,  8, 13]),\n"," tensor([ 5, 15, 12]),\n"," tensor([ 6, 12, 14]),\n"," tensor([14,  6, 12]),\n"," tensor([ 8, 10, 14]),\n"," tensor([12, 14,  6]),\n"," tensor([ 9, 14,  9]),\n"," tensor([12,  8, 12]),\n"," tensor([13, 14,  5]),\n"," tensor([ 6, 12, 14]),\n"," tensor([11, 10, 11]),\n"," tensor([12,  8, 12]),\n"," tensor([ 7, 14, 11]),\n"," tensor([11, 11, 10]),\n"," tensor([10, 13,  9]),\n"," tensor([12, 10, 10]),\n"," tensor([ 7, 10, 15]),\n"," tensor([12, 11,  9]),\n"," tensor([ 9,  5, 18]),\n"," tensor([15,  7, 10]),\n"," tensor([ 7, 18,  7]),\n"," tensor([13, 14,  5]),\n"," tensor([13,  7, 12]),\n"," tensor([11,  8, 13]),\n"," tensor([15, 13,  4]),\n"," tensor([ 9, 15,  8]),\n"," tensor([11, 15,  6]),\n"," tensor([ 9, 12, 11]),\n"," tensor([13,  9, 10]),\n"," tensor([11, 11, 10]),\n"," tensor([13, 12,  7]),\n"," tensor([11, 12,  9]),\n"," tensor([14, 10,  8]),\n"," tensor([11,  9, 12]),\n"," tensor([15,  6, 11]),\n"," tensor([11,  9, 12]),\n"," tensor([ 9, 13, 10]),\n"," tensor([13, 11,  8]),\n"," tensor([11, 12,  9]),\n"," tensor([11,  9, 12]),\n"," tensor([ 8, 14, 10]),\n"," tensor([10, 15,  7]),\n"," tensor([ 9, 10, 13]),\n"," tensor([13,  8, 11]),\n"," tensor([ 9, 15,  8]),\n"," tensor([17,  8,  7]),\n"," tensor([12, 15,  5]),\n"," tensor([14,  8, 10]),\n"," tensor([16,  3, 13]),\n"," tensor([10, 11, 11]),\n"," tensor([11, 11, 10]),\n"," tensor([14,  7, 11]),\n"," tensor([10, 10, 12]),\n"," tensor([10, 13,  9]),\n"," tensor([12, 12,  8]),\n"," tensor([ 9,  9, 14]),\n"," tensor([10,  8, 14]),\n"," tensor([15,  5, 12]),\n"," tensor([ 6, 16, 10]),\n"," tensor([ 9, 15,  8]),\n"," tensor([11, 11, 10]),\n"," tensor([ 9, 13, 10]),\n"," tensor([10, 11, 11]),\n"," tensor([11,  9, 12]),\n"," tensor([14,  6, 12]),\n"," tensor([ 9, 11, 12]),\n"," tensor([ 5, 16, 11]),\n"," tensor([13, 11,  8]),\n"," tensor([11, 11, 10]),\n"," tensor([12, 12,  8]),\n"," tensor([12,  8, 12]),\n"," tensor([ 9,  7, 16]),\n"," tensor([13,  9, 10]),\n"," tensor([10, 12, 10]),\n"," tensor([11,  9, 12]),\n"," tensor([14, 12,  6]),\n"," tensor([ 5, 16, 11]),\n"," tensor([10, 10, 12]),\n"," tensor([ 9, 10, 13]),\n"," tensor([13, 10,  9]),\n"," tensor([18,  7,  7]),\n"," tensor([10, 14,  8]),\n"," tensor([ 6, 10, 16]),\n"," tensor([12,  9, 11]),\n"," tensor([11, 11, 10]),\n"," tensor([10, 14,  8]),\n"," tensor([10, 13,  9]),\n"," tensor([ 8, 13, 11]),\n"," tensor([15,  8,  9]),\n"," tensor([ 9, 10, 13]),\n"," tensor([10, 14,  8]),\n"," tensor([10,  9, 13]),\n"," tensor([10, 14,  8]),\n"," tensor([ 9, 16,  7]),\n"," tensor([11, 10, 11]),\n"," tensor([12, 11,  9]),\n"," tensor([ 8, 13, 11]),\n"," tensor([11, 12,  9]),\n"," tensor([20,  8,  4]),\n"," tensor([10, 13,  9]),\n"," tensor([13,  7, 12]),\n"," tensor([10, 16,  6]),\n"," tensor([11, 11, 10]),\n"," tensor([11, 13,  8]),\n"," tensor([12,  9, 11]),\n"," tensor([12,  9, 11]),\n"," tensor([11, 10, 11]),\n"," tensor([14, 11,  7]),\n"," tensor([10,  8, 14]),\n"," tensor([15,  8,  9]),\n"," tensor([12, 12,  8]),\n"," tensor([ 8, 13, 11]),\n"," tensor([14,  7, 11]),\n"," tensor([11,  9, 12]),\n"," tensor([ 9, 15,  8]),\n"," tensor([13,  9, 10]),\n"," tensor([14,  7, 11]),\n"," tensor([11,  5, 16]),\n"," tensor([ 8, 11, 13]),\n"," tensor([10, 15,  7]),\n"," tensor([ 6,  9, 17]),\n"," tensor([13, 10,  9]),\n"," tensor([12,  9, 11]),\n"," tensor([12, 11,  9]),\n"," tensor([ 8, 13, 11]),\n"," tensor([11, 13,  8]),\n"," tensor([ 9, 11, 12]),\n"," tensor([10,  8, 14]),\n"," tensor([ 6, 10, 16]),\n"," tensor([ 9,  6, 17]),\n"," tensor([11,  9, 12]),\n"," tensor([ 9,  9, 14]),\n"," tensor([14, 11,  7]),\n"," tensor([13, 10,  9]),\n"," tensor([ 5, 11, 16]),\n"," tensor([ 7, 13, 12]),\n"," tensor([10,  7, 15]),\n"," tensor([ 9, 12, 11]),\n"," tensor([ 8, 11, 13]),\n"," tensor([14, 10,  8]),\n"," tensor([ 9,  7, 16]),\n"," tensor([10, 12, 10]),\n"," tensor([13, 10,  9]),\n"," tensor([13,  8, 11]),\n"," tensor([13, 11,  8]),\n"," tensor([ 8,  9, 15]),\n"," tensor([10, 12, 10]),\n"," tensor([15, 10,  7]),\n"," tensor([10, 11, 11]),\n"," tensor([16, 11,  5]),\n"," tensor([12,  7, 13]),\n"," tensor([10, 10, 12]),\n"," tensor([10, 12, 10]),\n"," tensor([12, 12,  8]),\n"," tensor([13, 10,  9]),\n"," tensor([16, 10,  6]),\n"," tensor([ 8, 15,  9]),\n"," tensor([12, 11,  9]),\n"," tensor([10, 13,  9]),\n"," tensor([ 9, 11, 12]),\n"," tensor([12, 10, 10]),\n"," tensor([10, 10, 12]),\n"," tensor([10, 18,  4]),\n"," tensor([ 8, 16,  8]),\n"," tensor([ 8, 15,  9]),\n"," tensor([ 8, 14, 10]),\n"," tensor([ 9, 12, 11]),\n"," tensor([ 8, 10, 14]),\n"," tensor([17,  5, 10]),\n"," tensor([10, 11, 11]),\n"," tensor([ 7, 11, 14]),\n"," tensor([10, 13,  9]),\n"," tensor([10, 16,  6]),\n"," tensor([14, 10,  8]),\n"," tensor([12, 14,  6]),\n"," tensor([10, 11, 11]),\n"," tensor([12, 14,  6]),\n"," tensor([ 8, 15,  9]),\n"," tensor([ 8, 12, 12]),\n"," tensor([13,  7, 12]),\n"," tensor([14, 11,  7]),\n"," tensor([15, 10,  7]),\n"," tensor([14, 10,  8]),\n"," tensor([10,  9, 13]),\n"," tensor([ 9, 11, 12]),\n"," tensor([ 5, 13, 14]),\n"," tensor([16, 10,  6]),\n"," tensor([ 9, 15,  8]),\n"," tensor([10, 11, 11]),\n"," tensor([10, 10, 12]),\n"," tensor([10, 12, 10]),\n"," tensor([10,  9, 13]),\n"," ...]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["print(len(dataloader))\n","counts = []\n","for batch in dataloader:\n","    _, c = torch.unique(batch[\"yng_label\"], return_counts=True)\n","    counts.append(c)\n","\n","print(len(counts))\n","counts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a1797d7882742f2acabeec3a859b5e0","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'passage_mask', 'rationale_labels', 'decoder_input_ids', 'labels', 'decoder_attention_mask', 'yng_label', 'yes_no', 'yng_logits', 'rationale_logits', 'answer', 'rationale_f1', 'pred_yng_label'],\n","    num_rows: 21441\n","})"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["outputs = results[\"val\"][0]\n","outputs = outputs.map(\n","        lambda example: {\n","            \"pred_yng_label\": logits_to_class(torch.Tensor(example[\"yng_logits\"]), task=\"multiclass\")\n","        },\n","        batched=True,\n","        load_from_cache_file=False,\n","    )\n","outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([0.4886, 0.5323, 0.9775])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["f1 = MulticlassF1Score(\n","    num_classes=3,\n","    average=\"none\",\n","    ignore_index=-100,\n",")\n","\n","f1(torch.Tensor(outputs[\"pred_yng_label\"]), torch.Tensor(outputs[\"yng_label\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","cm = confusion_matrix(outputs[\"yng_label\"], outputs[\"pred_yng_label\"])\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"yes\", \"no\", \"generative\"])\n","disp.plot()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'dataset' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m steps_per_epoch \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mnum_rows \u001b[39m/\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m      2\u001b[0m total_steps \u001b[39m=\u001b[39m steps_per_epoch \u001b[39m*\u001b[39m config\u001b[39m.\u001b[39mnum_epochs\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msteps_per_epoch:\u001b[39m\u001b[39m\"\u001b[39m, steps_per_epoch)\n","\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"]}],"source":["steps_per_epoch = dataset.num_rows / config.batch_size\n","total_steps = steps_per_epoch * config.num_epochs\n","\n","print(\"steps_per_epoch:\", steps_per_epoch)\n","print(\"total_steps:\", total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(array([0, 1, 2]), array([ 9481,  7287, 68806], dtype=int64))"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["np.unique(get_data(\"train\", config)[\"yng_label\"], return_counts=True)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
