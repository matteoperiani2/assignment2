{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:53:31.627490Z","iopub.status.busy":"2023-08-02T15:53:31.627180Z","iopub.status.idle":"2023-08-02T15:53:58.780541Z","shell.execute_reply":"2023-08-02T15:53:58.779222Z","shell.execute_reply.started":"2023-08-02T15:53:31.627461Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install accelerate==0.19.0 datasets==2.12.0 transformers==4.29.2 evaluate==0.4.0 scikit-learn text2num tokenizers torch>=2.0.0 torchmetrics tqdm wandb "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:53:58.783332Z","iopub.status.busy":"2023-08-02T15:53:58.782923Z","iopub.status.idle":"2023-08-02T15:54:12.037266Z","shell.execute_reply":"2023-08-02T15:54:12.036268Z","shell.execute_reply.started":"2023-08-02T15:53:58.783293Z"},"trusted":true},"outputs":[],"source":["import os, llib.request, inspect, functools, itertools, gc, random, re, string, warnings\n","from tqdm.auto import tqdm\n","from typing import List, Optional, Union, Dict, Literal, Tuple, Protocol\n","from enum import Enum, auto\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from dataclasses import dataclass\n","from text_to_num import text2num\n","from collections import defaultdict\n","from accelerate import Accelerator\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.utils.data import Dataset, DataLoader\n","from torchmetrics.classification import MulticlassF1Scores\n","\n","import transformers, datasets\n","from transformers import AutoModel, AutoTokenizer, EncoderDecoderModel\n","\n","import wandb\n","\n","# keep datasets in memory if < 8 GB\n","datasets.config.IN_MEMORY_MAX_SIZE = 8 * 1024**3\n","CPU = True"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:54:12.039135Z","iopub.status.busy":"2023-08-02T15:54:12.038781Z","iopub.status.idle":"2023-08-02T15:57:46.700132Z","shell.execute_reply":"2023-08-02T15:57:46.699077Z","shell.execute_reply.started":"2023-08-02T15:54:12.039083Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimonemele999\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# wandb.login()"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:46.714260Z","iopub.status.busy":"2023-08-02T15:57:46.713486Z","iopub.status.idle":"2023-08-02T15:57:46.736904Z","shell.execute_reply":"2023-08-02T15:57:46.735542Z","shell.execute_reply.started":"2023-08-02T15:57:46.714224Z"},"trusted":true},"outputs":[],"source":["class AnswerType(Enum):\n","    UNKNOWN = auto()\n","    SPAN = auto()\n","    YES_NO = auto()\n","    FLUENCY = auto()\n","    COUNTING = auto()\n","    MULTIPLE_CHOICE = auto()\n","\n","    def __str__(self):\n","        return self.name.lower()\n","\n","    @classmethod\n","    def list(cls, return_unknown=True):\n","        return [str(c) for c in cls if return_unknown or c != AnswerType.UNKNOWN]\n","        return list(map(lambda c: str(c), cls))\n","\n","@dataclass\n","class Config:\n","    \n","    class Dataset:\n","        train_url: str = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n","        test_url: str = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n","\n","        data_dir: str = \"data\"\n","\n","        raw_dir: str = os.path.join(data_dir, \"raw\")\n","        train_data_raw: str = os.path.join(raw_dir, \"train.json\")\n","        test_data_raw: str = os.path.join(raw_dir, \"test.json\")\n","\n","        annotated_dir: str = os.path.join(data_dir, \"annotated\")\n","        train_data_annotated: str = os.path.join(annotated_dir, \"train.json\")\n","        test_data_annotated: str = os.path.join(annotated_dir, \"test.json\")\n","\n","        readable_dir: str = os.path.join(data_dir, \"readable\")\n","        train_data_readable: str = os.path.join(readable_dir, \"train.txt\")\n","        test_data_readable: str = os.path.join(readable_dir, \"test.txt\")\n","\n","        filtered_dir: str = os.path.join(data_dir, \"filtered\")\n","        splitted_dir: str = os.path.join(data_dir, \"splitted\")\n","        processed_dir: str = os.path.join(data_dir, \"processed\")\n","        train_dir: str = os.path.join(data_dir, \"train\")\n","\n","        def train_no_history(self, model_name: str, split=\"\") -> str:\n","            return os.path.join(self.train_dir, \"train_no_history\", model_name, split)\n","\n","        def train_with_history(self, model_name: str, split=\"\") -> str:\n","            return os.path.join(self.train_dir, \"train_with_history\", model_name, split)\n","\n","    class Checkpoints:\n","        def __init__(\n","            self, distil_roberta=\"distilroberta-base\", bert_tiny=\"prajjwal1/bert-tiny\"\n","        ) -> None:\n","            self.distil_roberta = distil_roberta\n","            self.bert_tiny = bert_tiny\n","\n","    class Models:\n","        def __init__(\n","            self,\n","            model_dir_name=\"models\",\n","            checkpoint_dir_name=\"checkpoints\",\n","            final_checkpoint_name=\"final.pt\",\n","        ) -> None:\n","            self.__model_dir = model_dir_name\n","            self.__checkpoints_dir_name = checkpoint_dir_name\n","            self.__final_checkpoint_name = final_checkpoint_name\n","\n","        def model_dir(self, model_name, history: Optional[bool] = None):\n","            if history is None:\n","                history_str = \"\"\n","            elif history:\n","                history_str = \"history\"\n","            else:\n","                history_str = \"no_history\"\n","            return os.path.join(self.__model_dir, model_name, history_str)\n","\n","        def checkpoints_dir(self, model_name, history: Optional[bool]):\n","            return os.path.join(\n","                self.model_dir(model_name, history=history), self.__checkpoints_dir_name\n","            )\n","\n","        def checkpoint(self, model_name, history: Optional[bool]):\n","            return os.path.join(\n","                self.model_dir(model_name, history=history),\n","                self.__final_checkpoint_name,\n","            )\n","\n","    @dataclass\n","    class Preprocessing:\n","        encoder_max_length: int\n","        decoder_max_length: int\n","        stride: int = 196\n","        use_window: bool = False\n","        max_history_length: int = 4\n","\n","    @dataclass\n","    class WandbConfig:\n","        \"\"\"Specify the parameters of `wandb.init`\"\"\"\n","\n","        project: str = \"nlp_assignment2\"\n","        entity: str = \"nlp_assignment2\"\n","\n","    dataset: Dataset = Dataset()\n","    checkpoints: Checkpoints = Checkpoints()\n","    models: Models = Models()\n","\n","    # remove all span answers longer than span_max_length words\n","    span_max_length: int = 37\n","    # ignore loss of rationales longer than rationale_max_length\n","    rationale_max_length: int = 150\n","\n","    encoder_max_length = 512\n","    # decoder_max_length = 350\n","    decoder_max_length = 64\n","\n","    preprocessing = Preprocessing(encoder_max_length, decoder_max_length)\n","    generation = dict(penalty_alpha=0.6, top_k=6)\n","\n","    wandbConfig = WandbConfig()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:46.738968Z","iopub.status.busy":"2023-08-02T15:57:46.738542Z","iopub.status.idle":"2023-08-02T15:57:46.751005Z","shell.execute_reply":"2023-08-02T15:57:46.750025Z","shell.execute_reply.started":"2023-08-02T15:57:46.738935Z"},"trusted":true},"outputs":[],"source":["CONFIG: Config = Config()"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def answer_to_idx(answer: str) -> int:\n","    if answer.lower() == \"yes\":\n","        return 0\n","    if answer.lower() == \"no\":\n","        return 1\n","    return 2\n","\n","\n","def idx_to_answer(idx: int) -> str:\n","    if idx == 0:\n","        return \"yes\"\n","    if idx == 1:\n","        return \"no\"\n","    return None\n","\n","\n","class CoQADatasetPreprocessing:\n","    def __init__(\n","        self,\n","        tokenizer: transformers.PreTrainedTokenizer = None,\n","        label_pad_token_id=-100,\n","        encoder_max_length=512,\n","        decoder_max_length=350,\n","        stride=196,\n","        use_window=False,\n","        max_history_length=4,\n","    ) -> None:\n","        self.tokenizer = tokenizer\n","        self.label_pad_token_id = label_pad_token_id\n","        self.encoder_max_length = encoder_max_length\n","        self.decoder_max_length = decoder_max_length\n","        self.stride = stride\n","        self.use_window = use_window\n","        self.max_history_length = max_history_length\n","\n","    def explode_questions(self, example):\n","        questions = example[\"questions\"]\n","        answers = example[\"answers\"]\n","        histories = []\n","\n","        for idx in range(len(questions)):\n","            history = self.__create_history(idx, questions, answers)\n","            histories.append(history)\n","\n","        output = {\n","            \"id\": [example[\"id\"]] * example[\"qa_length\"],\n","            \"turn\": [question_item[\"turn_id\"] for question_item in questions],\n","            \"question\": [question_item[\"input_text\"] for question_item in questions],\n","            \"answer\": [answer_item[\"input_text\"] for answer_item in answers],\n","            \"rationale\": [answer_item[\"span_text\"] for answer_item in answers],\n","            \"span_start\": [answer_item[\"span_start\"] for answer_item in answers],\n","            \"span_end\": [answer_item[\"span_end\"] for answer_item in answers],\n","            \"answer_type\": [answer_item[\"answer_type\"] for answer_item in answers],\n","            \"history\": histories,\n","            \"history_length\": [len(history) for history in histories],\n","        }\n","        for key, value in example.items():\n","            if key not in output:\n","                output[key] = [value] * example[\"qa_length\"]\n","        return output\n","\n","    def __create_history(self, current_index, questions, answers):\n","        history = [\n","            {\n","                \"question\": questions[i][\"input_text\"],\n","                \"answer\": answers[i][\"input_text\"],\n","                \"turn\": questions[i][\"turn_id\"],\n","            }\n","            for i in range(current_index)\n","        ]\n","        return history\n","\n","    def preprocess_texts(self, example):\n","        handle_rationale = \"rationale\" in example\n","        if handle_rationale:\n","            example = self.__fix_rationale(example)\n","\n","        example = self.__preprocess_passage(example, handle_rationale=handle_rationale)\n","        example = self.__preprocess_questions(example)\n","        example = self.__preprocess_answers(example)\n","\n","        return example\n","\n","    def __fix_rationale(self, example):\n","        rationale, span_start, span_end = fix_rationale(\n","            example[\"passage\"],\n","            example[\"rationale\"],\n","            example[\"span_start\"],\n","            example[\"span_end\"],\n","        )\n","\n","        example[\"rationale\"] = rationale\n","        example[\"span_start\"] = span_start\n","        example[\"span_end\"] = span_end\n","\n","        return example\n","\n","    def __preprocess_passage(self, example, handle_rationale=True):\n","        return self.__fix_passage_white_space(\n","            example, handle_rationale=handle_rationale\n","        )\n","\n","    def __preprocess_questions(self, example):\n","        example[\"question\"] = white_space_fix(example[\"question\"])\n","        for item in example.get(\"history\", []):\n","            item[\"question\"] = white_space_fix(item[\"question\"])\n","        return example\n","\n","    def __preprocess_answers(self, example):\n","        if \"answer\" in example:\n","            example[\"answer\"] = self.__preprocess_answer(example[\"answer\"])\n","        for item in example.get(\"history\", []):\n","            item[\"answer\"] = self.__preprocess_answer(item[\"answer\"])\n","        return example\n","\n","    def __preprocess_answer(self, answer):\n","        answer = white_space_fix(answer)\n","        answer = strip_non_alphanumeric_chars(answer)\n","        return answer\n","\n","    def __fix_passage_white_space(self, example, handle_rationale=True):\n","        passage = example[\"passage\"]\n","        if handle_rationale:\n","            span_start = example[\"span_start\"]\n","            span_end = example[\"span_end\"]\n","\n","            if span_end - span_start > 0:\n","                # assert rationale has already been fixed\n","                assert (\n","                    passage[span_start].isalnum() and passage[span_end - 1].isalnum()\n","                ), \"Rationale must start and end with alphanumeric characters. You must fix it before.\"\n","\n","            passage_start = white_space_fix(passage[:span_start])\n","            rationale = white_space_fix(passage[span_start:span_end])\n","            passage_end = white_space_fix(passage[span_end:])\n","\n","            passage = \" \".join((passage_start, rationale, passage_end))\n","            span_start = len(passage_start) + 1\n","            span_end = span_start + len(rationale)\n","\n","            assert rationale == passage[span_start:span_end]\n","\n","            example[\"passage\"] = passage\n","            example[\"rationale\"] = rationale\n","            example[\"span_start\"] = span_start\n","            example[\"span_end\"] = span_end\n","        else:\n","            example[\"passage\"] = white_space_fix(passage)\n","\n","        return example\n","\n","    def process_data_to_model_inputs(\n","        self,\n","        examples,\n","        add_history=False,\n","        padding=False,\n","    ) -> transformers.BatchEncoding:\n","        assert (\n","            self.tokenizer is not None\n","        ), \"A tokenizer is required to prepare the inputs for the model\"\n","        process_rationale = \"rationale\" in examples\n","        process_answer = \"answer\" in examples\n","\n","        sentences = [examples[\"question\"], examples[\"passage\"]]\n","\n","        if add_history:\n","            sentences[0] = self.__concat_history_and_question(\n","                examples[\"history\"], examples[\"question\"]\n","            )\n","\n","        inputs = self.tokenizer(\n","            *sentences,\n","            padding=padding,\n","            truncation=\"only_second\",\n","            max_length=self.encoder_max_length,\n","            stride=self.stride,\n","            return_overflowing_tokens=self.use_window,\n","            return_offsets_mapping=True,\n","        )\n","        if process_answer:\n","            outputs = self.tokenizer(\n","                examples[\"answer\"],\n","                padding=padding,\n","                truncation=True,\n","                max_length=self.decoder_max_length,\n","            )\n","\n","        offset_mapping = inputs[\"offset_mapping\"]\n","        if self.use_window:\n","            sample_map = lambda i: inputs[\"overflow_to_sample_mapping\"][i]\n","        else:\n","            sample_map = lambda i: i\n","\n","        yes_no_types = []\n","        yng_labels = []\n","\n","        passage_masks = []\n","        rationale_starts = []\n","        rationale_ends = []\n","        rationale_labels = []\n","        decoder_input_ids = []\n","        labels = []\n","        decoder_attention_masks = []\n","\n","        ids = []\n","        turns = []\n","\n","        # # store the presence of the rationale in the passage for at least one row\n","        # rationale_in_passage = [False] * len(examples[\"question\"])\n","        for i, offset in enumerate(offset_mapping):\n","            sample_idx = sample_map(i)\n","            sequence_ids = inputs.sequence_ids(i)\n","\n","            passage_start, passage_end = self.__find_passage(sequence_ids)\n","            passage_masks.append(\n","                self.__create_mask(sequence_ids, passage_start, passage_end + 1)\n","            )\n","\n","            if process_rationale:\n","                start_char = examples[\"span_start\"][sample_idx]\n","                end_char = examples[\"span_end\"][sample_idx]\n","                rationale_start, rationale_end = self.__char2token_rationale_span(\n","                    offset, (passage_start, passage_end), (start_char, end_char)\n","                )\n","                rationale_starts.append(rationale_start)\n","                rationale_ends.append(rationale_end)\n","                rationale_labels_ = self.__create_mask(\n","                    sequence_ids, rationale_start, rationale_end, dtype=np.float32\n","                )\n","                rationale_labels_[passage_masks[-1] == 0] = self.label_pad_token_id\n","                rationale_labels.append(rationale_labels_)\n","\n","                # rationale_in_passage[sample_idx] |= rationale_start != -1\n","\n","            if process_answer:\n","                # Remove <eos> from decoder_input_ids\n","                decoder_input_ids_ = outputs.input_ids[sample_idx][:-1]\n","                # Remove <bos> from labels\n","                labels_ = outputs.input_ids[sample_idx].copy()[1:]\n","                labels_ = [\n","                    self.label_pad_token_id\n","                    if token == self.tokenizer.pad_token_id\n","                    else token\n","                    for token in labels_\n","                ]\n","                decoder_attention_mask = outputs.attention_mask[sample_idx][:-1]\n","\n","                decoder_input_ids.append(decoder_input_ids_)\n","                labels.append(labels_)\n","                decoder_attention_masks.append(decoder_attention_mask)\n","\n","                yng_label = answer_to_idx(examples[\"answer\"][sample_idx])\n","                is_yes_no = yng_label < 2\n","                assert is_yes_no == (examples[\"answer_type\"][sample_idx] == \"yes_no\")\n","                yng_labels.append(yng_label)\n","                yes_no_types.append(int(is_yes_no))\n","\n","            ids.append(examples[\"id\"][sample_idx])\n","            if \"turn\" in examples:\n","                turns.append(examples[\"turn\"][sample_idx])\n","\n","        # if process_rationale:\n","        #     for sample_idx, is_rationale_in_passage in enumerate(rationale_in_passage):\n","        #         if not is_rationale_in_passage:\n","        #             warnings.warn(f\"The rationale is never contained in the passage. Id: {examples['id'][sample_idx]}, turn:{examples['turn'][sample_idx]}\")\n","\n","        inputs[\"passage_mask\"] = passage_masks\n","        if process_rationale:\n","            inputs[\"rationale_start\"] = rationale_starts\n","            inputs[\"rationale_end\"] = rationale_ends\n","            inputs[\"rationale_labels\"] = rationale_labels\n","        if process_answer:\n","            inputs[\"decoder_input_ids\"] = decoder_input_ids\n","            inputs[\"labels\"] = labels\n","            inputs[\"decoder_attention_mask\"] = decoder_attention_masks\n","            inputs[\"yng_label\"] = yng_labels\n","            inputs[\"yes_no\"] = yes_no_types\n","        inputs[\"id\"] = ids\n","        if len(turns) > 0:\n","            inputs[\"turn\"] = turns\n","\n","        return inputs\n","\n","    def __concat_history_and_question(self, histories, questions):\n","        outputs = []\n","        for history, question in zip(histories, questions):\n","            history_str = self.__create_history_str(history)\n","            history_question = self.tokenizer.sep_token.join((history_str, question))\n","            outputs.append(history_question)\n","        return outputs\n","\n","    def __create_history_str(self, history):\n","        history_items = reversed(\n","            tuple(itertools.islice((reversed(history)), self.max_history_length))\n","        )\n","        qa_pairs = []\n","        for item in history_items:\n","            qa = (item[\"question\"], item[\"answer\"])\n","            qa = self.tokenizer.sep_token.join(qa)\n","            qa_pairs.append(qa)\n","        return self.tokenizer.sep_token.join(qa_pairs)\n","\n","    def __find_passage(self, sequence_ids: List[int]) -> Tuple[int, int]:\n","        \"\"\"\n","        Find the start and the end of the passage w.r.t. the tokens.\n","        \"\"\"\n","\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        passage_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        passage_end = idx - 1\n","\n","        return passage_start, passage_end\n","\n","    def __char2token_rationale_span(\n","        self,\n","        token_to_span: List[Tuple[int, int]],\n","        passage_token_span: Tuple[int, int],\n","        rationale_char_span: Tuple[int, int],\n","    ) -> Tuple[int, int]:\n","        \"\"\"\n","        Map the rationale span from char indexes to token indexes\n","        \"\"\"\n","        passage_start, passage_end = passage_token_span\n","        start_char, end_char = rationale_char_span\n","\n","        # If the rationale is not fully inside the passage, returns (-1, -1)\n","        if (\n","            token_to_span[passage_start][0] > start_char\n","            or token_to_span[passage_end][1] < end_char\n","        ):\n","            return (-1, -1)\n","\n","        # Otherwise it's the start and end token positions\n","        idx = passage_start\n","        while idx <= passage_end and token_to_span[idx][0] <= start_char:\n","            idx += 1\n","        start_position = idx - 1\n","\n","        idx = passage_end\n","        while idx >= passage_start and token_to_span[idx][1] >= end_char:\n","            idx -= 1\n","        end_position = idx + 1\n","\n","        return start_position, end_position + 1\n","\n","    def __create_mask(self, arr, start, end, dtype=np.int8):\n","        mask = np.zeros_like(arr, dtype=dtype)\n","        mask[start:end] = 1\n","        return mask\n","\n","\n","def remove_articles_(text):\n","    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n","    return re.sub(regex, \" \", text)\n","\n","\n","def white_space_fix(text: str):\n","    return \" \".join(text.split())\n","\n","\n","def remove_punc(text):\n","    exclude = set(string.punctuation)\n","    return \"\".join(ch for ch in text if ch not in exclude)\n","\n","\n","def lower(text):\n","    return text.lower()\n","\n","\n","def normalize_text(text, remove_articles=False):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    text = remove_punc(lower(text))\n","    if remove_articles:\n","        text = remove_articles_(text)\n","    return white_space_fix(text)\n","\n","\n","def normalize_answer(text):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    return normalize_text(text, remove_articles=True)\n","\n","\n","def strip_non_alphanumeric_chars(text: str):\n","    \"\"\"\n","    Removes trailing and leading non alpha-numeric characters from a given string.\n","    \"\"\"\n","    start_index = 0\n","    while start_index < len(text) and not text[start_index].isalnum():\n","        start_index += 1\n","\n","    end_index = len(text) - 1\n","    while end_index > start_index and not text[end_index].isalnum():\n","        end_index -= 1\n","\n","    return text[start_index : end_index + 1]\n","\n","\n","def find_span(passage: str, text: str, span_start: int = None, span_end: int = None):\n","    if len(text) == 0:\n","        return (span_start, span_start)\n","    assert (\n","        text[0].isalnum() and text[-1].isalnum()\n","    ), \"Text must begin and end with an alphanumeric character.\"\n","\n","    start_idx = passage.find(text, span_start, span_end)\n","    end_idx = start_idx + len(text) - 1\n","\n","    if start_idx == -1:\n","        raise ValueError(\"The text is not present in the passage.\")\n","\n","    # Find the beginning of the word in the passage\n","    while start_idx > 0 and passage[start_idx - 1].isalnum():\n","        start_idx -= 1\n","\n","    # Find the end of the word in the passage\n","    while end_idx < len(passage) - 1 and passage[end_idx + 1].isalnum():\n","        end_idx += 1\n","\n","    return start_idx, end_idx + 1\n","\n","\n","def fix_rationale(passage: str, rationale: str, span_start: int, span_end: int):\n","    rationale = strip_non_alphanumeric_chars(rationale)\n","    span_start, span_end = find_span(\n","        passage, rationale, span_start=span_start, span_end=span_end\n","    )\n","    return passage[span_start:span_end], span_start, span_end\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:46.753196Z","iopub.status.busy":"2023-08-02T15:57:46.752763Z","iopub.status.idle":"2023-08-02T15:57:47.087220Z","shell.execute_reply":"2023-08-02T15:57:47.086277Z","shell.execute_reply.started":"2023-08-02T15:57:46.753161Z"},"trusted":true},"outputs":[],"source":["class AvgValue:\n","    def __init__(self, initial_value=0.0) -> None:\n","        self.__total_value = initial_value\n","        self.__last_value = initial_value\n","        self.__n = 0\n","\n","    def update(self, value, n=1):\n","        self.__last_value = value\n","        self.__total_value += n * value\n","        self.__n += n\n","\n","    def avg(self):\n","        return self.__total_value / self.__n\n","\n","    @property\n","    def last_value(self):\n","        return self.__last_value\n","\n","    @property\n","    def n(self):\n","        return self.__n\n","\n","\n","################# Reproducibility ######################à\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","\n","# Using a generator and the following function as `worker_init_fn` preserves reproducibility when using DataLoader\n","def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","\n","def create_reproducible_dataloader(*args, **kwargs):\n","    generator = torch.Generator()\n","    return DataLoader(\n","        *args,\n","        **kwargs,\n","        #   worker_init_fn=seed_worker,\n","        #   generator=generator\n","    )\n","\n","\n","###############################################################\n","\n","\n","def create_dirs_for_file(file_path):\n","    dir = os.path.dirname(file_path)\n","    ensure_dir_exists(dir)\n","\n","\n","def ensure_dir_exists(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","\n","def is_number(string: str) -> bool:\n","    \"\"\"\n","    Check whether a string is a number, both written or numeric.\n","\n","    Args:\n","    - string (str): The string to be checked.\n","\n","    Returns:\n","    - True if the string is a number, False otherwise.\n","    \"\"\"\n","    if string.isdigit():\n","        return True\n","    try:\n","        # Try to convert written number to integer\n","        text2num(string, \"en\", relaxed=True)\n","        return True\n","    except ValueError:\n","        return False\n","\n","\n","def batched_function(fn, scalar_output=True):\n","    def execute_on_batch(batch):\n","        examples = [\n","            fn(dict(zip(batch.keys(), values))) for values in zip(*batch.values())\n","        ]\n","\n","        if scalar_output:\n","            return {\n","                key: [example[key] for example in examples]\n","                for key in examples[0].keys()\n","            }\n","\n","        return {\n","            key: list(itertools.chain(*(example[key] for example in examples)))\n","            for key in examples[0].keys()\n","        }\n","\n","    return execute_on_batch\n","\n","\n","def create_dataframe(dataset: datasets.DatasetDict):\n","    dataset.set_format(\"pandas\")\n","\n","    dataset_ = []\n","    for split, ds in dataset.items():\n","        split_df = ds[:]\n","        split_df[\"split\"] = split\n","        dataset_.append(split_df)\n","    dataset_ = pd.concat(dataset_)\n","    dataset_.reset_index(drop=True, inplace=True)\n","    dataset.reset_format()\n","\n","    return dataset_\n","\n","\n","def explode_qa(dataset: pd.DataFrame):\n","    dataset = dataset.explode([\"questions\", \"answers\"])\n","    dataset.rename(columns={\"questions\": \"question\", \"answers\": \"answer\"}, inplace=True)\n","\n","    questions = pd.json_normalize(dataset[\"question\"])\n","    questions = questions[[\"turn_id\", \"input_text\"]]\n","    questions.rename(\n","        columns={\"input_text\": \"question\", \"turn_id\": \"turn\"}, inplace=True\n","    )\n","\n","    answers = pd.json_normalize(dataset[\"answer\"])\n","    answers = answers[\n","        [\"input_text\", \"span_text\", \"span_start\", \"span_end\", \"answer_type\"]\n","    ]\n","    answers.rename(\n","        columns={\"input_text\": \"answer\", \"span_text\": \"rationale\"}, inplace=True\n","    )\n","\n","    dataset.reset_index(inplace=True)\n","    dataset.drop([\"index\", \"question\", \"answer\"], axis=1, inplace=True)\n","    dataset = dataset.join(questions)\n","    dataset = dataset.join(answers)\n","\n","    cols = dataset.columns.tolist()\n","    cols.append(cols.pop(cols.index(\"last_turn\")))\n","    cols.append(cols.pop(cols.index(\"qa_length\")))\n","    cols.append(cols.pop(cols.index(\"split\")))\n","    return dataset[cols]\n","\n","\n","def plot_answer_type_distribution(qa_dataset: pd.DataFrame):\n","    plot_distribution(qa_dataset, field=\"answer_type\", hue=\"split\")\n","\n","def plot_distribution(dataset: pd.DataFrame, field: str, hue: str = None):\n","    if hue is not None:\n","        dataset = dataset.groupby(hue)\n","\n","    distribution = dataset[field].value_counts(\n","        normalize=True\n","    )\n","    distribution = distribution.apply(\n","        lambda x: np.round(x, decimals=3) * 100\n","    )\n","    distribution = distribution.rename(\n","        \"frequency\"\n","    ).reset_index()\n","    ax = sns.barplot(\n","        distribution, x=field, y=\"frequency\", hue=hue\n","    )\n","\n","    for i in ax.containers:\n","        ax.bar_label(\n","            i,\n","        )\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def show_inputs(tokenizer, data, inputs):\n","    for k, v in inputs.items():\n","        print(f\"{k:<27}: {v}\")\n","    print()\n","\n","    for idx in range(len(inputs[\"input_ids\"])):\n","        show_input(tokenizer, data, inputs, idx)\n","        print()\n","\n","\n","def show_input(tokenizer, data, inputs, idx):\n","    sample_idx = (\n","        inputs[\"overflow_to_sample_mapping\"][idx]\n","        if \"overflow_to_sample_mapping\" in inputs\n","        else idx\n","    )\n","\n","    input_ids = np.asarray(inputs[\"input_ids\"][idx])\n","    passage_mask = np.asarray(inputs[\"passage_mask\"][idx])\n","    rationale_labels = np.asarray(inputs[\"rationale_labels\"][idx])\n","    rationale_start = inputs[\"rationale_start\"][idx]\n","    rationale_end = inputs[\"rationale_end\"][idx]\n","    labels = np.asarray(inputs[\"labels\"][idx])\n","    decoder_attention_mask = np.asarray(inputs[\"decoder_attention_mask\"][idx])\n","\n","    passage = input_ids[passage_mask.astype(np.bool_)]\n","    rationale = input_ids[rationale_labels > 0]\n","    assert np.all(rationale == input_ids[rationale_start:rationale_end])\n","    answer = labels[decoder_attention_mask.astype(np.bool_)]\n","\n","    print(\"Input:\", tokenizer.decode(input_ids))\n","    print(\"Q:\", data[\"question\"][sample_idx])\n","    print(\"P (-):\", data[\"passage\"][sample_idx])\n","    print(\"P (+):\", tokenizer.decode(passage))\n","    print(\"R (-):\", data[\"rationale\"][sample_idx])\n","    print(\"R (+):\", tokenizer.decode(rationale))\n","    print(\"A (-):\", data[\"answer\"][sample_idx])\n","    print(\"A (+):\", tokenizer.decode(answer))\n","    print(\"History:\", data[\"history\"][sample_idx])\n","\n","\n","def logits_to_class(logits, task: Literal[\"binary\", \"multiclass\"]) -> torch.LongTensor:\n","    if task == \"binary\":\n","        return (logits > 0.0).long()\n","    elif task == \"multiclass\":\n","        return torch.argmax(logits, dim=-1).long()\n","    else:\n","        raise ValueError(\n","            \"Invalid task. Supported values are 'binary' and 'multiclass'.\"\n","        )\n","    \n","def prepare_model_inputs(model: nn.Module, inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n","    forward_signature = set(inspect.signature(model.forward).parameters)\n","    inputs = {\n","        argument: value.to(model.device)\n","        for argument, value in inputs.items()\n","        if argument in forward_signature\n","    }\n","    return inputs"]},{"cell_type":"markdown","metadata":{},"source":["# Model definition"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.090027Z","iopub.status.busy":"2023-08-02T15:57:47.089361Z","iopub.status.idle":"2023-08-02T15:57:47.149413Z","shell.execute_reply":"2023-08-02T15:57:47.148289Z","shell.execute_reply.started":"2023-08-02T15:57:47.089990Z"},"trusted":true},"outputs":[],"source":["@dataclass\n","class QAEncoderModelOutput(transformers.utils.ModelOutput):\n","    \"\"\"\n","    Base class for outputs of question answering with rationale models.\n","    Args:\n","        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n","            Total rationale extraction loss is the sum of a Binary Cross-Entropy for the tokens in the passage.\n","        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n","            Sequence of hidden-states at the output of the last layer of the model.\n","        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n","            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n","            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n","            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n","        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n","            heads.\n","        rationale_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n","            Rationale classification scores (before Sigmoid).\n","        yng_logits (`torch.FloatTensor` of shape `(batch_size, 3)`):\n","            Yes/No/Generative scores (before Sigmoid).\n","    \"\"\"\n","\n","    loss: Optional[torch.FloatTensor] = None\n","    last_hidden_state: torch.FloatTensor = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    rationale_logits: torch.FloatTensor = None\n","    yng_logits: torch.FloatTensor = None\n","\n","\n","@dataclass\n","class QAEncoderDecoderModelOutput(transformers.utils.ModelOutput):\n","    \"\"\"\n","    Class for [`QAEncoderDecoderModel`] outputs.\n","    Args:\n","        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n","            Language modeling loss.\n","        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n","            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n","        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n","            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n","            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n","            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n","            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n","            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n","        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n","            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n","            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n","            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n","        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n","            self-attention heads.\n","        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n","            weighted average in the cross-attention heads.\n","        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n","            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n","        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n","            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n","            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n","            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n","        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n","            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n","            sequence_length)`.\n","            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n","            self-attention heads.\n","        encoder_rationale_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n","            Encoder rationale classification scores (before Sigmoid).\n","        encoder_yng_logits (`torch.FloatTensor` of shape `(batch_size, 3)`):\n","            Encoder Yes/No/Generative scores (before Sigmoid).\n","    \"\"\"\n","\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n","    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n","    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_rationale_logits: Optional[torch.FloatTensor] = None\n","    encoder_yng_logits: torch.FloatTensor = None\n","\n","\n","class QAEncoderDecoderModel(transformers.EncoderDecoderModel):\n","    def __init__(\n","        self,\n","        encoder: transformers.PreTrainedModel,\n","        decoder: transformers.PreTrainedModel,\n","        config: Optional[transformers.EncoderDecoderConfig] = None,\n","    ):\n","        super(QAEncoderDecoderModel, self).__init__(\n","            encoder=encoder, decoder=decoder, config=config\n","        )\n","\n","    def tie_weights(self):\n","        # tie encoder & decoder if needed\n","        if self.config.tie_encoder_decoder:\n","            # tie encoder and decoder base model\n","            encoder_base_model_prefix = self.encoder.base_model_prefix\n","            decoder_base_model_prefix = self.decoder.base_model_prefix\n","            self._tie_encoder_decoder_weights(\n","                self.encoder._modules[encoder_base_model_prefix],\n","                self.decoder._modules[decoder_base_model_prefix],\n","                self.decoder.base_model_prefix,\n","            )\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        token_type_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        passage_mask: Optional[torch.BoolTensor] = None,\n","        decoder_input_ids: Optional[torch.LongTensor] = None,\n","        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n","        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n","        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","        # labels: Optional[torch.LongTensor] = None,\n","        use_cache: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","        teacher_force: Optional[float] = None,\n","        rationale_labels: Optional[torch.BoolTensor] = None,\n","        **kwargs,\n","    ) -> Union[Tuple, transformers.modeling_outputs.Seq2SeqLMOutput]:\n","        kwargs_encoder = {\n","            argument: value\n","            for argument, value in kwargs.items()\n","            if not argument.startswith(\"decoder\")\n","        }\n","\n","        if encoder_outputs is None:\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","                passage_mask=passage_mask,\n","                return_dict=return_dict,\n","                teacher_force=teacher_force,\n","                rationale_labels=rationale_labels,\n","                **kwargs_encoder,\n","            )\n","\n","        outputs = super().forward(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            encoder_outputs=encoder_outputs,\n","            past_key_values=past_key_values,\n","            inputs_embeds=inputs_embeds,\n","            decoder_inputs_embeds=decoder_inputs_embeds,\n","            # labels=labels,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            token_type_ids=token_type_ids,\n","            **kwargs,\n","        )\n","\n","        if not return_dict:\n","            return outputs + encoder_outputs[-2:]\n","\n","        return QAEncoderDecoderModelOutput(\n","            **outputs,\n","            encoder_rationale_logits=encoder_outputs.rationale_logits,\n","            encoder_yng_logits=encoder_outputs.yng_logits,\n","        )\n","\n","\n","class QAEncoder(transformers.PreTrainedModel):\n","    base_model_prefix = \"encoder\"\n","\n","    def __init__(self, encoder, config) -> None:\n","        super().__init__(config)\n","        self.config = config\n","\n","        self.encoder = encoder\n","        self.rationale_head = TokenSelectionHead(config)\n","        self.yes_no_gen_head = YesNoGenHead(config)\n","\n","        self.post_init()\n","\n","    def _init_weights(self, module):\n","        \"\"\"Initialize the weights\"\"\"\n","        if isinstance(module, nn.Linear):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.Tensor] = None,\n","        token_type_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        passage_mask: Optional[torch.Tensor] = None,\n","        # labels: Optional[torch.FloatTensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","        teacher_force: Optional[float] = None,\n","        rationale_labels: Optional[torch.BoolTensor] = None,\n","        **kwargs,\n","    ) -> Union[Tuple, QAEncoderModelOutput]:\n","        assert passage_mask is not None, \"Passage mask is required\"\n","\n","        outputs = self.encoder(\n","            input_ids=input_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask,\n","            inputs_embeds=inputs_embeds,\n","            # labels=labels,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            **kwargs,\n","        )\n","\n","        last_hidden_state = outputs[0]\n","        pooled_output = outputs[1]\n","\n","        rationale_logits = self.rationale_head(last_hidden_state)\n","\n","        passage_mask = passage_mask.unsqueeze(-1)\n","        p_rationale = torch.sigmoid(rationale_logits)\n","        # substitute the last_hidden_state[passage] with p_rationale * last_hidden_state[passage]\n","        # ideally, our network keeps only the span of the passage which represents the rationale\n","        if self.training:\n","            if teacher_force is not None and torch.rand(1) < teacher_force:\n","                p_rationale = rationale_labels.unsqueeze(-1)\n","        else:\n","            p_rationale = p_rationale > self.config.p_rationale_threshold\n","\n","        weighted_passage_hidden_state = passage_mask * p_rationale * last_hidden_state\n","        qa_seq_hidden_state = (1 - passage_mask) * last_hidden_state\n","        last_hidden_state = weighted_passage_hidden_state + qa_seq_hidden_state\n","\n","        yng_logits = self.yes_no_gen_head(weighted_passage_hidden_state, pooled_output)\n","\n","        rationale_logits = rationale_logits.squeeze(-1)\n","\n","        loss = None\n","        if not return_dict:\n","            output = (last_hidden_state,) + outputs[2:] + (rationale_logits,)\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return QAEncoderModelOutput(\n","            loss=loss,\n","            last_hidden_state=last_hidden_state,\n","            rationale_logits=rationale_logits,\n","            yng_logits=yng_logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","\n","class TokenSelectionHead(nn.Module):\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n","        self.act_fn = nn.ReLU()\n","        self.hidden_to_logit = nn.Linear(config.hidden_size, 1, bias=False)\n","\n","    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.act_fn(hidden_states)\n","        logits = self.hidden_to_logit(hidden_states)\n","        return logits\n","\n","\n","class YesNoGenHead(nn.Module):\n","    def __init__(self, config) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n","        self.act_fn = nn.ReLU()\n","        self.hidden_to_logit = nn.Linear(config.hidden_size, 1, bias=False)\n","        self.softmax = nn.Softmax(dim=-2)\n","\n","        classifier_dropout = (\n","            config.classifier_dropout\n","            if config.classifier_dropout is not None\n","            else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(2 * config.hidden_size, 3)\n","\n","    def forward(\n","        self,\n","        rationale_weighted_hidden_states: torch.FloatTensor,\n","        pooled_output: torch.FloatTensor,\n","    ) -> torch.FloatTensor:\n","        rationale_weighted_hidden_states = self.dense(rationale_weighted_hidden_states)\n","        rationale_weighted_hidden_states = self.act_fn(\n","            rationale_weighted_hidden_states\n","        )  # BxTxD\n","        logits = self.hidden_to_logit(rationale_weighted_hidden_states)  # BxTx1\n","        attention_scores = self.softmax(logits)  # BxTx1\n","        weighted_tokens = attention_scores * rationale_weighted_hidden_states  # BxTxD\n","        weighted_pooled_output = torch.sum(weighted_tokens, dim=-2)  # BxD\n","        pooled_output = torch.cat(\n","            (weighted_pooled_output, pooled_output), dim=-1\n","        )  # Bx2D\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        return logits\n","\n","\n","def initialize_cross_attention_layer_with_self_attention_layer(\n","    self_attention: nn.Module,\n","    cross_attention: nn.Module,\n","    cross_attention_layer_prefix: str,\n","):\n","    uninitialized_cross_attention_weights: List[str] = []\n","    if cross_attention.__class__ != self_attention.__class__:\n","        print(\n","            f\"{cross_attention.__class__} and {self_attention.__class__} are not equal. In this case make sure that all encoder\"\n","            \" weights are correctly initialized.\"\n","        )\n","\n","    def initialize_cross_attention_with_self_attention_recursively(\n","        self_attention_pointer: nn.Module,\n","        cross_attention_pointer: nn.Module,\n","        module_name: str,\n","        uninitialized_cross_attention_weights: List[str],\n","        depth=0,\n","    ):\n","        assert isinstance(self_attention_pointer, nn.Module) and isinstance(\n","            cross_attention_pointer, nn.Module\n","        ), f\"{self_attention_pointer} and {cross_attention_pointer} have to be of type nn.Module\"\n","        if hasattr(self_attention_pointer, \"weight\"):\n","            assert hasattr(cross_attention_pointer, \"weight\")\n","            cross_attention_pointer.weight.data = (\n","                self_attention_pointer.weight.data.clone().detach()\n","            )\n","            if hasattr(self_attention_pointer, \"bias\"):\n","                assert hasattr(cross_attention_pointer, \"bias\")\n","                cross_attention_pointer.bias.data = (\n","                    self_attention_pointer.bias.data.clone().detach()\n","                )\n","            return\n","\n","        cross_attention_modules = cross_attention_pointer._modules\n","        self_attention_modules = self_attention_pointer._modules\n","        if len(self_attention_modules) > 0:\n","            assert (\n","                len(cross_attention_modules) > 0\n","            ), f\"Cross-attention module {cross_attention_pointer} does not match self-attention module {self_attention_pointer}\"\n","\n","            all_cross_attention_weights = {\n","                module_name + \"/\" + sub_name\n","                for sub_name in cross_attention_modules.keys()\n","            }\n","            cross_attention_layer_pos = 0\n","            for name, module in self_attention_modules.items():\n","                if name.isdigit():\n","                    cross_attention_name = str(int(name) + cross_attention_layer_pos)\n","                    self_attention_name = name\n","                    if not isinstance(\n","                        self_attention_modules[self_attention_name],\n","                        type(cross_attention_modules[cross_attention_name]),\n","                    ) and len(cross_attention_modules) != len(self_attention_modules):\n","                        # this can happen if the name corresponds to the position in a list module list of layers\n","                        # in this case the decoder has added a cross-attention that the encoder does not have\n","                        # thus skip this step and subtract one layer pos from encoder\n","                        cross_attention_layer_pos -= 1\n","                        continue\n","                elif name not in cross_attention_modules:\n","                    continue\n","                elif depth > 500:\n","                    raise ValueError(\n","                        \"Max depth of recursive function `initialize_cross_attention_with_self_attention` reached. It seems that there is\"\n","                        \" a circular dependency between two or more `nn.Modules` of your model.\"\n","                    )\n","                else:\n","                    self_attention_name = cross_attention_name = name\n","                initialize_cross_attention_with_self_attention_recursively(\n","                    self_attention_modules[self_attention_name],\n","                    cross_attention_modules[cross_attention_name],\n","                    module_name + \"/\" + name,\n","                    uninitialized_cross_attention_weights,\n","                    depth=depth + 1,\n","                )\n","                all_cross_attention_weights.remove(\n","                    module_name + \"/\" + cross_attention_name\n","                )\n","\n","            uninitialized_cross_attention_weights += list(all_cross_attention_weights)\n","\n","    # initialize weights recursively\n","    initialize_cross_attention_with_self_attention_recursively(\n","        self_attention,\n","        cross_attention,\n","        cross_attention_layer_prefix,\n","        uninitialized_cross_attention_weights,\n","    )\n","    if len(uninitialized_cross_attention_weights) > 0:\n","        warnings.warn(\n","            f\"The following cross_attention weights were not initialized with self_attention weights: {uninitialized_cross_attention_weights}\"\n","        )\n","\n","\n","def initialize_cross_attention_with_self_attention(model: EncoderDecoderModel):\n","    decoder_base_model_prefix = model.decoder.base_model_prefix\n","    for layer_idx in range(model.config.decoder.num_hidden_layers):\n","        decoder_layer = model.decoder._modules[decoder_base_model_prefix].encoder.layer[\n","            layer_idx\n","        ]\n","        cross_attention = decoder_layer.crossattention\n","        self_attention = decoder_layer.attention\n","        cross_attention_name = f\"layer.{layer_idx}.crossattention\"\n","        initialize_cross_attention_layer_with_self_attention_layer(\n","            self_attention, cross_attention, cross_attention_name\n","        )\n","    print(\"Cross-attention has been initialized with self-attention weights.\")\n","\n","\n","def make_encoder_decoder_model(\n","    checkpoint,\n","    decoder_max_length,\n","    generation_kwargs,\n","    tokenizer: Optional[transformers.PreTrainedTokenizer] = None,\n","    initialize_cross_attention=True,\n","):\n","    tokenizer, encoder = make_qa_encoder(checkpoint, tokenizer=tokenizer)\n","    decoder = transformers.AutoModelForCausalLM.from_pretrained(\n","        checkpoint,\n","        is_decoder=True,\n","        add_cross_attention=True,\n","        decoder_start_token_id=tokenizer.bos_token_id,\n","    )\n","\n","    config = transformers.EncoderDecoderConfig.from_encoder_decoder_configs(\n","        encoder.config,\n","        decoder.config,\n","        tie_encoder_decoder=True,\n","        decoder_start_token_id=tokenizer.cls_token_id,\n","        eos_token_id=tokenizer.sep_token_id,\n","        pad_token_id=tokenizer.pad_token_id,\n","        # sensible parameters for generation\n","        vocab_size=decoder.config.vocab_size,\n","        max_new_tokens=decoder_max_length,\n","        **generation_kwargs,\n","    )\n","\n","    model = QAEncoderDecoderModel(encoder=encoder, decoder=decoder, config=config)\n","    if initialize_cross_attention:\n","        initialize_cross_attention_with_self_attention(model)\n","\n","    return tokenizer, model\n","\n","\n","def make_qa_encoder(\n","    checkpoint, tokenizer: Optional[transformers.PreTrainedTokenizer] = None\n","):\n","    if tokenizer is None:\n","        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","    encoder = AutoModel.from_pretrained(checkpoint)\n","    encoder.config.p_rationale_threshold = 0.5\n","    encoder = QAEncoder(encoder, encoder.config)\n","    return tokenizer, encoder\n"]},{"cell_type":"markdown","metadata":{},"source":["# Losses"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.151673Z","iopub.status.busy":"2023-08-02T15:57:47.151229Z","iopub.status.idle":"2023-08-02T15:57:47.180257Z","shell.execute_reply":"2023-08-02T15:57:47.179183Z","shell.execute_reply.started":"2023-08-02T15:57:47.151636Z"},"trusted":true},"outputs":[],"source":["_EPSILON = 1e-7\n","\n","def apply_reduction(input: torch.Tensor, reduction: str, dim=0):\n","    if reduction == \"none\":\n","        return input\n","    if reduction == \"mean\":\n","        return torch.mean(input, dim=dim)\n","    if reduction == \"sum\":\n","        return torch.sum(input, dim=dim)\n","\n","    raise ValueError(\n","        \"Invalid reduction. Supported values are 'none', 'mean' and 'sum'.\"\n","    )\n","\n","\n","class Loss(Protocol):\n","    def __call__(self, outputs, targets: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n","        pass\n","\n","\n","class ComputeLoss(Protocol):\n","    def __call__(\n","        outputs, targets: Dict[str, torch.Tensor]\n","    ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","        pass\n","\n","\n","def wrap_loss_fn(\n","    name: str, loss_fn: Loss\n",") -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","    def loss(outputs, targets):\n","        loss_value = loss_fn(outputs, targets)\n","        return loss_value, {name: loss_value.item()}\n","\n","    return loss\n","\n","\n","@dataclass\n","class Criterion:\n","    name: str\n","    loss_fn: Loss\n","    weight: float = 1.0\n","\n","\n","class UncertaintyLoss(nn.Module, ComputeLoss):\n","    def __init__(self, name: str, loss_fn: Loss, initial_weight: float = 1.0) -> None:\n","        super(UncertaintyLoss, self).__init__()\n","        self.name = name\n","        self.loss_fn = loss_fn\n","        log_sigma_square = -np.log(initial_weight)\n","        self.log_sigma_square = nn.Parameter(\n","            torch.tensor(log_sigma_square, requires_grad=True, dtype=torch.float32)\n","        )\n","\n","    def forward(\n","        self, outputs, targets: Dict[str, torch.Tensor]\n","    ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","        inner_loss = self.loss_fn(outputs, targets)\n","        # 1/sigma^2 * L + 2 log sigma\n","        weight = torch.exp(-self.log_sigma_square)\n","        loss = weight * inner_loss + self.log_sigma_square\n","\n","        return loss, {\n","            self.name: inner_loss.item(),\n","            f\"{self.name}_weight\": weight.item(),\n","        }\n","\n","\n","# class UncertaintyLoss(nn.Module):\n","#     def __init__(self, *criteria: Criterion) -> None:\n","#         super(UncertaintyLoss, self).__init__()\n","#         self.criteria = criteria\n","#         weights = [criterion.weight for criterion in self.criteria]\n","#         log_square_sigmas = -np.log(weights)\n","#         self.log_square_sigmas = nn.Parameter(\n","#             torch.tensor(log_square_sigmas, requires_grad=True, dtype=torch.float32)\n","#         )\n","\n","#     def forward(\n","#         self, outputs, targets: Dict[str, torch.Tensor]\n","#     ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","#         losses = {}\n","#         total_loss = 0.0\n","#         for criterion, log_sigma_square in zip(self.criteria, self.log_square_sigmas):\n","#             loss = criterion.loss_fn(outputs, targets)\n","#             # 1/sigma^2 * L + 2 log sigma\n","#             weight = torch.exp(-log_sigma_square)\n","#             total_loss += weight * loss + log_sigma_square\n","#             losses[f\"{criterion.name}_weight\"] = weight.item()\n","#             losses[criterion.name] = loss.item()\n","\n","#         return total_loss, losses\n","\n","\n","def generative_loss(\n","    logits: torch.FloatTensor,\n","    labels: torch.IntTensor,\n","    reduction: str = \"mean\",\n","    mask: torch.Tensor = None,\n",") -> torch.FloatTensor:\n","    if mask is not None:\n","        logits = logits[mask.bool()]\n","        labels = labels[mask.bool()]\n","\n","    # swap seq_length with vocabulary dimension\n","    logits = torch.transpose(logits, 1, 2)  # batch_size x seq_length x vocab\n","    loss = F.cross_entropy(\n","        input=logits, target=labels, reduction=\"none\"\n","    )  # batch_size x seq_length\n","    n_tokens_per_sample = torch.sum(labels != -100, dim=-1)  # batch_size\n","    n_tokens_per_sample = torch.clamp(n_tokens_per_sample, min=_EPSILON)\n","    loss = torch.sum(loss, dim=-1) / n_tokens_per_sample  # batch_size\n","    return apply_reduction(loss, reduction=reduction)\n","\n","\n","class EncoderDecoderGenerativeLoss(Loss):\n","    def __init__(self, reduction: str = \"mean\") -> None:\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"logits\"]\n","        labels = targets[\"labels\"]\n","\n","        return generative_loss(logits, labels, reduction=self.reduction, mask=mask)\n","\n","\n","# def rationale_loss(self, outputs, targets: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n","def rationale_loss(\n","    logits: torch.FloatTensor,\n","    labels: torch.IntTensor,\n","    passage_mask: torch.IntTensor,\n","    max_rationale_length: int,\n","    reduction=\"mean\",\n","    mask: torch.Tensor = None,\n",") -> torch.FloatTensor:\n","    \"\"\"\n","    li = w * BCE(y_pred_i, y_true_i)\n","    , where w = w_positive if y_true_i is positive\n","            w = w_negative if y_true_i is negative\n","    w_positive = totals / positives\n","    w_negative = totals / negatives\n","    , where totals, positives and negatives are computed for each sequence\n","\n","    Ls = sum_i=1..seq_length li / sum(w_i)\n","    L = sum_s=1..N Ls / N,\n","    , where N is the #sequences whose rationale length is <= max_rationale_length\n","    \"\"\"\n","\n","    # rationale_logits = outputs[self.rationale_logits_name]\n","    # rationale_labels = targets[self.rationale_labels_name]\n","    # passage_mask = targets[self.passage_mask_name]\n","\n","    labels = labels * passage_mask\n","\n","    rationale_lengths = torch.sum(labels, dim=-1)  # batch_size\n","    valid_rationales = rationale_lengths <= max_rationale_length\n","    if mask is not None:\n","        valid_rationales = valid_rationales & mask.bool()\n","\n","    labels = labels[valid_rationales]\n","    passage_mask = passage_mask[valid_rationales]\n","    logits = logits[valid_rationales]\n","\n","    # n_sequences = torch.sum(valid_rationales)\n","\n","    totals = torch.sum(passage_mask, -1, keepdim=True)  # N x 1\n","    positives = torch.sum(labels, -1, keepdim=True)  # N x 1\n","    negatives = totals - positives  # N x 1\n","    weights = torch.where(\n","        labels == 1.0, totals / positives, totals / negatives\n","    )  # N x seq_length\n","    weights = torch.where(weights != torch.inf, weights, 0.0)  # N x seq_length\n","    weights = weights * passage_mask  # N x seq_length\n","    normalize_factor = torch.clamp(\n","        torch.sum(weights, dim=-1, keepdim=True), min=_EPSILON\n","    )\n","    weights = weights / normalize_factor  # N x seq_length\n","    # weights = weights * valid_rationales / n_sequences\n","\n","    # N x seq_length\n","    per_token_loss = F.binary_cross_entropy_with_logits(\n","        input=logits,\n","        target=labels,\n","        weight=weights,\n","        reduction=\"none\",\n","    )\n","\n","    loss = torch.sum(per_token_loss, dim=-1)  # N\n","    return apply_reduction(loss, reduction=reduction)\n","\n","\n","class EncoderDecoderRationaleLoss(Loss):\n","    def __init__(self, max_rationale_length: int, reduction: str = \"mean\") -> None:\n","        self.max_rationale_length = max_rationale_length\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"encoder_rationale_logits\"]\n","        labels = targets[\"rationale_labels\"]\n","        passage_mask = targets[\"passage_mask\"]\n","\n","        return rationale_loss(\n","            logits,\n","            labels,\n","            passage_mask,\n","            self.max_rationale_length,\n","            reduction=self.reduction,\n","            mask=mask,\n","        )\n","\n","\n","class EncoderRationaleLoss(Loss):\n","    def __init__(self, max_rationale_length: int, reduction: str = \"mean\") -> None:\n","        self.max_rationale_length = max_rationale_length\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"rationale_logits\"]\n","        labels = targets[\"rationale_labels\"]\n","        passage_mask = targets[\"passage_mask\"]\n","\n","        return rationale_loss(\n","            logits,\n","            labels,\n","            passage_mask,\n","            self.max_rationale_length,\n","            reduction=self.reduction,\n","            mask=mask,\n","        )\n","\n","\n","def yes_no_gen_loss(\n","    logits: torch.FloatTensor,\n","    labels: torch.IntTensor,\n","    weight: Optional[torch.FloatTensor] = None,\n","    reduction=\"mean\",\n","    mask: torch.Tensor = None,\n",") -> torch.FloatTensor:\n","    if mask is not None:\n","        logits = logits[mask.bool()]\n","        labels = labels[mask.bool()]\n","\n","    if weight is not None:\n","        weight.to(logits.device)\n","\n","    loss = F.cross_entropy(logits, labels, reduction=reduction)\n","    return loss\n","\n","\n","class EncoderDecoderYNGLoss(Loss):\n","    def __init__(\n","        self, weight: Optional[torch.FloatTensor] = None, reduction: str = \"mean\"\n","    ) -> None:\n","        self.weight = weight\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,\n","    ) -> torch.FloatTensor:\n","        logits = outputs[\"encoder_yng_logits\"]\n","        labels = targets[\"yng_label\"]\n","\n","        return yes_no_gen_loss(\n","            logits, labels, weight=self.weight, reduction=self.reduction, mask=mask\n","        )\n","\n","\n","class EncoderYNGLoss(Loss):\n","    def __init__(self, weight: Optional[torch.FloatTensor] = None, reduction: str = \"mean\") -> None:\n","        self.weight = weight\n","        self.reduction = reduction\n","\n","    def __call__(\n","        self,\n","        outputs,\n","        targets: Dict[str, torch.Tensor],\n","        mask: torch.Tensor = None,) -> torch.FloatTensor:\n","        logits = outputs[\"yng_logits\"]\n","        labels = targets[\"yng_label\"]\n","\n","        return yes_no_gen_loss(\n","            logits,\n","            labels,\n","            weight=self.weight,\n","            reduction=self.reduction,\n","            mask=mask\n","        )\n","\n","\n","class EncoderDecoderLoss(nn.Module):\n","    def __init__(\n","        self,\n","        max_rationale_length,\n","        yng_loss_weight=1.0,\n","        rationale_loss_weight=1.0,\n","        generative_loss_weight=1.0,\n","    ) -> None:\n","        super().__init__()\n","\n","        self.yng_loss_weight = yng_loss_weight\n","        self.rationale_loss_weight = rationale_loss_weight\n","        self.generative_loss_weight = generative_loss_weight\n","\n","        weight = torch.Tensor([1 / 11.0, 1 / 9.0, 1 / 80.0])\n","        weight = weight / torch.sum(weight)\n","        weight = None\n","        self.yes_no_gen_loss_fn = EncoderDecoderYNGLoss(weight=weight)\n","        self.yes_no_gen_loss_fn = EncoderDecoderYNGLoss()\n","        self.rationale_loss_fn = EncoderDecoderRationaleLoss(\n","            max_rationale_length=max_rationale_length\n","        )\n","        self.generative_loss_fn = EncoderDecoderGenerativeLoss()\n","\n","    def forward(\n","        self, outputs, targets: Dict[str, torch.Tensor]\n","    ) -> Tuple[torch.FloatTensor, Dict[str, float]]:\n","        yng_loss = self.yes_no_gen_loss_fn(outputs, targets)\n","\n","        is_generative = ~targets[\"yes_no\"].bool()\n","        rationale_loss = self.rationale_loss_fn(outputs, targets, mask=is_generative)\n","        generative_loss = self.generative_loss_fn(outputs, targets, mask=is_generative)\n","\n","        total_loss = (\n","            self.yng_loss_weight * yng_loss\n","            + self.rationale_loss_weight * rationale_loss\n","            + self.generative_loss_weight * generative_loss\n","        )\n","        loss_logs = {\n","            \"yng_loss\": yng_loss.item(),\n","            \"rationale_loss\": rationale_loss.item(),\n","            \"generative_loss\": generative_loss.item(),\n","        }\n","\n","        return total_loss, loss_logs\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class Linear:\n","    def __init__(\n","        self, start_value:float, end_value:float, total_iters:int\n","    ):\n","\n","        self.start_value = float(start_value)\n","        self.end_value = float(end_value)\n","        self.total_iters = total_iters\n","        self.current_step = 0\n","\n","    def step(self):\n","        self.current_step += 1\n","\n","    def get_value(self):\n","        return (\n","            self.start_value\n","            + (self.end_value - self.start_value)\n","            / self.total_iters\n","            * self.current_step\n","        )\n","\n","class DummyScheduler:\n","    def __init__(self, optimizer: torch.optim.Optimizer) -> None:\n","        self.optimizer = optimizer\n","\n","    def step(self):\n","        None\n","\n","    def get_last_lr(self):\n","        return [group[\"lr\"] for group in self.optimizer.param_groups]\n","\n","    def state_dict(self):\n","        return {}\n","\n","\n","@dataclass\n","class DynamicPaddingCollatorForSeq2Seq:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs received, as well as the labels.\n","\n","    Args:\n","        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n","            The tokenizer used for encoding the data.\n","        model ([`PreTrainedModel`]):\n","            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n","            prepare the *decoder_input_ids*\n","\n","            This is useful when using *label_smoothing* to avoid calculating loss twice.\n","        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n","            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n","            among:\n","\n","            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n","              sequence is provided).\n","            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n","              acceptable input length for the model if that argument is not provided.\n","            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n","        max_length (`int`, *optional*):\n","            Maximum length of the returned list and optionally padding length (see above).\n","        pad_to_multiple_of (`int`, *optional*):\n","            If set will pad the sequence to a multiple of the provided value.\n","\n","            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n","            7.5 (Volta).\n","        label_pad_token_id (`int`, *optional*, defaults to -100):\n","            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n","        return_tensors (`str`):\n","            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n","    \"\"\"\n","\n","    tokenizer: transformers.PreTrainedTokenizerBase\n","    model: Optional[Any] = None\n","    padding: Union[bool, str, transformers.utils.PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    label_pad_token_id: int = -100\n","    return_tensors: str = \"pt\"\n","\n","    def __call__(self, features, return_tensors=None):\n","        if return_tensors is None:\n","            return_tensors = self.return_tensors\n","\n","        # We have to pad the labels and other features not in  `tokenizer.model_input_names` before calling `tokenizer.pad`\n","        # as `tokenizer.pad` method will pad only features in `tokenizer.model_input_names`\n","        tokenizer_input_names = set(self.tokenizer.model_input_names)\n","        for feature_name in features[0].keys():\n","            if feature_name not in tokenizer_input_names and isinstance(\n","                features[0][feature_name], list\n","            ):\n","                if feature_name.endswith(\"labels\"):\n","                    self.pad_feature(feature_name, features, self.label_pad_token_id)\n","                else:\n","                    self.pad_feature(feature_name, features)\n","\n","        features = self.tokenizer.pad(\n","            features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=return_tensors,\n","        )\n","\n","        # prepare decoder_input_ids\n","        if (\n","            \"labels\" in features\n","            and \"decoder_input_ids\" not in features\n","            and self.model is not None\n","            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n","        ):\n","            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(\n","                labels=features[\"labels\"]\n","            )\n","            features[\"decoder_input_ids\"] = decoder_input_ids\n","\n","        return features\n","\n","    def pad_feature(self, feature_name, features, pad_id=0):\n","        items = (\n","            [feature[feature_name] for feature in features]\n","            if feature_name in features[0].keys()\n","            else None\n","        )\n","        # We have to pad the feature before calling `tokenizer.pad` as this method won't pad them and needs them of the\n","        # same length to return tensors.\n","        if items is not None:\n","            max_item_length = max(len(l) for l in items)\n","            if self.pad_to_multiple_of is not None:\n","                max_item_length = (\n","                    (max_item_length + self.pad_to_multiple_of - 1)\n","                    // self.pad_to_multiple_of\n","                    * self.pad_to_multiple_of\n","                )\n","\n","            padding_side = self.tokenizer.padding_side\n","            for feature in features:\n","                remainder = [pad_id] * (max_item_length - len(feature[feature_name]))\n","                if isinstance(feature[feature_name], list):\n","                    feature[feature_name] = (\n","                        feature[feature_name] + remainder\n","                        if padding_side == \"right\"\n","                        else remainder + feature[feature_name]\n","                    )\n","                elif padding_side == \"right\":\n","                    feature[feature_name] = np.concatenate(\n","                        [feature[feature_name], remainder]\n","                    )\n","                else:\n","                    feature[feature_name] = np.concatenate(\n","                        [remainder, feature[feature_name]]\n","                    )\n","\n","\n","def save_checkpoint(\n","    model, optimizer, scheduler, epoch, step, checkpoint_counter, checkpoint_path\n","):\n","    checkpoint = {\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","        \"scheduler_state_dict\": scheduler.state_dict(),\n","        \"epoch\": epoch,\n","        \"step\": step,\n","        \"checkpoint_counter\": checkpoint_counter,\n","    }\n","\n","    torch.save(checkpoint, checkpoint_path)\n","\n","\n","def load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None):\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","    if scheduler is not None:\n","        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","    epoch = checkpoint[\"epoch\"]\n","    step = checkpoint[\"step\"]\n","    checkpoint_counter = checkpoint[\"checkpoint_counter\"]\n","\n","    print(f\"Loaded checkpoint from '{checkpoint_path}'\")\n","\n","    return model, optimizer, scheduler, epoch, step, checkpoint_counter\n","\n","def prepare_inputs_for_train(\n","    dataset: datasets.DatasetDict,\n","    checkpoints: Dict[str, str],\n","    filename_fn,\n","    add_history=False,\n","    num_processes=None,\n","    verbose=True,\n","    **preprocessing_kwargs,\n","):\n","    for name, checkpoint in checkpoints.items():\n","        tokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)\n","        preprocessing = CoQADatasetPreprocessing(tokenizer, **preprocessing_kwargs)\n","\n","        if verbose:\n","            print(\"Preparing inputs for\", name, \"...\")\n","\n","        if not os.path.exists(filename_fn(name)):\n","            dataset_ = dataset.map(\n","                preprocessing.process_data_to_model_inputs,\n","                fn_kwargs={\"add_history\": add_history},\n","                batched=True,\n","                remove_columns=dataset[\"train\"].column_names,\n","                num_proc=num_processes,\n","            )\n","\n","            dataset_.save_to_disk(filename_fn(name))\n","            del dataset_\n","\n","        if verbose:\n","            dataset_ = datasets.load_from_disk(filename_fn(name))\n","            print(dataset_)\n","            print()\n","            print(\"Showing some input examples:\")\n","            decoded_inputs = tokenizer.batch_decode(dataset_[\"train\"][:5][\"input_ids\"])\n","            for decoded in decoded_inputs:\n","                print(decoded)\n","            print()\n","            del dataset_"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.184314Z","iopub.status.busy":"2023-08-02T15:57:47.183878Z","iopub.status.idle":"2023-08-02T15:57:47.198916Z","shell.execute_reply":"2023-08-02T15:57:47.197893Z","shell.execute_reply.started":"2023-08-02T15:57:47.184277Z"},"trusted":true},"outputs":[],"source":["per_token_f1_metric = MulticlassF1Score(\n","    num_classes=2,\n","    average=\"macro\",\n","    multidim_average=\"samplewise\",\n","    ignore_index=-100,\n",")\n","\n","macro_f1 = MulticlassF1Score(\n","    num_classes=3,\n","    average=\"macro\",\n","    ignore_index=-100,\n",")\n","\n","wh = [\"what\", \"when\", \"where\", \"which\", \"who\", \"how\", \"whose\", \"why\"]\n","\n","\n","def labels_to_answer(labels: torch.Tensor, tokenizer, ignore_index=-100) -> str:\n","    labels[labels == ignore_index] = tokenizer.pad_token_id\n","    answer = tokenizer.decode(labels, skip_special_tokens=True)\n","    return answer\n","\n","\n","def pad_input_tensors(inputs, collator):\n","    features = [dict(zip(inputs.keys(), values)) for values in zip(*inputs.values())]\n","    features = collator(features)\n","\n","    return features\n","\n","\n","def evaluate_answer(example: dict):\n","    return {\"answer_f1\": compute_f1(example[\"answer\"], example[\"pred_answer\"])}\n","\n","\n","def evaluate_rationale_f1(example: dict):\n","    rationale_f1 = per_token_f1_metric(\n","        logits_to_class(example[\"rationale_logits\"], task=\"binary\").long(),\n","        example[\"rationale_labels\"].long(),\n","    )\n","    # Ensure it is an array, not a scalar\n","    if rationale_f1.dim() == 0:\n","        rationale_f1.unsqueeze_(dim=0)\n","    return {\"rationale_f1\": rationale_f1}\n","\n","\n","def evaluate_model(model, tokenizer, dataset: datasets.Dataset, config):\n","    if \"passage\" in dataset.column_names:\n","        return evaluate_raw_data(model, tokenizer, dataset, config)\n","    elif \"input_ids\" in dataset.column_names:\n","        return evaluate_tokenized_dataset(model, tokenizer, dataset, config)\n","    else:\n","        raise ValueError(\"Date provided are not valid!\")\n","\n","\n","def evaluate_tokenized_dataset(model, tokenizer, dataset: datasets.Dataset, config):\n","    accelerator = Accelerator(mixed_precision=config.mixed_precision, cpu=config.cpu)\n","    model = accelerator.prepare(model)\n","    model.eval()\n","\n","    collator = DynamicPaddingCollatorForSeq2Seq(tokenizer, model)\n","\n","    dataset = dataset.map(\n","        lambda example: pad_input_tensors(example, collator),\n","        batched=True,\n","        batch_size=config.val_batch_size,\n","        load_from_cache_file=False,\n","    )\n","\n","    dataset = dataset.with_format(\"torch\", device=model.device)\n","\n","    dataset = dataset.map(\n","        lambda example: generate_answer(model, tokenizer, example),\n","        batched=True,\n","        batch_size=config.val_batch_size,\n","        load_from_cache_file=False,\n","    )\n","\n","    dataset = dataset.map(\n","        lambda example: {\n","            \"answer\": labels_to_answer(example[\"labels\"], tokenizer=tokenizer)\n","        },\n","        load_from_cache_file=False,\n","    )\n","\n","    # outputs = outputs.select_columns([\"source\", \"passage\", \"question\", \"rationale\", \"answer\", \"pred_answer\", \"answer_type\", 'yng_logits', 'rationale_logits'])\n","\n","    dataset = dataset.map(evaluate_answer, load_from_cache_file=False)\n","\n","    dataset = dataset.map(\n","        evaluate_rationale_f1, batched=True, batch_size=32, load_from_cache_file=False\n","    )\n","\n","    yng_data = dataset.select_columns([\"yng_logits\", \"yng_label\"])\n","    yng_data = yng_data.map(\n","        lambda example: {\n","            \"pred_yng_label\": logits_to_class(example[\"yng_logits\"], task=\"multiclass\")\n","        },\n","        batched=True,\n","        batch_size=config.val_batch_size,\n","        load_from_cache_file=False,\n","    )\n","    macro_f1_ = macro_f1.to(model.device)\n","    yng_f1 = macro_f1_(yng_data[\"pred_yng_label\"], yng_data[\"yng_label\"]).item()\n","\n","    rationales_f1 = torch.mean(dataset[\"rationale_f1\"]).item()\n","    answers_squad_f1 = torch.mean(dataset[\"answer_f1\"]).item()\n","\n","    dataset.reset_format()\n","    return dataset, {\n","        \"yng_f1\": yng_f1,\n","        \"rationales_f1\": rationales_f1,\n","        \"answers_squad_f1\": answers_squad_f1,\n","    }\n","\n","def evaluate_raw_data(model, tokenizer, data, config):\n","    model.eval()\n","    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    preprocessing = CoQADatasetPreprocessing(tokenizer, **CONFIG.preprocessing.__dict__)\n","\n","    outputs = data.map(\n","        lambda example: generate_answer_from_raw_data(\n","            model, tokenizer, preprocessing, example[\"passage\"], example[\"question\"]\n","        ),\n","        batched=True,\n","        batch_size=config.val_batch_size,\n","        load_from_cache_file=False\n","    )\n","\n","    outputs = outputs.map(evaluate_answer,\n","                          load_from_cache_file=False)\n","\n","    outputs = outputs.select_columns([\"source\",\n","                                      \"passage\",\n","                                      \"question\",\n","                                      \"rationale\",\n","                                      \"answer\",\n","                                      \"pred_answer\",\n","                                      \"answer_type\",\n","                                      \"answer_f1\",\n","                                    ])\n","    \n","    yes_answer = outputs.filter(lambda ex: \"yes\" in re.findall(r\"[\\w']+\", ex[\"answer\"].lower()), load_from_cache_file=False)\n","    no_answer = outputs.filter(lambda ex: \"no\" in re.findall(r\"[\\w']+\", ex[\"answer\"].lower()), load_from_cache_file=False)\n","    mc_question = outputs.filter(lambda ex: \"or\" in re.findall(r\"[\\w']+\", ex[\"question\"].lower()), load_from_cache_file=False)\n","    wh_question = outputs.filter(lambda ex: any(w in re.findall(r\"[\\w']+\", ex[\"question\"].lower()) for w in wh), load_from_cache_file=False)\n","\n","    len_data = len(outputs)\n","    return outputs, {\n","        \"tot_squad_f1\":  (np.mean(outputs[\"answer_f1\"]), 100),\n","        \"yes_ans_f1\": (np.mean(yes_answer[\"answer_f1\"]), len(yes_answer) / len_data * 100),\n","        \"no_ans_f1\":  (np.mean(no_answer[\"answer_f1\"]), len(no_answer) / len_data * 100),\n","        \"mc_quest_f1\":  (np.mean(mc_question[\"answer_f1\"]), len(mc_question) / len_data * 100),\n","        \"wh_quest_f1\":  (np.mean(wh_question[\"answer_f1\"]), len(wh_question) / len_data * 100)\n","    }\n","\n","\n","def generate_answer_from_raw_data(\n","    model,\n","    tokenizer,\n","    preprocessing,\n","    passage: Union[str, List[str]],\n","    question: Union[str, List[str]],\n","    history: Optional[Union[str, List[str]]] = None,\n",") -> List[str]:\n","    use_history = history is not None\n","    preprocess = batched_function(preprocessing.preprocess_texts)\n","    if isinstance(passage, str):\n","        passage = [passage]\n","        question = [question]\n","        history = [history]\n","\n","    inputs = {\n","        \"id\": list(range(len(passage))),\n","        \"passage\": passage,\n","        \"question\": question,\n","    }\n","    if use_history:\n","        inputs[\"history\"] = history\n","\n","    inputs = preprocess(inputs)\n","    inputs = preprocessing.process_data_to_model_inputs(\n","        inputs, add_history=use_history, padding=\"max_length\"\n","    )\n","    inputs = inputs.convert_to_tensors(\"pt\")\n","\n","    return generate_answer(model, tokenizer, inputs)\n","\n","\n","def generate_answer(model, tokenizer, inputs):\n","    encoder = model.get_encoder()\n","    encoder_inputs = prepare_model_inputs(encoder, inputs)\n","\n","    inputs = prepare_model_inputs(model, inputs)\n","    inputs.pop(\"decoder_input_ids\", None)\n","\n","    with torch.no_grad():\n","        encoder_outputs = model.encoder(**encoder_inputs, return_dict=True)\n","        outputs = model.generate(**inputs)\n","\n","    answer_types = logits_to_class(encoder_outputs[\"yng_logits\"], task=\"multiclass\")\n","    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    for i in range(len(output_str)):\n","        if answer_types[i] != 2:\n","            output_str[i] = idx_to_answer(answer_types[i])\n","\n","    return {\n","        \"pred_answer\": output_str,\n","        \"yng_logits\": encoder_outputs[\"yng_logits\"],\n","        \"rationale_logits\": encoder_outputs[\"rationale_logits\"],\n","    }\n","\n","\n","def evaluate_conversation(model, tokenizer, df):\n","\n","    model.eval()\n","    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    preprocessing = CoQADatasetPreprocessing(tokenizer, **CONFIG.preprocessing.__dict__)\n","    \n","    conversations_results = []\n","    for _, row in tqdm(df.iterrows(), total=df.shape[0], leave=False):\n","        passage   = row['story']\n","        questions = [q['input_text'] for q in row['questions']]\n","        answers   = [a['input_text'] for a in row['answers']]\n","    \n","        answer_f1_scores = []   # f1-score of single answers within the dialogue\n","\n","        pred_answer = []\n","        for quest, answ in zip(questions, answers):\n","            outputs = generate_answer_from_raw_data(model, tokenizer, preprocessing, passage, quest)\n","            pred_answer.append(outputs[\"pred_answer\"][0])\n","            answer_f1_scores.append(evaluate_answer({\"answer\": answ,\n","                                                     \"pred_answer\": pred_answer[-1]})[\"answer_f1\"])\n","\n","        results = {'source' : row['source'],\n","                    'passage': passage,\n","                    'questions' : questions,\n","                    'answers': answers,\n","                    'predicted_answers' : pred_answer,\n","                    'answers_f1_scores' : answer_f1_scores,\n","                    'conversation_f1_score' : np.mean(answer_f1_scores)}\n","\n","        conversations_results.append(results)\n","\n","    return conversations_results"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.201193Z","iopub.status.busy":"2023-08-02T15:57:47.200504Z","iopub.status.idle":"2023-08-02T15:57:47.830225Z","shell.execute_reply":"2023-08-02T15:57:47.829178Z","shell.execute_reply.started":"2023-08-02T15:57:47.201159Z"},"trusted":true},"outputs":[],"source":["from typing import List, Optional, Union\n","import numpy as np\n","\n","import torch\n","from torchmetrics.classification import MulticlassF1Score\n","\n","import datasets\n","from accelerate import Accelerator\n","\n","\n","per_token_f1_metric = MulticlassF1Score(\n","    num_classes=2,\n","    average=\"macro\",\n","    multidim_average=\"samplewise\",\n","    ignore_index=-100,\n",")\n","\n","macro_f1 = MulticlassF1Score(\n","    num_classes=3,\n","    average=\"macro\",\n","    ignore_index=-100,\n",")\n","\n","\n","def labels_to_answer(labels: torch.Tensor, tokenizer, ignore_index=-100) -> str:\n","    labels[labels == ignore_index] = tokenizer.pad_token_id\n","    answer = tokenizer.decode(labels, skip_special_tokens=True)\n","    return answer\n","\n","\n","def evaluate_answer(example: dict):\n","    return {\"answer_f1\": compute_f1(example[\"answer\"], example[\"pred_answer\"])}\n","\n","\n","def evaluate_rationale_f1(example: dict):\n","    return {\n","        \"rationale_f1\": per_token_f1_metric(\n","            logits_to_class(example[\"rationale_logits\"]).long(),\n","            example[\"rationale_labels\"].long(),\n","        )\n","    }\n","\n","\n","def evaluate_model(model, tokenizer, dataset: datasets.Dataset, config):\n","    accelerator = Accelerator(mixed_precision=config.mixed_precision, cpu=CPU)\n","    model = accelerator.prepare(model)\n","    model.eval()\n","\n","    collator = DynamicPaddingCollatorForSeq2Seq(tokenizer, model)\n","    dataset = dataset.map(\n","        lambda example: generate_answer_from_input_tensors(\n","            model, tokenizer, example, collator\n","        ),\n","        batched=True,\n","        batch_size=4,\n","    )\n","    dataset = dataset.map(\n","        lambda example: {\n","            \"answer\": labels_to_answer(example[\"labels\"], tokenizer=tokenizer)\n","        }\n","    )\n","\n","    # outputs = outputs.select_columns([\"source\", \"passage\", \"question\", \"rationale\", \"answer\", \"pred_answer\", \"answer_type\", 'yng_logits', 'rationale_logits'])\n","\n","    dataset = dataset.map(evaluate_answer)\n","    dataset = dataset.map(evaluate_rationale_f1)\n","\n","    yng_data = dataset.select_columns([\"yng_logits\", \"yng_labels\"])\n","    yng_data = yng_data.map(\n","        lambda example: {\"yng_logits\": logits_to_class(example[\"yng_logits\"])}\n","    )\n","    yng_f1 = macro_f1(yng_data[\"yng_logits\"], yng_data[\"yng_labels\"])\n","\n","    rationale_f1 = np.mean(dataset[\"rationale_f1\"])\n","    answer_squad_f1 = np.mean(dataset[\"answer_f1\"])\n","\n","    return dataset, {\n","        \"yng_f1\": yng_f1,\n","        \"rationale_f1\": rationale_f1,\n","        \"answer_squad_f1\": answer_squad_f1,\n","    }\n","\n","\n","def evaluate_answers(model, tokenizer, data):\n","    model.eval()\n","    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    preprocessing = CoQADatasetPreprocessing(tokenizer, **CONFIG.preprocessing.__dict__)\n","\n","    outputs = data.map(\n","        lambda example: generate_answer(\n","            model, tokenizer, preprocessing, example[\"passage\"], example[\"question\"]\n","        ),\n","        batched=True,\n","        batch_size=4,\n","    )\n","\n","    outputs = outputs.select_columns(\n","        [\n","            \"source\",\n","            \"passage\",\n","            \"question\",\n","            \"rationale\",\n","            \"answer\",\n","            \"pred_answer\",\n","            \"answer_type\",\n","        ]\n","    )\n","\n","    outputs = outputs.map(evaluate_answer)\n","\n","    return outputs\n","\n","\n","def generate_answer(\n","    model,\n","    tokenizer,\n","    preprocessing,\n","    passage: Union[str, List[str]],\n","    question: Union[str, List[str]],\n","    history: Optional[Union[str, List[str]]] = None,\n",") -> List[str]:\n","    use_history = history is not None\n","    preprocess = batched_function(preprocessing.preprocess_texts)\n","    if isinstance(passage, str):\n","        passage = [passage]\n","        question = [question]\n","        history = [history]\n","\n","    inputs = {\n","        \"id\": list(range(len(passage))),\n","        \"passage\": passage,\n","        \"question\": question,\n","    }\n","    if use_history:\n","        inputs[\"history\"] = history\n","\n","    inputs = preprocess(inputs)\n","    inputs = preprocessing.process_data_to_model_inputs(\n","        inputs, add_history=use_history, padding=\"max_length\"\n","    )\n","    inputs = inputs.convert_to_tensors(\"pt\")\n","\n","    return generate_answer_from_input_tensors(model, tokenizer, inputs)\n","\n","\n","def generate_answer_from_input_tensors(model, tokenizer, inputs, collator):\n","    features = [dict(zip(inputs.keys(), values)) for values in zip(*inputs.values())]\n","    features = collator(features)\n","\n","    encoder = model.get_encoder()\n","    encoder_inputs = prepare_model_inputs(encoder, features)\n","\n","    inputs = prepare_model_inputs(model, features)\n","    inputs.pop(\"decoder_input_ids\", None)\n","\n","    with torch.no_grad():\n","        encoder_outputs = model.encoder(**encoder_inputs, return_dict=True)\n","        outputs = model.generate(**inputs)\n","\n","    answer_types = logits_to_class(encoder_outputs[\"yng_logits\"], task=\"multiclass\")\n","    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    for i in range(len(output_str)):\n","        if answer_types[i] != 2:\n","            output_str[i] = idx_to_answer(answer_types[i])\n","\n","    return {\n","        \"pred_answer\": output_str,\n","        \"yng_logits\": encoder_outputs[\"yng_logits\"],\n","        \"rationale_logits\": encoder_outputs[\"rationale_logits\"],\n","    }\n"]},{"cell_type":"markdown","metadata":{},"source":["# Pipeline"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.859701Z","iopub.status.busy":"2023-08-02T15:57:47.859016Z","iopub.status.idle":"2023-08-02T15:57:47.928384Z","shell.execute_reply":"2023-08-02T15:57:47.927437Z","shell.execute_reply.started":"2023-08-02T15:57:47.859673Z"},"trusted":true},"outputs":[],"source":["def pipeline(hyperparameters: dict):\n","    with wandb.init(**CONFIG.wandbConfig.__dict__, config=hyperparameters):\n","        config = wandb.config\n","\n","        set_seed(config.seed)\n","\n","        (\n","            model,\n","            tokenizer,\n","            train_data,\n","            val_data,\n","            train_dataloader,\n","            val_dataloader,\n","            loss_fn,\n","            optimizer,\n","            scheduler,\n","            # metrics,\n","        ) = make(config)\n","        print(model)\n","\n","        train(\n","            model,\n","            train_dataloader,\n","            val_dataloader,\n","            loss_fn,\n","            optimizer,\n","            scheduler,\n","            config,\n","            # metrics=metrics,\n","        )\n","\n","        evaluate(model, tokenizer, train_data, val_data, config)\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","    return model\n","\n","\n","def make(config):\n","    # Make the model\n","    tokenizer, model = make_model(config)\n","\n","    # Make the data\n","    train_data, val_data = get_data(\"train\", config), get_data(\"validation\", config)\n","    train_dataloader = make_dataloader(train_data, tokenizer, config, split=\"train\")\n","    val_dataloader = make_dataloader(val_data, tokenizer, config, split=\"validation\")\n","\n","    # Make the loss, the optimizer and the scheduler\n","    loss_fn = make_loss(config)\n","    optimizer = make_optimizer(model, loss_fn, config)\n","    scheduler = make_scheduler(\n","        optimizer, steps_per_epoch=len(train_dataloader), config=config\n","    )\n","\n","    # # Make the evaluation metrics\n","    # metrics = make_metrics(tokenizer, config)\n","\n","    return (\n","        model,\n","        tokenizer,\n","        train_data,\n","        val_data,\n","        train_dataloader,\n","        val_dataloader,\n","        loss_fn,\n","        optimizer,\n","        scheduler,\n","        # metrics,\n","    )\n","\n","\n","def make_model(config):\n","    checkpoint = CONFIG.checkpoints.__dict__[config.checkpoint_name]\n","    if config.model_type == \"encoder_decoder\":\n","        return make_encoder_decoder_model(\n","            checkpoint=checkpoint,\n","            decoder_max_length=CONFIG.decoder_max_length,\n","            generation_kwargs=CONFIG.generation,\n","            initialize_cross_attention=config.initialize_cross_attention,\n","        )\n","    if config.model_type == \"encoder\":\n","        return make_qa_encoder(checkpoint=checkpoint)\n","\n","    raise ValueError(\n","        \"Invalid model_type. Supported values are 'encoder_decoder' and 'encoder'.\"\n","    )\n","\n","\n","def get_data(split: str, config):\n","    if config.get(\"history_length\", 0) > 0:\n","        path = CONFIG.dataset.train_with_history(config.checkpoint_name, split=split)\n","    else:\n","        path = CONFIG.dataset.train_no_history(config.checkpoint_name, split=split)\n","\n","    dataset = datasets.load_from_disk(path)\n","    dataset = dataset.remove_columns(\n","        [\"id\", \"turn\", \"offset_mapping\", \"rationale_start\", \"rationale_end\"]\n","    )\n","    return dataset\n","\n","\n","def make_dataloader(dataset, tokenizer, config, split: str):\n","    data_collator = DynamicPaddingCollatorForSeq2Seq(tokenizer)\n","    dataloader = create_reproducible_dataloader(\n","        dataset,\n","        batch_size=config.val_batch_size\n","        if split != \"train\" and \"val_batch_size\" in config\n","        else config.batch_size,\n","        collate_fn=data_collator,\n","        num_workers=config.val_num_workers\n","        if split != \"train\" and \"val_num_workers\" in config\n","        else config.num_workers,\n","        pin_memory=True,\n","        shuffle=True,\n","    )\n","    return dataloader\n","\n","\n","def make_loss(config) -> ComputeLoss:\n","    if config.model_type == \"encoder_decoder\":\n","        # loss = composite_loss(\n","        #     Criterion(\n","        #         \"rationale_loss\", encoder_decoder_rationale_loss, weight=config.rationale_loss_weight\n","        #     ),\n","        #     Criterion(\n","        #         \"generative_loss\", encoder_decoder_generative_loss, weight=config.generative_loss_weight\n","        #     ),\n","        # )\n","        # loss = UncertaintyLoss(\n","        #     Criterion(\n","        #         \"rationale_loss\",\n","        #         EncoderDecoderRationaleLoss(\n","        #             max_rationale_length=CONFIG.rationale_max_length\n","        #         ),\n","        #         weight=config.rationale_loss_weight,\n","        #     ),\n","        #     Criterion(\n","        #         \"generative_loss\",\n","        #         encoder_decoder_generative_loss,\n","        #         weight=config.generative_loss_weight,\n","        #     ),\n","        # )\n","\n","        # loss = CompositeLoss(\n","        #     Criterion(\n","        #         \"rationale_loss\",\n","        #         UncertaintyLoss(\n","        #             EncoderDecoderRationaleLoss(\n","        #                 max_rationale_length=CONFIG.rationale_max_length\n","        #             ),\n","        #             initial_weight=config.rationale_loss_weight,\n","        #         ),\n","        #     ),\n","        #     Criterion(\n","        #         \"generative_loss\",\n","        #         UncertaintyLoss(\n","        #             encoder_decoder_generative_loss,\n","        #             initial_weight=config.generative_loss_weight,\n","        #         ),\n","        #     ),\n","        # )\n","\n","        loss = EncoderDecoderLoss(\n","            max_rationale_length=CONFIG.rationale_max_length,\n","            yng_loss_weight=config.yng_loss_weight,\n","            rationale_loss_weight=config.rationale_loss_weight,\n","            generative_loss_weight=config.generative_loss_weight,\n","        )\n","\n","    elif config.model_type == \"encoder\":\n","        loss = EncoderRationaleLoss(max_rationale_length=CONFIG.rationale_max_length)\n","    else:\n","        raise ValueError(\n","            \"Invalid model_type. Supported values are 'encoder_decoder' and 'encoder'.\"\n","        )\n","    return loss\n","\n","\n","def make_optimizer(model, loss_fn, config):\n","    optimizer_cls = getattr(torch.optim, config.optimizer_name)\n","    parameters = [{\"params\": model.parameters()}]\n","    if hasattr(loss_fn, \"_parameters\"):\n","        loss_params = {\"params\": loss_fn.parameters()}\n","        if \"loss_learning_rate\" in config:\n","            loss_params[\"lr\"] = config.loss_learning_rate\n","        parameters.append(loss_params)\n","\n","    return optimizer_cls(\n","        parameters,\n","        lr=config.learning_rate,\n","        **config.get(\"optimizer_args\", {}),\n","    )\n","\n","\n","def make_scheduler(optimizer, steps_per_epoch, config):\n","    total_steps = steps_per_epoch * config.num_epochs\n","    warmup_steps = int(config.warmup_fraction * total_steps)\n","    if config.get(\"scheduler\", \"none\") != \"none\":\n","        return transformers.get_scheduler(\n","            config.scheduler,\n","            optimizer=optimizer,\n","            num_warmup_steps=warmup_steps,\n","            num_training_steps=total_steps,\n","        )\n","\n","    return DummyScheduler(optimizer=optimizer)\n","\n","\n","# def make_metrics(tokenizer, config) -> Dict[str, Metric]:\n","#     if config.model_type not in [\"encoder_decoder\", \"encoder\"]:\n","#         raise ValueError(\n","#             \"Invalid model_type. Supported values are 'encoder_decoder' and 'encoder'.\"\n","#         )\n","\n","#     metrics = {}\n","#     metrics[\"encoder\"] = {\n","#         \"yng_accuracy\": EncoderYNGAccuracy(),\n","#         \"yng_f1\": EncoderYNGF1(),\n","#         \"rationale_accuracy\": EncoderRationaleAccuracy(),\n","#         \"rationale_f1\": EncoderRationaleF1(),\n","#     }\n","#     if config.model_type == \"encoder_decoder\":\n","#         metrics[\"encoder_decoder\"] = {\n","#             \"squad_f1\": GenerativeSquadF1(tokenizer),\n","#         }\n","\n","#     return metrics\n","\n","\n","def train(\n","    model: nn.Module,\n","    train_dataloader: torch.utils.data.DataLoader,\n","    val_dataloader: torch.utils.data.DataLoader,\n","    loss_fn: Union[ComputeLoss, nn.Module],\n","    optimizer: torch.optim.Optimizer,\n","    lr_scheduler,\n","    config,\n","    teacher_force_scheduler = None,\n","    # metrics: Dict[str, Metric] = {},\n","):\n","    watch_list = [model]\n","\n","    accelerator = Accelerator(mixed_precision=config.mixed_precision, cpu=config.cpu)\n","    (\n","        model,\n","        optimizer,\n","        train_dataloader,\n","        val_dataloader,\n","        lr_scheduler,\n","    ) = accelerator.prepare(\n","        model, optimizer, train_dataloader, val_dataloader, lr_scheduler\n","    )\n","    if isinstance(loss_fn, nn.Module):\n","        watch_list.append(loss_fn)\n","        loss_fn = accelerator.prepare(loss_fn)\n","\n","    wandb.watch(watch_list, log=\"all\", log_freq=config.log_interval)\n","\n","    # Run training and track with wandb\n","    steps_per_epoch = len(train_dataloader)\n","    total_steps = steps_per_epoch * config.num_epochs\n","\n","    checkpoint_counter = 0\n","    step = 0\n","    avg_loss = AvgValue()\n","    avg_inner_losses = defaultdict(AvgValue)\n","    model.train()\n","\n","    forward_signature = set(inspect.signature(model.forward).parameters)\n","    progress_bar = tqdm(range(total_steps))\n","    for epoch in range(config.num_epochs):\n","        for data in train_dataloader:\n","            inputs = {\n","                argument: value\n","                for argument, value in data.items()\n","                if argument in forward_signature\n","            }\n","\n","            loss, inner_losses = train_batch(\n","                inputs=inputs,\n","                data=data,\n","                step=step,\n","                model=model,\n","                loss_fn=loss_fn,\n","                optimizer=optimizer,\n","                lr_scheduler=lr_scheduler,\n","                teacher_force_scheduler=teacher_force_scheduler,\n","                accelerator=accelerator,\n","                config=config,\n","            )\n","            progress_bar.update(1)\n","\n","            # Compute statistics\n","            n_samples = len(next(iter(data.values())))\n","            step += 1\n","            avg_loss.update(loss, n_samples)\n","            for loss_name, loss_value in inner_losses.items():\n","                avg_inner_losses[f\"avg_{loss_name}\"].update(loss_value, n_samples)\n","\n","            wandb.log(\n","                {\n","                    \"train_loss\": loss,\n","                    **inner_losses,\n","                    \"lr\": lr_scheduler.get_last_lr()[0],\n","                },\n","                step=step,\n","            )\n","\n","            # Evaluate the model and save checkpoints\n","            if (step % config.log_interval == 0) or (step == total_steps):\n","                # Evaluate the model\n","                val_loss, val_inner_losses, val_metrics = train_evaluation(\n","                    model,\n","                    val_dataloader,\n","                    loss_fn,\n","                    # metrics=metrics,\n","                )\n","                model.train()\n","\n","                train_log(\n","                    avg_loss,\n","                    avg_inner_losses,\n","                    val_loss,\n","                    val_inner_losses,\n","                    val_metrics,\n","                    lr=lr_scheduler.get_last_lr()[0],\n","                    step=step,\n","                )\n","                avg_loss = AvgValue()\n","                avg_inner_losses = defaultdict(AvgValue)\n","\n","            if step % config.checkpoint_interval == 0:\n","                # Saving checkpoint\n","                save_model_checkpoint(\n","                    model,\n","                    optimizer,\n","                    lr_scheduler,\n","                    epoch,\n","                    step,\n","                    checkpoint_counter,\n","                    config,\n","                )\n","                wandb.log(\n","                    {\n","                        \"checkpoint_counter\": checkpoint_counter,\n","                    },\n","                    step=step,\n","                )\n","                checkpoint_counter += 1\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","    wandb.unwatch(watch_list)\n","\n","\n","def train_batch(\n","    inputs,\n","    data,\n","    step,\n","    model,\n","    loss_fn,\n","    optimizer,\n","    lr_scheduler,\n","    config,\n","    teacher_force_scheduler=None,\n","    accelerator=None,\n","    device=None,\n","):\n","    assert (\n","        accelerator is not None or device is not None\n","    ), \"One between accelerator and device must be set.\"\n","\n","    if accelerator is None:\n","        data = {key: value.to(device) for key, value in data.items()}\n","\n","    outputs = model(**inputs, teacher_force=teacher_force_scheduler.get_value(), return_dict=True)\n","\n","    loss, inner_losses = loss_fn(outputs, data)\n","    if accelerator is not None:\n","        accelerator.backward(loss)\n","    else:\n","        loss.backward()\n","\n","    if config.get(\"gradient_clip\", \"none\") != \"none\":\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n","\n","    if step % config.accumulation_steps == 0:\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    lr_scheduler.step()\n","    teacher_force_scheduler.step()\n","\n","    return loss.item(), inner_losses\n","\n","\n","def train_evaluation(\n","    model,\n","    dataloader,\n","    compute_loss: ComputeLoss = None\n","    # , metrics: Dict[str, Metric] = {}\n",") -> Tuple[AvgValue, Dict[str, AvgValue], Dict[str, AvgValue]]:\n","    model.eval()\n","    avg_loss = AvgValue()\n","    avg_inner_losses = defaultdict(AvgValue)\n","    avg_metrics = defaultdict(AvgValue)\n","\n","    forward_signature = set(inspect.signature(model.forward).parameters)\n","    with torch.no_grad():\n","        for data in dataloader:\n","            inputs_kwargs = {\n","                argument: value\n","                for argument, value in data.items()\n","                if argument in forward_signature\n","            }\n","            n_samples = len(next(iter(data.values())))\n","\n","            outputs = model(**inputs_kwargs, return_dict=True)\n","            if compute_loss is not None:\n","                loss, inner_losses = compute_loss(outputs, data)\n","\n","                avg_loss.update(loss.item(), n_samples)\n","                for loss_name, loss_value in inner_losses.items():\n","                    avg_inner_losses[loss_name].update(loss_value, n_samples)\n","\n","            # for metric_name, metric in metrics.items():\n","            #     metric_value = metric(outputs, data)\n","            #     avg_metrics[metric_name].update(metric_value, n_samples)\n","\n","    return avg_loss, avg_inner_losses, avg_metrics\n","\n","\n","def train_log(\n","    train_loss: AvgValue,\n","    train_inner_losses: Dict[str, AvgValue],\n","    val_loss: AvgValue,\n","    val_inner_losses: Dict[str, AvgValue],\n","    val_metrics: Dict[str, AvgValue],\n","    lr,\n","    step,\n","):\n","    train_loss = train_loss.avg()\n","    train_inner_losses = {\n","        f\"{loss_name}\": loss_value.avg()\n","        for loss_name, loss_value in train_inner_losses.items()\n","    }\n","\n","    val_loss = val_loss.avg()\n","    val_inner_losses = {\n","        f\"val_{loss_name}\": loss_value.avg()\n","        for loss_name, loss_value in val_inner_losses.items()\n","    }\n","\n","    val_metrics = {\n","        f\"val_{metric_name}\": metric_value.avg()\n","        for metric_name, metric_value in val_metrics.items()\n","    }\n","\n","    wandb.log(\n","        {\n","            \"avg_train_loss\": train_loss,\n","            **train_inner_losses,\n","            \"val_loss\": val_loss,\n","            **val_inner_losses,\n","            **val_metrics,\n","            \"lr\": lr,\n","        },\n","        step=step,\n","    )\n","    print(\n","        f\"Iteration: {step:6}\",\n","        f\"train loss: {train_loss:.4f}\",\n","        f\"val loss: {val_loss:.4f}\",\n","        f\"lr: {lr:.6f}\",\n","        sep=\"\\t\",\n","    )\n","\n","\n","def save_model_checkpoint(\n","    model, optimizer, lr_scheduler, epoch, step, checkpoint_counter, config\n","):\n","    checkpoint_file = os.path.join(\n","        CONFIG.models.checkpoints_dir(\n","            config.model_name, config.get(\"history_length\", 0) > 0\n","        ),\n","        config.model_name,\n","        f\"checkpoint_{checkpoint_counter}.pt\",\n","    )\n","    create_dirs_for_file(checkpoint_file)\n","\n","    save_checkpoint(\n","        model,\n","        optimizer,\n","        lr_scheduler,\n","        epoch,\n","        step,\n","        checkpoint_counter,\n","        checkpoint_path=checkpoint_file,\n","    )\n","    wandb.save(f\"{config.model_name}_{checkpoint_counter}.pt\")\n","\n","\n","def load_model_checkpoint(\n","    checkpoint_counter, config, model, optimizer=None, lr_scheduler=None\n","):\n","    checkpoint_file = os.path.join(\n","        CONFIG.models.checkpoints_dir(\n","            config.model_name, config.get(\"history_length\", 0) > 0\n","        ),\n","        config.model_name,\n","        f\"checkpoint_{checkpoint_counter}.pt\",\n","    )\n","    return load_checkpoint(\n","        checkpoint_file, model, optimizer=optimizer, scheduler=lr_scheduler\n","    )\n","\n","\n","def evaluate(model, tokenizer, train_data: datasets.Dataset, val_data: datasets.Dataset, config):\n","    datasets = [(\"train\", train_data), (\"val\", val_data)]\n","    results = {}\n","    for dataset_name, dataset in datasets:\n","        outputs, metrics = evaluate_model(model, tokenizer, dataset, config)\n","        results[dataset_name] = (outputs, metrics)\n","\n","        # for metric_name, metric_value in metrics.items():\n","        #     print(f\"{dataset_name}_{metric_name}: {metric_value:.4f}\")\n","        #     wandb.log({f\"evaluation/{dataset_name}_{metric_name}\": metric_value})\n","        \n","        gc.collect()\n","        torch.cuda.empty_cache()\n","    return results"]},{"cell_type":"markdown","metadata":{},"source":["# Run"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-08-02T15:57:47.931028Z","iopub.status.busy":"2023-08-02T15:57:47.930358Z","iopub.status.idle":"2023-08-02T15:59:04.346079Z","shell.execute_reply":"2023-08-02T15:59:04.345018Z","shell.execute_reply.started":"2023-08-02T15:57:47.930991Z"},"trusted":true},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.15.8"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>d:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\wandb\\run-20230823_103454-sgc435x9</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/simonemele999/nlp_assignment2/runs/sgc435x9' target=\"_blank\">expert-glitter-22</a></strong> to <a href='https://wandb.ai/simonemele999/nlp_assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/simonemele999/nlp_assignment2' target=\"_blank\">https://wandb.ai/simonemele999/nlp_assignment2</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/simonemele999/nlp_assignment2/runs/sgc435x9' target=\"_blank\">https://wandb.ai/simonemele999/nlp_assignment2/runs/sgc435x9</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following encoder weights were not tied to the decoder ['bert/pooler']\n"]},{"name":"stdout","output_type":"stream","text":["Cross-attention has been initialized with self-attention weights.\n","QAEncoderDecoderModel(\n","  (encoder): QAEncoder(\n","    (encoder): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n","        (position_embeddings): Embedding(512, 128)\n","        (token_type_embeddings): Embedding(2, 128)\n","        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-1): 2 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=128, out_features=128, bias=True)\n","                (key): Linear(in_features=128, out_features=128, bias=True)\n","                (value): Linear(in_features=128, out_features=128, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=128, out_features=128, bias=True)\n","                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=128, out_features=512, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=512, out_features=128, bias=True)\n","              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=128, out_features=128, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (rationale_head): TokenSelectionHead(\n","      (dense): Linear(in_features=128, out_features=128, bias=False)\n","      (act_fn): ReLU()\n","      (hidden_to_logit): Linear(in_features=128, out_features=1, bias=False)\n","    )\n","    (yes_no_gen_head): YesNoGenHead(\n","      (dense): Linear(in_features=128, out_features=128, bias=False)\n","      (act_fn): ReLU()\n","      (hidden_to_logit): Linear(in_features=128, out_features=1, bias=False)\n","      (softmax): Softmax(dim=-2)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (classifier): Linear(in_features=256, out_features=3, bias=True)\n","    )\n","  )\n","  (decoder): BertLMHeadModel(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n","        (position_embeddings): Embedding(512, 128)\n","        (token_type_embeddings): Embedding(2, 128)\n","        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-1): 2 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=128, out_features=128, bias=True)\n","                (key): Linear(in_features=128, out_features=128, bias=True)\n","                (value): Linear(in_features=128, out_features=128, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=128, out_features=128, bias=True)\n","                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (crossattention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=128, out_features=128, bias=True)\n","                (key): Linear(in_features=128, out_features=128, bias=True)\n","                (value): Linear(in_features=128, out_features=128, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=128, out_features=128, bias=True)\n","                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=128, out_features=512, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=512, out_features=128, bias=True)\n","              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (cls): BertOnlyMLMHead(\n","      (predictions): BertLMPredictionHead(\n","        (transform): BertPredictionHeadTransform(\n","          (dense): Linear(in_features=128, out_features=128, bias=True)\n","          (transform_act_fn): GELUActivation()\n","          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (decoder): Linear(in_features=128, out_features=30522, bias=True)\n","      )\n","    )\n","  )\n",")\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/80 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","  6%|▋         | 5/80 [00:03<00:43,  1.71it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:      5\ttrain loss: 13.4015\tval loss: 8.5973\tlr: 0.003125\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▎        | 10/80 [00:06<00:41,  1.70it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     10\ttrain loss: 7.8429\tval loss: 9.6581\tlr: 0.004861\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▉        | 15/80 [00:10<00:33,  1.96it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     15\ttrain loss: 6.3875\tval loss: 10.4707\tlr: 0.004514\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 20/80 [00:13<00:32,  1.87it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     20\ttrain loss: 5.1804\tval loss: 11.5895\tlr: 0.004167\n"]},{"name":"stderr","output_type":"stream","text":[" 31%|███▏      | 25/80 [00:16<00:28,  1.95it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     25\ttrain loss: 4.8131\tval loss: 11.8111\tlr: 0.003819\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 30/80 [00:20<00:25,  1.92it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     30\ttrain loss: 4.7102\tval loss: 12.0724\tlr: 0.003472\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▍     | 35/80 [00:23<00:27,  1.64it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     35\ttrain loss: 4.7573\tval loss: 11.8095\tlr: 0.003125\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 40/80 [00:27<00:22,  1.79it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     40\ttrain loss: 4.6585\tval loss: 11.6243\tlr: 0.002778\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▋    | 45/80 [00:30<00:19,  1.79it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     45\ttrain loss: 4.4948\tval loss: 11.5416\tlr: 0.002431\n"]},{"name":"stderr","output_type":"stream","text":[" 62%|██████▎   | 50/80 [00:34<00:16,  1.79it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     50\ttrain loss: 4.3357\tval loss: 11.4409\tlr: 0.002083\n"]},{"name":"stderr","output_type":"stream","text":[" 69%|██████▉   | 55/80 [00:37<00:14,  1.68it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     55\ttrain loss: 4.3140\tval loss: 11.3337\tlr: 0.001736\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 60/80 [00:41<00:12,  1.59it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     60\ttrain loss: 4.0489\tval loss: 11.4211\tlr: 0.001389\n"]},{"name":"stderr","output_type":"stream","text":[" 81%|████████▏ | 65/80 [00:45<00:09,  1.63it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     65\ttrain loss: 3.9232\tval loss: 11.2571\tlr: 0.001042\n"]},{"name":"stderr","output_type":"stream","text":[" 88%|████████▊ | 70/80 [00:49<00:06,  1.64it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     70\ttrain loss: 3.7808\tval loss: 11.2997\tlr: 0.000694\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|█████████▍| 75/80 [00:53<00:03,  1.38it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     75\ttrain loss: 3.5524\tval loss: 11.2137\tlr: 0.000347\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 80/80 [00:57<00:00,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["Iteration:     80\ttrain loss: 3.3540\tval loss: 11.1455\tlr: 0.000000\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 80/80 [00:59<00:00,  1.35it/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01c4134f21e74e74bd1bfe594536d462","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/32 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4df23836509482d9c4ddf7a39bf75cb","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/32 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_generative_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>avg_rationale_loss</td><td>▁█▂▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>avg_train_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>avg_yng_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>checkpoint_counter</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>generative_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▂▄▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>rationale_loss</td><td>▁▁▁▂█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_generative_loss</td><td>▁▄▅▇███▇▇▇▇▇▇▇▆▆</td></tr><tr><td>val_loss</td><td>▁▃▅▇▇█▇▇▇▇▇▇▆▆▆▆</td></tr><tr><td>val_rationale_loss</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_yng_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>yng_loss</td><td>█▇▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_generative_loss</td><td>2.6599</td></tr><tr><td>avg_rationale_loss</td><td>0.69413</td></tr><tr><td>avg_train_loss</td><td>3.35404</td></tr><tr><td>avg_yng_loss</td><td>1e-05</td></tr><tr><td>checkpoint_counter</td><td>15</td></tr><tr><td>generative_loss</td><td>2.56383</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>rationale_loss</td><td>0.6938</td></tr><tr><td>train_loss</td><td>3.25763</td></tr><tr><td>val_generative_loss</td><td>10.438</td></tr><tr><td>val_loss</td><td>11.14554</td></tr><tr><td>val_rationale_loss</td><td>0.69407</td></tr><tr><td>val_yng_loss</td><td>0.01347</td></tr><tr><td>yng_loss</td><td>0.0</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">expert-glitter-22</strong> at: <a href='https://wandb.ai/simonemele999/nlp_assignment2/runs/sgc435x9' target=\"_blank\">https://wandb.ai/simonemele999/nlp_assignment2/runs/sgc435x9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>.\\wandb\\run-20230823_103454-sgc435x9\\logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"ValueError","evalue":"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`answer` in this case) have excessive nesting (inputs type `list` where type `int` is expected).","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","File \u001b[1;32md:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:718\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> 718\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[0;32m    720\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    721\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    724\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    725\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n","\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[15], line 71\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39mprint\u001b[39m(model)\n\u001b[0;32m     61\u001b[0m     train(\n\u001b[0;32m     62\u001b[0m         model,\n\u001b[0;32m     63\u001b[0m         train_dataloader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m         config,\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 71\u001b[0m     evaluate(model, tokenizer, train_data, val_data, config)\n\u001b[0;32m     73\u001b[0m \u001b[39m# pipeline(hyperparameters)\u001b[39;00m\n","Cell \u001b[1;32mIn[14], line 532\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, tokenizer, train_data, val_data, config)\u001b[0m\n\u001b[0;32m    530\u001b[0m results \u001b[39m=\u001b[39m {}\n\u001b[0;32m    531\u001b[0m \u001b[39mfor\u001b[39;00m dataset_name, dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m--> 532\u001b[0m     outputs, metrics \u001b[39m=\u001b[39m evaluate_model(model, tokenizer, dataset, config)\n\u001b[0;32m    533\u001b[0m     results[dataset_name] \u001b[39m=\u001b[39m (outputs, metrics)\n\u001b[0;32m    535\u001b[0m     \u001b[39mfor\u001b[39;00m metric_name, metric_value \u001b[39min\u001b[39;00m metrics\u001b[39m.\u001b[39mitems():\n","Cell \u001b[1;32mIn[13], line 55\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, tokenizer, dataset, config)\u001b[0m\n\u001b[0;32m     49\u001b[0m collator \u001b[39m=\u001b[39m DynamicPaddingCollatorForSeq2Seq(tokenizer, model)\n\u001b[0;32m     50\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m     51\u001b[0m     \u001b[39mlambda\u001b[39;00m example: {\n\u001b[0;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m: labels_to_answer(example[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m], tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[0;32m     53\u001b[0m     }\n\u001b[0;32m     54\u001b[0m )\n\u001b[1;32m---> 55\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m     56\u001b[0m     \u001b[39mlambda\u001b[39;49;00m example: generate_answer_from_input_tensors(\n\u001b[0;32m     57\u001b[0m         model, tokenizer, example, collator\n\u001b[0;32m     58\u001b[0m     ),\n\u001b[0;32m     59\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     60\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[39m# outputs = outputs.select_columns([\"source\", \"passage\", \"question\", \"rationale\", \"answer\", \"pred_answer\", \"answer_type\", 'yng_logits', 'rationale_logits'])\u001b[39;00m\n\u001b[0;32m     65\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(evaluate_answer)\n","File \u001b[1;32md:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    577\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    579\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    580\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    581\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n","File \u001b[1;32md:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    541\u001b[0m }\n\u001b[0;32m    542\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    544\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    545\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n","File \u001b[1;32md:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3065\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3066\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3067\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3068\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3071\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3072\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3073\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3074\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   3075\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n","File \u001b[1;32md:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3449\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3445\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   3446\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[0;32m   3447\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3448\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3449\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   3450\u001b[0m         batch,\n\u001b[0;32m   3451\u001b[0m         indices,\n\u001b[0;32m   3452\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m   3453\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[0;32m   3454\u001b[0m     )\n\u001b[0;32m   3455\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3456\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3458\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32md:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3330\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3328\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   3329\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> 3330\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39madditional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3332\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m   3333\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[0;32m   3334\u001b[0m     }\n","Cell \u001b[1;32mIn[13], line 56\u001b[0m, in \u001b[0;36mevaluate_model.<locals>.<lambda>\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m     49\u001b[0m collator \u001b[39m=\u001b[39m DynamicPaddingCollatorForSeq2Seq(tokenizer, model)\n\u001b[0;32m     50\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m     51\u001b[0m     \u001b[39mlambda\u001b[39;00m example: {\n\u001b[0;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m: labels_to_answer(example[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m], tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[0;32m     53\u001b[0m     }\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     55\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mlambda\u001b[39;00m example: generate_answer_from_input_tensors(\n\u001b[0;32m     57\u001b[0m         model, tokenizer, example, collator\n\u001b[0;32m     58\u001b[0m     ),\n\u001b[0;32m     59\u001b[0m     batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[39m# outputs = outputs.select_columns([\"source\", \"passage\", \"question\", \"rationale\", \"answer\", \"pred_answer\", \"answer_type\", 'yng_logits', 'rationale_logits'])\u001b[39;00m\n\u001b[0;32m     65\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(evaluate_answer)\n","Cell \u001b[1;32mIn[13], line 149\u001b[0m, in \u001b[0;36mgenerate_answer_from_input_tensors\u001b[1;34m(model, tokenizer, inputs, collator)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_answer_from_input_tensors\u001b[39m(model, tokenizer, inputs, collator):\n\u001b[0;32m    148\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(inputs\u001b[39m.\u001b[39mkeys(), values)) \u001b[39mfor\u001b[39;00m values \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39minputs\u001b[39m.\u001b[39mvalues())]\n\u001b[1;32m--> 149\u001b[0m     features \u001b[39m=\u001b[39m collator(features)\n\u001b[0;32m    151\u001b[0m     encoder \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_encoder()\n\u001b[0;32m    152\u001b[0m     encoder_inputs \u001b[39m=\u001b[39m prepare_model_inputs(encoder, features)\n","Cell \u001b[1;32mIn[11], line 87\u001b[0m, in \u001b[0;36mDynamicPaddingCollatorForSeq2Seq.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_feature(feature_name, features)\n\u001b[1;32m---> 87\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[0;32m     88\u001b[0m     features,\n\u001b[0;32m     89\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[0;32m     90\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[0;32m     91\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m     92\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m     93\u001b[0m )\n\u001b[0;32m     95\u001b[0m \u001b[39m# prepare decoder_input_ids\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m features\n\u001b[0;32m     98\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdecoder_input_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m features\n\u001b[0;32m     99\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mprepare_decoder_input_ids_from_labels\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m ):\n","File \u001b[1;32md:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3045\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3042\u001b[0m             batch_outputs[key] \u001b[39m=\u001b[39m []\n\u001b[0;32m   3043\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m-> 3045\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n","File \u001b[1;32md:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    207\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[0;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[1;32m--> 211\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n","File \u001b[1;32md:\\Simone\\Programmazione\\Universita\\Natural Language Processing\\assignment2\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:734\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    729\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    730\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    732\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    733\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    735\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m features (`\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n","\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`answer` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."]}],"source":["class PropertyDict(dict):\n","    def __getattr__(self, key):\n","        if key in self:\n","            return self[key]\n","        raise AttributeError(\n","            f\"'{self.__class__.__name__}' object has no attribute '{key}'\"\n","        )\n","\n","    def __setattr__(self, key, value):\n","        self[key] = value\n","\n","\n","hyperparameters = PropertyDict(\n","    seed=42,\n","    checkpoint_name=\"bert_tiny\",\n","    model_name=\"bert_tiny\",\n","    model_type=\"encoder_decoder\",\n","    initialize_cross_attention=True,\n","    yng_loss_weight=1.0,\n","    generative_loss_weight=0.06,\n","    rationale_loss_weight=1.44,\n","    batch_size=8,\n","    val_batch_size=64,\n","    num_workers=0,\n","    num_epochs=20,\n","    optimizer_name=\"AdamW\",\n","    learning_rate=5e-3,\n","    loss_learning_rate=1e-1,\n","    scheduler=\"linear\",\n","    warmup_fraction=0.1,\n","    accumulation_steps=1,\n","    mixed_precision=\"fp16\",\n","    checkpoint_interval=5,\n","    log_interval=5,\n",")\n","\n","with wandb.init(project=CONFIG.wandbConfig.project, config=hyperparameters):\n","    config = wandb.config\n","\n","    set_seed(config.seed)\n","\n","    # Make the model\n","    tokenizer, model = make_model(config)\n","\n","    # Make the data\n","    train_data = get_data(\"train\", config).shuffle(42).select(range(32))\n","    val_data = get_data(\"validation\", config).shuffle(42).select(range(64))\n","    train_dataloader = make_dataloader(train_data, tokenizer, config, split=\"train\")\n","    val_dataloader = make_dataloader(val_data, tokenizer, config, split=\"validation\")\n","\n","    # Make the loss, the optimizer and the scheduler\n","    loss_fn = make_loss(config)\n","    optimizer = make_optimizer(model, loss_fn, config)\n","    scheduler = make_scheduler(\n","        optimizer, steps_per_epoch=len(train_dataloader), config=config\n","    )\n","\n","    # model, train_dataloader, val_dataloader, loss_fn, optimizer, scheduler, metrics = make(config)\n","    print(model)\n","\n","    train(\n","        model,\n","        train_dataloader,\n","        val_dataloader,\n","        loss_fn,\n","        optimizer,\n","        scheduler,\n","        config,\n","    )\n","\n","    evaluate(model, tokenizer, train_data, val_data, config)\n","\n","# pipeline(hyperparameters)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
