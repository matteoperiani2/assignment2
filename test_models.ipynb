{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.pipeline import *\n",
    "from src.utils import PropertyDict\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "import pandas as pd\n",
    "from src.evaluation import evaluate_conversation\n",
    "from src.utils import print_5_worst_source_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PropertyDict(\n",
    "    seed=42,\n",
    "    checkpoint_name=\"bert_tiny\",\n",
    "    model_name=\"bert_tiny\",\n",
    "    model_type=\"encoder_decoder\",\n",
    "    initialize_cross_attention=True,\n",
    "    # generative_loss_weight=0.06,\n",
    "    # rationale_loss_weight=1.44,\n",
    "    generative_loss_weight=1.,\n",
    "    rationale_loss_weight=1.,\n",
    "    yng_loss_weight=1.,\n",
    "    batch_size=32,\n",
    "    val_batch_size=256,\n",
    "    num_workers=0,\n",
    "    num_epochs=3,\n",
    "    optimizer_name=\"AdamW\",\n",
    "    learning_rate=5e-4,\n",
    "    #loss_learning_rate=5e-1,\n",
    "    scheduler=\"linear\",\n",
    "    warmup_fraction=0.1,\n",
    "    accumulation_steps=1,\n",
    "    mixed_precision=\"fp16\",\n",
    "    checkpoint_interval=600,\n",
    "    log_interval=600,\n",
    "    cpu=False,\n",
    ")\n",
    "\n",
    "set_seed(config.seed)\n",
    "# disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: empty expression not allowed (3015379063.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 27\u001b[0;36m\u001b[0m\n\u001b[0;31m    torch.save(model.state_dict(), f\"checkpoints/bert_tiny_{}.pt\")\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: empty expression not allowed\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = make_model(config)\n",
    "# print(model)\n",
    "\n",
    "# Make the data\n",
    "train_data = get_data(\"train\", config).shuffle(42)\n",
    "val_data = get_data(\"validation\", config).shuffle(42)\n",
    "train_dataloader = make_dataloader(train_data, tokenizer, config, split=\"train\")\n",
    "val_dataloader = make_dataloader(val_data, tokenizer, config, split=\"validation\")\n",
    "\n",
    "# Make the loss, the optimizer and the scheduler\n",
    "loss_fn = make_loss(config)\n",
    "optimizer = make_optimizer(model, loss_fn, config)\n",
    "scheduler = make_scheduler(\n",
    "    optimizer, steps_per_epoch=len(train_dataloader), config=config\n",
    ")\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    config,\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), f\"checkpoints/{config.model_name}_{config.seed}_nh.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-attention has been initialized with self-attention weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/matteo/uni/nlp/nlp_assignment2/data/train/train_no_history/bert_tiny/train/cache-63078ed47e5b4d10.arrow\n",
      "Loading cached shuffled indices for dataset at /home/matteo/uni/nlp/nlp_assignment2/data/train/train_no_history/bert_tiny/validation/cache-6228d23398cad876.arrow\n",
      "Loading cached shuffled indices for dataset at /home/matteo/uni/nlp/nlp_assignment2/data/train/train_no_history/bert_tiny/test/cache-20451cac29f656b2.arrow\n",
      "Parameter 'function'=<function evaluate_tokenized_dataset.<locals>.<lambda> at 0x7ff4f46b1620> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911362cff5334495b09e2b8078a7db69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839662972f9c4242a43d1535db8d92ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1500f8ca4ad6437aa202c2cdf3cc3ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989526a84c4a4591a127ca1c544671eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c1d95c00364d55a8ae6face99192e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a2c4a5a93c406493d901b8643a994e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad0f6a5dd514c278324b10c6e31f9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a4132ef5a24658ae5b230d8f98d3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb9346fdc424433bef78bd061f0147b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7b9bf168e444d084057f054f06cce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6655437ae0bc43388531afbbd1efa4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5eb92257fda43b799a9344bca745974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667b1a15a2f347d99baa80265417454e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b99eb0c14504106b7bef75a7efb6a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a7841719174c96af54bc3ffa48c61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cfb72d9d3b47e388c0dafbcf10068a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a338d19cc9c0477d8cf29d40a5781da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcc3225184b46c1912f82570737e20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m Evaluating train set: \u001b[0m\n",
      "Example of Q&A generated:\n",
      "A_pred: progressive growth\n",
      "A_true: progressive growth\n",
      "\n",
      "A_pred: mr. weston\n",
      "A_true: place of rendezvous\n",
      "\n",
      "A_pred: waterman\n",
      "A_true: junk food\n",
      "\n",
      "A_pred: sam\n",
      "A_true: sam and kumar are\n",
      "\n",
      "A_pred: mawda\n",
      "A_true: dawda jawara\n",
      "\n",
      "YesNoGen head f1: 77.66 %\n",
      "Rationale f1: 50.78 %\n",
      "SQUAD-f1: 36.94 %\n",
      "______________________________\n",
      "\n",
      "\u001b[1m Evaluating val set: \u001b[0m\n",
      "Example of Q&A generated:\n",
      "A_pred: every day\n",
      "A_true: each month\n",
      "\n",
      "A_pred: four dollars\n",
      "A_true: swim across\n",
      "\n",
      "A_pred: scott\n",
      "A_true: the public\n",
      "\n",
      "A_pred: a guning\n",
      "A_true: with the pontoon train\n",
      "\n",
      "A_pred: admiration\n",
      "A_true: it concerns him\n",
      "\n",
      "YesNoGen head f1: 72.05 %\n",
      "Rationale f1: 50.26 %\n",
      "SQUAD-f1: 25.48 %\n",
      "______________________________\n",
      "\n",
      "\u001b[1m Evaluating test set: \u001b[0m\n",
      "Example of Q&A generated:\n",
      "A_pred: yes\n",
      "A_true: yes\n",
      "\n",
      "A_pred: six\n",
      "A_true: over 11 million\n",
      "\n",
      "A_pred: yes\n",
      "A_true: no\n",
      "\n",
      "A_pred: no\n",
      "A_true: no\n",
      "\n",
      "A_pred: photos\n",
      "A_true: with a prayer\n",
      "\n",
      "YesNoGen head f1: 71.66 %\n",
      "Rationale f1: 50.48 %\n",
      "SQUAD-f1: 26.42 %\n",
      "______________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = make_model(config)\n",
    "model.load_state_dict(torch.load(\"checkpoints/bert_tiny.pt\"))\n",
    "\n",
    "set_size = None\n",
    "train_set = get_data(\"train\", config).shuffle(42)\n",
    "val_set = get_data(\"validation\", config).shuffle(42)\n",
    "test_set = get_data(\"test\", config).shuffle(42)\n",
    "\n",
    "if set_size is not None:\n",
    "    train_set = train_set.select(range(set_size))\n",
    "    val_set = val_set.select(range(set_size))\n",
    "    test_set = test_set.select(range(set_size))\n",
    "\n",
    "results = evaluate(model, tokenizer, train_set, val_set, test_set, config)\n",
    "\n",
    "print()\n",
    "for key, (data, res)in results.items():\n",
    "    print(f\"\\033[1m Evaluating {key} set: \\033[0m\")\n",
    "\n",
    "    print(\"Example of Q&A generated:\")\n",
    "    for i in range(5):\n",
    "        print(\"A_pred:\", data[\"pred_answer\"][i])\n",
    "        print(\"A_true:\", tokenizer.decode(np.abs(data[\"labels\"][i]), skip_special_tokens=True ))\n",
    "        print()\n",
    "\n",
    "    print(f\"YesNoGen head f1: {res['yng_f1']*100:.2f} %\")\n",
    "    print(f\"Rationale f1: {res['rationales_f1']*100:.2f} %\")\n",
    "    print(f\"SQUAD-f1: {res['answers_squad_f1']*100:.2f} %\")\n",
    "    print(\"_\"*30)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mEvaluating train set: \u001b[0m\n",
      "\n",
      "Example of Q&A generated:\n",
      "Q: How did they greet?\n",
      "A_pred: a pipe\n",
      "A_true: The two men shook hands\n",
      "\n",
      "Q: what?\n",
      "A_pred: gloves\n",
      "A_true: gloves\n",
      "\n",
      "Q: Are they proud?\n",
      "A_pred: yes\n",
      "A_true: Yes\n",
      "\n",
      "Q: How many terms did he serve?\n",
      "A_pred: two\n",
      "A_true: Three\n",
      "\n",
      "Q: What was her opponent ranked?\n",
      "A_pred: fourth\n",
      "A_true: Fourth\n",
      "\n",
      "Total train dataset SQUAD-f1: 0.37\n",
      " - yes_ans_f1 = 0.74 (10.0 %)\n",
      " - no_ans_f1 = 0.61 (8.2 %)\n",
      " - mc_quest_f1 = 0.40 (0.6 %)\n",
      " - wh_quest_f1 = 0.31 (76.4 %)\n",
      "______________________________\n",
      "\n",
      "\u001b[1mEvaluating val set: \u001b[0m\n",
      "\n",
      "Example of Q&A generated:\n",
      "Q: How often?\n",
      "A_pred: every day\n",
      "A_true: each month\n",
      "\n",
      "Q: How?\n",
      "A_pred: four dollars\n",
      "A_true: swim across\n",
      "\n",
      "Q: Who will decide the outcome of the election?\n",
      "A_pred: scott\n",
      "A_true: the public\n",
      "\n",
      "Q: With what?\n",
      "A_pred: a guning\n",
      "A_true: With the pontoon train\n",
      "\n",
      "Q: How does Horn feel about the plan?\n",
      "A_pred: admiration\n",
      "A_true: It concerns him\n",
      "\n",
      "Total val dataset SQUAD-f1: 0.26\n",
      " - yes_ans_f1 = 0.70 (10.4 %)\n",
      " - no_ans_f1 = 0.49 (9.6 %)\n",
      " - mc_quest_f1 = 0.33 (0.5 %)\n",
      " - wh_quest_f1 = 0.18 (76.5 %)\n",
      "______________________________\n",
      "\n",
      "\u001b[1mEvaluating test set: \u001b[0m\n",
      "\n",
      "Example of Q&A generated:\n",
      "Q: Did Stanley want to shake hands?\n",
      "A_pred: yes\n",
      "A_true: yes\n",
      "\n",
      "Q: How many?\n",
      "A_pred: six\n",
      "A_true: over 11 million\n",
      "\n",
      "Q: Is he dumber than most film heroes?\n",
      "A_pred: yes\n",
      "A_true: no\n",
      "\n",
      "Q: Did he testify when he was the subject of a suit?\n",
      "A_pred: no\n",
      "A_true: No\n",
      "\n",
      "Q: How does the recording start?\n",
      "A_pred: photos\n",
      "A_true: with a prayer\n",
      "\n",
      "Total test dataset SQUAD-f1: 0.27\n",
      " - yes_ans_f1 = 0.76 (11.5 %)\n",
      " - no_ans_f1 = 0.46 (10.8 %)\n",
      " - mc_quest_f1 = 0.60 (0.5 %)\n",
      " - wh_quest_f1 = 0.18 (73.9 %)\n",
      "______________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'tokenizer' not in locals() or 'model' not in locals():\n",
    "    tokenizer, model = make_model(config)\n",
    "    model.load_state_dict(torch.load(\"checkpoints/bert_tiny.pt\"))\n",
    "\n",
    "set_size = 1000\n",
    "if 'train_set' not in locals(): train_set = datasets.load_from_disk(CONFIG.dataset.processed_dir, )[\"train\"].shuffle(42)\n",
    "if 'val_set' not in locals(): val_set = datasets.load_from_disk(CONFIG.dataset.processed_dir)[\"validation\"].shuffle(42)\n",
    "if 'test_set' not in locals(): test_set = datasets.load_from_disk(CONFIG.dataset.processed_dir)[\"test\"].shuffle(42)\n",
    "\n",
    "if set_size is not None:\n",
    "    train_set = train_set.select(range(set_size))\n",
    "    val_set = val_set.select(range(set_size))\n",
    "    test_set = test_set.select(range(set_size))\n",
    "\n",
    "results = evaluate(model, tokenizer, train_set, val_set, test_set, config)\n",
    "\n",
    "print()\n",
    "for key, (data, res)in results.items():\n",
    "    print(f\"\\033[1mEvaluating {key} set: \\033[0m\\n\")\n",
    "\n",
    "    print(\"Example of Q&A generated:\")\n",
    "    for i in range(5):\n",
    "        print(\"Q:\", data[\"question\"][i])\n",
    "        print(\"A_pred:\", data[\"pred_answer\"][i])\n",
    "        print(\"A_true:\", data[\"answer\"][i])\n",
    "        print()\n",
    "\n",
    "    tot_squad_f1 = res.pop(\"tot_squad_f1\")\n",
    "\n",
    "    print(f\"Total {key} dataset SQUAD-f1: {tot_squad_f1[0]:.2f}\")\n",
    "\n",
    "    for k,v in res.items():\n",
    "        print(f\" - {k} = {v[0]:.2f} ({v[1]:.1f} %)\")\n",
    "\n",
    "    print(\"_\"*30)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tokenizer' not in locals() or 'model' not in locals():\n",
    "    tokenizer, model = make_model(config)\n",
    "    model.load_state_dict(torch.load(\"checkpoints/bert_tiny.pt\"))\n",
    "\n",
    "if 'dataset' not in locals(): dataset = datasets.load_from_disk(CONFIG.dataset.filtered_dir).shuffle(42)\n",
    "\n",
    "df_test = pd.DataFrame(dataset[\"test\"]).iloc[:10]\n",
    "\n",
    "for source, df in df_test.groupby(by=['source']):\n",
    "\n",
    "    conversations_results = pd.DataFrame(evaluate_conversation(model, tokenizer, df))\n",
    "    conversations_results = conversations_results.sort_values(by='conversation_f1_score', ascending=True, inplace=False).iloc[:5, :].reset_index(drop=True)\n",
    "    print(conversations_results)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source                                            passage  \\\n",
      "0    cnn  Los Angeles (CNN) -- A man convicted of stalki...   \n",
      "1    cnn  (CNN) -- Lionel Messi is not for sale. \\n\\nTha...   \n",
      "2    cnn  (CNN) -- We should all be so lucky to have fri...   \n",
      "\n",
      "                                           questions  \\\n",
      "0  [Who was arrested?, Did he escape from anywher...   \n",
      "1  [who is not for sale, who was this message fro...   \n",
      "2  [What author is this about?, What news media w...   \n",
      "\n",
      "                                             answers  \\\n",
      "0  [Robert Dewey Hoskins,, A mental hospital, A w...   \n",
      "1  [Lionel Messi, Barcelona's new president, the ...   \n",
      "2  [Robert Crais, CNN, stars of crime novels, Pri...   \n",
      "\n",
      "                                   predicted_answers  \\\n",
      "0  [a man, yes, friday, murder, police officer, p...   \n",
      "1  [lionel messi, barcelona, manager, argentine, ...   \n",
      "2  [billais, cnn, joe, oscar, los angeles, 1987, ...   \n",
      "\n",
      "                                   answers_f1_scores  conversation_f1_score  \n",
      "0  [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...               0.125000  \n",
      "1  [1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, ...               0.275000  \n",
      "2  [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...               0.294118  \n"
     ]
    }
   ],
   "source": [
    "print_5_worst_source_answers(conversations_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
